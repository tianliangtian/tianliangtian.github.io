{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my website!!!!!","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Learning/","title":"Here is an index","text":""},{"location":"Learning/Miscellaneous/","title":"Miscellaneous","text":""},{"location":"Learning/Miscellaneous/#module-import-in-python","title":"Module Import in Python","text":"<p>For a package structure as following: </p> <pre><code>package\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 subpackage1\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 moduleX.py\n\u2502   \u2514\u2500\u2500 moduleY.py\n\u251c\u2500\u2500 subpackage2\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 moduleZ.py\n\u2514\u2500\u2500 moduleA.py\n</code></pre> <p>Assuming you execute the following command under <code>package</code></p> <pre><code>1. python subpackage1/moduleX.py   \n2. python -m subpackage1.moduleX\n</code></pre> <p>The first one will import modules from <code>sys.path</code>. A <code>ModuleNotFound</code> error raises if the desired one is not in <code>sys.path</code>. So if <code>moduleX.py</code> import function in <code>moduleZ.py</code>, it may cause error since the <code>sys.path</code> may only contains <code>xxx/package/subpackage1</code></p> <p>Instead, the second one will first import the desired package or module, then execute the script.</p>"},{"location":"Learning/Python/","title":"Python","text":""},{"location":"Learning/Python/#introduction","title":"introduction","text":"<ul> <li>Don't need to explicitly compile and run your code like C.</li> <li>Use interpreter, a program to read and run your code.</li> <li>Less code, more work.</li> <li>A good ecosystem with lots of libraries to solve problems which have been solved by others. <pre><code>code hello.py\npython hello.py         # python means the interpreter program here \n</code></pre></li> </ul> hello.py<pre><code>print(\"hello, world\")\n</code></pre>"},{"location":"Learning/Python/#types","title":"Types","text":"<p><code>bool</code>, <code>int</code>, <code>float</code> and <code>str</code> are types in python. Use <code>input</code> to get str from your keyboard calculator.py<pre><code>x = input(\"x: \")\ny = input(\"y: \")\n\nprint(x + y)\n</code></pre></p> <p>Note that you can perform <code>str</code> by <code>+</code> and <code>*</code>. <code>+</code> performs string concatenation and <code>*</code> performs repetition.</p> <p>Run the code and we get following result. The variable type that <code>input</code> returns is <code>str</code> explains the reason why output is 12, i.e. '1' + '2' <pre><code>python .\\calculator.py\nx: 1\ny: 2\n12\n</code></pre> The correct version is shown below new_calculator.py<pre><code>x = int(input(\"x: \"))\ny = int(input(\"y: \"))\n\nprint(x + y)\n</code></pre> A traceback will appear if the input isn't number. <code>type()</code> can be used to tell you the type of a value. <pre><code>&gt;&gt;&gt; type(1.2)\n&lt;class 'float'&gt;\n</code></pre> In these results, the word \u201cclass\u201d is used in the sense of a category; a type is a category of values.</p>"},{"location":"Learning/Python/#string","title":"String","text":"<p>Use <code>index</code> to choose the char you want in the sequence. Use <code>slice</code> to select a segment of a string. <pre><code>&gt;&gt;&gt; s = \"hello, world\"\n&gt;&gt;&gt; s[1]\n'e'\n&gt;&gt;&gt; s[0:5]\n'hello'\n&gt;&gt;&gt; s[:7]\n'hello, '\n&gt;&gt;&gt; s[7:]\n'world'\n&gt;&gt;&gt; s[:]\n'hello, world'\n</code></pre> The operator <code>[n:m]</code> returns the part of the string from the \u201cn-eth\u201d character to the \u201cm-eth\u201d character, including the first but excluding the last.</p> <p><code>str</code> is immutable.</p>"},{"location":"Learning/Python/#conditions","title":"Conditions","text":"<ul> <li>Indentation is important in python</li> </ul> <pre><code>if x &lt; y:\n    print(\"x is less than y\")\nelif x &gt; y:\n    print(\"x is greater than y\")\nelse:\n    print(\"x is equal to y\")\n</code></pre>"},{"location":"Learning/Python/#object-oriented-programming","title":"Object-Oriented Programming","text":"<p>Some functions are built into objectives themselves. These functions called <code>method</code> comes with some data types, like a string. agree.py<pre><code>s = input(\"Do you agree? \")\n# method for string to convert it to lowercase\ns = s.lower()               \n\nif s in [\"y\", \"yes\"]:\n    print(\"Agreed\")\nelif s in [\"n\", \"no\"]:\n    print(\"Not agreed\")\n</code></pre></p> <p>Click here for official documentation</p>"},{"location":"Learning/Python/#class","title":"Class","text":"<p>A programmer-defined type is also called a class. We can define a class like this:</p> <pre><code>class Point:\n     \"\"\"Represents a point in 2-D space\"\"\"\n</code></pre> <p>To create a Point class, you can call <code>Point</code> as if it were a function. Creating a new object is called instantiation, and the object is an instance of the class.</p> <pre><code>&gt;&gt;&gt; black = Point()\n&gt;&gt;&gt; black\n&lt;__main__.Point object at 0x000002595BC5D890&gt;\n</code></pre> <p>You can assign values to named elements of an object using dot notation. These elements are called attributes</p> <pre><code>&gt;&gt;&gt; black.x = 3.0\n&gt;&gt;&gt; black.y = 4.0\n</code></pre> <p>Actually, when creating an object or instance of a class, the built-in method <code>__init__</code> will initialize the content of the object for us. We have an additional parameter in the defination of <code>__init__</code> called <code>self</code>. It refers to the object itself and provides us a way to store values in the object.</p> <pre><code>class Student:\n    def __init__(self, name, house):\n        self.name = name\n        self.house = house\n\n\ndef get_student():\n    name = input(\"Name: \")\n    house = input(\"House: \")\n    student = Student(name, house)\n    return student\n</code></pre> <p><code>__str__</code> is also a function called when the object needed to be treated as a <code>str</code>, such as <code>print(student)</code>. When we define <code>__str__</code>, the string ver of the object is just the return value of <code>__str__</code>. <pre><code>class Student:\n    def __init__(self, name, house):\n        self.name = name\n        self.house = house\n\n    def __str__(self):\n        return f\"{self.name} from {self.house}\"\n</code></pre></p>"},{"location":"Learning/Python/#decorator","title":"Decorator","text":"<p><code>@</code> is called a decorator to decorate functions in class.</p> <p>For example, a getter function is a function called when we want to get the attribute of an object, a setter function is a function called when we want to set the attribute of an object.</p> <p>We can define getter function with <code>@property</code> and setter function with <code>@[attribute_name].setter</code>.</p> <p><pre><code>class Student:\n    def __init__(self, name, house):\n        self.name = name\n        self.house = house\n\n    def __str__(self):\n        return f\"{self.name} from {self.house}\"\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        if not name:\n            raise ValueError(\"Missing name\")\n        self._name = name\n\n    @property\n    def house(self):\n        return self._house\n\n    @house.setter\n    def house(self, house):\n        if house not in [\"A\", \"B\", \"C\", \"D\"]:\n            raise ValueError(\"Invalid house\")\n        self._house = house\n\ndef get_student():\n    name = input(\"Name: \")\n    house = input(\"House: \")\n    student = Student(name, house)\n    return student\n</code></pre> Now, each time we try to set the attributes of objects, the setter function will be called to check whether the assignment is valid. Note that in our setter or getter function, we store the value in <code>self._house</code> or <code>self._name</code>. That's because our attibutes can't have the same name with functions. So we add a underscore to distinguish them. In <code>__init__</code> function, we call those setter functions so we don't use underscore.</p> <p>Tragically, you can still set the attribute invalidly without raising error by assigning <code>self._house</code> directly. So the convention is don't change attributes begin with <code>_</code>.</p>"},{"location":"Learning/Python/#classmethod","title":"Classmethod","text":"<p>The function decorated by <code>@Classmethod</code> can't access attributes of <code>self</code>. But it can access class variables. A classmethod can be directly called without creating an instance.</p>"},{"location":"Learning/Python/#inheritance","title":"Inheritance","text":"<p>When we want to define two classes that shares some same attributes or method, we can define a third class and pass that class to other two classes as a super class, which is called inheritance.</p> <p>In the following example, <code>Wizard</code> is a <code>parent</code>, or <code>super</code> class of <code>Student</code> and <code>Professor</code>. <code>super().[function_name]</code> means calling the function of its super class. <pre><code>class Wizard:\n    def __init__(self, name):\n        if not name:\n            raise ValueError(\"Missing name\")\n        self.name = name\n\n\nclass Student(Wizard):\n    def __init__(self, name, house):\n        super().__init__(name)\n        self.house = house\n\n\nclass Professor(Wizard):\n    def __init__(self, name, subject):\n        super().__init__(name)\n        self.subject = subject\n</code></pre></p>"},{"location":"Learning/Python/#oprator-overloading","title":"Oprator Overloading","text":"<p>You can define the distinct usage of specific operator which concatenates the class. For example, if you want to use <code>+</code> to operate your class, you can define <code>__add__</code> function.</p> <pre><code>class Vault:\n    def __init__(self, galleons=0, sickles=0, knuts=0):\n        self.galleons = galleons\n        self.sickles = sickles\n        self.knuts = knuts\n\n    def __str__(self):\n        return f\"{self.galleons} Galleons, {self.sickles} Sickles, {self.knuts} Knuts\"\n\n    def __add__(self, other):\n        galleons = self.galleons + other.galleons\n        sickles = self.sickles + other.sickles\n        knuts = self.knuts + other.knuts\n        return Vault(galleons, sickles, knuts)\n\npotter = Vault(100, 50, 25)\nharry = Vault(25, 50, 100)\ntotal = potter + harry\n</code></pre> <p>Objects are mutable.</p> <p>Copying an object is often used. You can use <code>copy</code> module to do that. There are two types of copy</p> <ul> <li> <p>Shallow copy: <code>copy.copy()</code> copies the object and any references it contains, but not the embedded objects.</p> </li> <li> <p>Deep copy: <code>copy.deepcopy()</code> copies not only the object but also the objects it refers to, and the objects they refer to, and so on.</p> </li> </ul>"},{"location":"Learning/Python/#loop","title":"Loop","text":"while loopfor loop <pre><code>i = 0\nwhile i &lt; 3:\n    print(\"meow\")\n    i += 1\n</code></pre> <pre><code># on every iteration of this loop, \n# Python is assigning `i` to the next value in the list behind `in`\nfor i in range(3):          # range(3) returns a list of integers from 0 to 2\n    print(\"hello, world\")\n</code></pre> <p>A forever loop <pre><code>while True:\n    print(\"meow\")\n</code></pre></p> <p>True and False are capitalized in Python</p> <p>You can loop over almost anything that is iterable in Python, such as string. uppercase.py<pre><code>before = input(\"Before: \")\nprint(\"After: \", end=\"\")\nfor c in before:\n    print(c.upper(), end=\"\")    \n    # end is a second parameter here telling print to insert nothing instead of a new line after every output\n    # otherwise each letter will be outputed in seperate lines\nprint()     # move cursor to next line\n</code></pre> Actually if you want to convert a string to uppercase, you just need to call the method <pre><code>before = input(\"Before: \")\nafter = before.upper()\nprint(f\"After: {after}\")\n</code></pre> When you want to implement a <code>do-while</code> loop, just put the loop body into a forever loop and jump out when something happens. In Python, even a for loop can have a <code>else</code> clause. <pre><code>names = [\"Cat\", \"Dog\", \"Bird\"]\n\nname = input(\"Name: \")\n\nfor n in names:\n    if name == n:\n        print(\"Found\")\n        break\nelse:\n    print(\"Not Found\")\n</code></pre> If the for loop terminates without calling <code>break</code>, then the code in <code>else</code> will execute. This is just an example to explain <code>for-else</code>. Actually it can be solved below <pre><code>names = [\"Cat\", \"Dog\", \"Bird\"]\n\nname = input(\"Name: \")\n\nif name in names:\n    print(\"Found\")\nelse:\n    print(\"Not Found\")\n</code></pre></p>"},{"location":"Learning/Python/#function","title":"Function","text":"<p>Practically, we want the main part of a program at the top of the file so that we can dive right in and know what the file is doing. Let's look at the following way. <pre><code>for i in range(3):\n    meow()\n\ndef meow():\n    print(\"meow\")\n</code></pre> If we run the code, we will find a traceback. <pre><code>Traceback (most recent call last):\n  File \"C:\\Users\\86156\\desktop\\meow.py\", line 2, in &lt;module&gt;\n    meow()\n    ^^^^\nNameError: name 'meow' is not defined\n</code></pre> This could happen because the interpreter read the file from top to down and <code>meow()</code> is called before its defination. To solve this problem, we need to define a function called <code>main</code> and takes no arguments. <pre><code>def main():\n    for i in range(3):\n        meow()\n\ndef meow():\n    print(\"meow\")\n\nmain()\n</code></pre> Note that the last line is necessary since the beginning only define the <code>main</code> function and it will be executed only when we call it. Now, let's add some parameter so that we can control the times the loop will run. <pre><code>def main():\n    meow(5)\n\ndef meow(n):\n    for i in range(n):\n        print(\"meow\")\n\nmain()\n</code></pre></p>"},{"location":"Learning/Python/#flow-of-execution","title":"Flow of execution","text":"<p>The order statements run in is called flow of execution. Execution always begin at the first statement of the program. Statements are run one at a time from top to bottom. Function definition don't alter the flow of execution of the program, but remember that statements inside the function don\u2019t run until the function is called. A function call is like a detour in the flow of execution. Instead of going to the next statement, the flow jumps to the body of the function, runs the statements there, and then comes back to pick up where it left off.</p>"},{"location":"Learning/Python/#higher-order-functions","title":"Higher-Order Functions","text":""},{"location":"Learning/Python/#functions-as-arguments","title":"Functions as Arguments","text":"<p>Consider you want to compute the sum of a series. Here are two examples: <pre><code>def sum_naturals(n):\n    total, k = 0, 1\n    while k &lt;= n:\n        total, k = total + k, k + 1\n    return total\n\ndef sum_cubes(n):\n    total, k = 0, 1\n    while k &lt;= n:\n        total, k = total + k*k*k, k + 1\n    return total\n</code></pre> The upper one returns the sum of natural numbers up to n, while the other returns the sum of the cubes of natural numbers. These two functions share a common pattern as following: <pre><code>def &lt;name&gt;(n):\n    total, k = 0, 1\n    while k &lt;= n:\n        total, k = total + &lt;term&gt;(k), k + 1\n    return total\n</code></pre> This remind us that we can pass another parameter <code>term</code>, which is a function, into the sum function so that we don't need to write a paricular function for every series. Let's define our function in the following way: <pre><code>def summation(n, term):\n    total, k = 0, 1\n    while k &lt;= n:\n        total, k = total + term(k), k + 1\n    return total\n\ndef cube(x):\n    return x*x*x\n\ndef identity(x):\n    return x\n\nsum_cube = summation(n, cube)\nsum_natural = summation(n, identity)\n</code></pre> The last two rows compute the two series. We can pass more <code>term</code> function to get the sum of more series.</p>"},{"location":"Learning/Python/#functions-as-general-methods","title":"Functions as General Methods","text":"<p>Consider the following function to compute the golden ratio. <pre><code>def improve(update, close, guess = 1):\n    while not close(guess):\n        guess = update(guess)\n    return guess\n\ndef golden_update(guess):\n    return 1 / guess + 1\n\ndef square_close_to_successor(guess):\n    return approx_eq(guess * guess, guess + 1)\n\ndef approx_eq(x, y, tolerance = 1e-3):\n    return abs(x - y) &lt; tolerance\n\nphi = improve(golden_update, square_close_to_successor)\n</code></pre> The <code>improve</code> function is asgeneral expression of repetitive refinement. It doesn't specify what problem is being solved: those details are left to the <code>update</code> and <code>close</code> functions passed in as arguments. <code>approx_eq</code> returns True when its arguments are close to each other. </p> <p>This example illustrates two related big ideas in computer science. </p> <ul> <li> <p>Naming and functions allow us to abstract away a vast amount of complexity. While each function definition has been trivial, the computational process set in motion by our evaluation procedure is quite intricate. </p> </li> <li> <p>It is only by virtue of the fact that we have an extremely general evaluation procedure for the Python language that small components can be composed into complex processes. Understanding the procedure of interpreting programs allows us to validate and inspect the process we have created.</p> </li> </ul>"},{"location":"Learning/Python/#nested-definitions","title":"Nested Definitions","text":"<p>The above examples demonstrate how the ability to pass functions as arguments significantly enhances the expressive power of our programming language. Each general concept or equation maps onto its own short function. One negative consequence of this approach is that the global frame becomes cluttered with names of small functions, which must all be unique. Another problem is that we are constrained by particular function signatures: the <code>update</code> argument to <code>improve</code> must take exactly one argument. Nested function definitions address both of these problems, but require us to enrich our environment model. The following program compute a square root of a number. <pre><code>def sqrt(a):\n    def sqrt_update(x):\n        return average(x, a/x)\n    def sqrt_close(x):\n        return approx_eq(x * x, a)\n    return improve(sqrt_update, sqrt_close)\n</code></pre> Like local assignment, local <code>def</code> statements only affect the current local frame. These functions are only in scope while <code>sqrt</code> is being evaluated. Consistent with our evaluation procedure, these local <code>def</code> statements don't even get evaluated until <code>sqrt</code> is called.</p> <ul> <li> <p>The names of a local function do not interfere with names external to the function in which it is defined, because the local function name will be bound in the current local environment in which it was defined, rather than the global environment.</p> </li> <li> <p>A local function can access the environment of the enclosing function, because the body of the local function is evaluated in an environment that extends the evaluation environment in which it was defined.</p> </li> </ul> <p>The <code>sqrt_update</code> function carries with it some data: the value for a referenced in the environment in which it was defined. Because they \"enclose\" information in this way, locally defined functions are often called closures.</p>"},{"location":"Learning/Python/#functions-as-returned-values","title":"Functions as Returned Values","text":"<p>We can define function composition like \\(h(x)=f(g(x))\\) using our existing tools: <pre><code>def square(x):\n    return x * x\n\ndef successor(x):\n    return x + 1 \n\ndef composel(f, g):\n    def h(x):\n        return f(g(x))\n    return h\n\nsquare_successor = composel(square, successor)\nresult = square_successor(12)\n</code></pre></p>"},{"location":"Learning/Python/#lambda-expressions","title":"Lambda Expressions","text":"<p><code>lambda</code> expression is an unnamed function, reture a single expression as its body. <pre><code>s = lambda x, y : x * y             # input x and y, return x*y\nt = lambda : None                   # no input and always output None\nu = lambda *args: sum(args)         # input arbitary arguments, reutrn their sum\nv = lambda **kwargs: 1              # input arbitary key-value arguments, return 1\n</code></pre> <code>s</code>, <code>t</code>, <code>u</code> and <code>v</code> are all functions</p>"},{"location":"Learning/Python/#exceptions","title":"Exceptions","text":"<p>In C, we often return some distinct value to signifies the function failed to achieve its goal. In Python, we can use <code>exception</code> to handle it. <pre><code>def main():\n    x = get_int(\"x: \")\n    y = get_int(\"y: \")\n\n    print(x + y)\n\ndef get_int(prompt):\n    return int(input(prompt))\n\nmain()\n</code></pre> In the above program, the <code>get_int()</code> function works well when we input some numbers. But a exception happens when we input other things. <pre><code>python .\\calculator.py\nx: cat\nTraceback (most recent call last):\n  File \"C:\\Users\\86156\\desktop\\calculator.py\", line 10, in &lt;module&gt;\n    main()\n  File \"C:\\Users\\86156\\desktop\\calculator.py\", line 2, in main\n    x = get_int(\"x: \")\n        ^^^^^^^^^^^^^^\n  File \"C:\\Users\\86156\\desktop\\calculator.py\", line 8, in get_int\n    return int(input(prompt))\n           ^^^^^^^^^^^^^^^^^^\nValueError: invalid literal for int() with base 10: 'cat'\n</code></pre> The <code>ValueError</code> tells us what kind of exception we have. We can revise it in this way: get_int()<pre><code>def get_int(prompt):\n    try:\n        return int(input(prompt))\n    except ValueError:\n        print(\"Not an interger\")\n</code></pre> In this program, we use <code>try</code> and <code>except</code> to tell the interpreter we try to do something and if <code>ValueError</code> exception happens, we will see \"Not an integer\" instead of traceback. Only <code>try</code> and <code>except</code> are not enough. We need to add a loop so that the function will loop again and again until get a valid input. get_int()<pre><code>def get_int(prompt):\n    while True:\n        try:\n            return int(input(prompt))\n        except ValueError:\n            print(\"Not an interger\")\n</code></pre> You can also use <code>else</code> as part of try-except block. The statement in <code>else</code> execute when the <code>except</code> not happened. <pre><code>while True:\n    try:\n        x = int(input(\"What's x? \"))\n    except ValueError:\n        print(\"x is not an integer\")\n    else:\n        break\n\nprint(f\"x is {x}\")\n</code></pre></p> <p>You can also use <code>raise</code> to raise an exception. Combine it with <code>try-except</code> can catch the error.</p> <pre><code>class Student:\n    def __init__(self, name, house):\n        if not name:\n            raise ValueError(\"Missing name\")\n        if house not in [\"G\", \"H\", \"R\", \"S\"]\n            raise ValueError(\"Invalid house\")\n        self.name = name\n        self.house = house\n\n\ndef get_student():\n    name = input(\"Name: \")\n    house = input(\"House: \")\n    try:\n        student = Student(name, house)\n    except ValueError:\n        print(\"Please input the correct name and house\")\n        return None\n    else:\n        return student\n</code></pre>"},{"location":"Learning/Python/#unit-tests","title":"Unit Tests","text":"<p>Assume we have a program <code>calculator.py</code> to calculate the square of the input. We want to write a program to automaticaly test whether our program is right. We can use <code>assert</code> to assert the boolean expression following it is True. If True, nothing will happen. But if not, <code>AssertionError</code> will appear on screen</p> calculator.pytest_calculator.py <pre><code>def main():\n    x = int(input(\"What's x? \"))\n    print(\"x squared is\", square(x))\n\n\ndef square(n):\n    return n * n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>from calculator import square\n\ndef main():\n    test_square()\n\ndef test_square():\n    try:\n        assert square(2) == 4\n    except AssertionError:\n        print(\"2 squared was not 4\")\n    try:\n        assert square(3) == 9\n    except AssertionError:\n        print(\"3 squared was not 9\")\n    try:\n        assert square(0) == 0\n    except AssertionError:\n        print(\"0 squared was not 0\")\n    try:\n        assert square(-2) == 4\n    except AssertionError:\n        print(\"-2 squared was not 4\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"Learning/Python/#pytest","title":"pytest","text":"<p>You can also use <code>pytest</code>, is a third-party library that allows you to unit test your program.</p> <p>Run <code>pytest test_calculator.py</code> in your command line, which is shown below, pytest will automatically test it for you.</p> test_calculator.py<pre><code>from calculator import square\n\n\ndef test_positive():\n    assert square(2) == 4\n    assert square(3) == 9\n\n\ndef test_negative():\n    assert square(-2) == 4\n    assert square(-3) == 9\n\n\ndef test_positive():\n    assert square(0) == 0\n</code></pre> <p></p> <p>Let's add some bug in <code>calculator.py</code>, rerun the command and see what will happen. calculator.py<pre><code>def main():\n    x = int(input(\"What's x? \"))\n    print(\"x squared is\", square(x))\n\n\ndef square(n):\n    return n + n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> <p></p> <ul> <li>Unit testing code using multiple tests is so common that you have the ability to run a whole folder of tests with a single command.</li> <li>Add a folder called <code>test</code> with test programs in it. An additional file <code>__init__.py</code> is necessary. Leave it empty and <code>pytest</code> is informed that the whole folder containing <code>__init__.py</code> has tests that can be run.</li> <li>Run <code>pytest test</code> for testing.</li> </ul>"},{"location":"Learning/Python/#list","title":"list","text":"<p>List is something like array but their memory is automatically handled for you. An array is about having contiguously in memory. In Python, a list is more like a linked list. It will allocate memory for you and you don't have to know about pointers and nodes. </p> <pre><code>scores = [72, 73, 33]           # List using square brackets\n# use the bulid-in functions to get the sum and length of the list\naverage = sum(scores) / len(scores)\n</code></pre>"},{"location":"Learning/Python/#traverse","title":"Traverse","text":"<p>You can use following ways to traverse a list <pre><code>names = ['adam', 'bob', 'tony']\n\nfor name in names:\n    print(name)\n\nfor i in range(len(names)):\n    print(names[i])\n</code></pre></p>"},{"location":"Learning/Python/#opeartions","title":"Opeartions","text":"<p>You can use <code>+</code> and <code>*</code> to manipulate lists <pre><code>&gt;&gt;&gt; a = [1, 2, 3]\n&gt;&gt;&gt; b = [4, 5, 6]\n&gt;&gt;&gt; a + b\n[1, 2, 3, 4, 5, 6]\n&gt;&gt;&gt; a * 3\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n</code></pre></p>"},{"location":"Learning/Python/#method","title":"Method","text":"<p><code>append</code> add a element to the end of a list</p> <p><code>extend</code> takes a list as an argument and append its element to the end of the origin list</p> <p><code>sort</code> sort the elements from low to high <pre><code>&gt;&gt;&gt; a = [1, 2, 3]\n&gt;&gt;&gt; a.append(6)\n&gt;&gt;&gt; a\n[1, 2, 3, 6]\n&gt;&gt;&gt; b = [5, 7]\n&gt;&gt;&gt; b.extend(a)\n&gt;&gt;&gt; b\n[5, 7, 1, 2, 3, 6]\n&gt;&gt;&gt; b.sort()\n&gt;&gt;&gt; b\n[1, 2, 3, 5, 6, 7]\n</code></pre> <code>pop</code> delete the element with given index in the list and return that element. If no index is provided, delete the last one.</p> <p><code>remove</code> delete the element with given value and return nothing</p> <p><code>del</code> operator can remove more than one element with slice <pre><code>&gt;&gt;&gt; x = b.pop(2)\n&gt;&gt;&gt; x\n3\n&gt;&gt;&gt; b\n[1, 2, 5, 6, 7]\n&gt;&gt;&gt; b.remove(7)\n&gt;&gt;&gt; b\n[1, 2, 5, 6]\n&gt;&gt;&gt; del b[1:3]\n&gt;&gt;&gt; b\n[1, 6]\n</code></pre></p>"},{"location":"Learning/Python/#dist","title":"dist","text":"<p>Dictionary is essentially a hash tabel, a collection of key-value pairs. <pre><code>people = [\n    {\"name\": \"Carter\", \"number\": \"12345\"},     # dist uses curly brace\n    {\"name\": \"David\", \"number\": \"12335\"},\n    {\"name\": \"John\", \"number\": \"16345\"},\n]\n\nname = input(\"Name: \")\n\nfor person in people:\n    if person[\"name\"] == name:\n        number = person[\"number\"]\n        print(f\"Found {number}\")\n        break\nelse:\n    print(\"Not Found\")\n</code></pre> A big dist can be used, which is tidier in the following case. Note that Python will traverse the keys of the dirtionary. <pre><code>people = {\n    \"Carter\": \"12345\",\n    \"David\": \"12335\",\n    \"John\": \"16345\",\n}\n\nname = input(\"Name: \")\n\nif name in people:              # Python will look for the name among the keys in the dist\n    number = people[name]\n    print(f\"Found {number}\")\nelse:\n    print(\"Not Found\")\n</code></pre> <code>len</code> function returns the number of key-value pairs.</p> <p><code>in</code> operator tells whether something appears as a key in the dist.</p> <p><code>values</code> method returns a collection of values. <pre><code>&gt;&gt;&gt; dist = {\"apple\":\"red\", \"banana\":\"yellow\", \"potato\":\"brown\"}\n&gt;&gt;&gt; len(dist)\n3\n&gt;&gt;&gt; \"apple\" in dist\nTrue\n&gt;&gt;&gt; \"red\" in dist\nFalse\n&gt;&gt;&gt; value = dist.values()\n&gt;&gt;&gt; \"red\" in value\nTrue\n</code></pre></p>"},{"location":"Learning/Python/#tuple","title":"tuple","text":"<p>Tuples are immutable, you can't modify the elements. But you can replace one tuple with another.</p> <pre><code>&gt;&gt;&gt; t = ('a', 'b', 'c', 'd', 'e')\n&gt;&gt;&gt; t = ('A', ) + t[1:]\n&gt;&gt;&gt; t\n('A', 'b', 'c', 'd', 'e')\n</code></pre> <p>Note that one value in parentheses without a comma is not a tuple <pre><code>&gt;&gt;&gt; t = ('A')\n&gt;&gt;&gt; type(t)\n&lt;class 'str'&gt;\n&gt;&gt;&gt; t = ('A', )\n&gt;&gt;&gt; type(t)\n&lt;class 'tuple'&gt;\n</code></pre></p>"},{"location":"Learning/Python/#assignment","title":"Assignment","text":"<p>tuple assignment is more elegant for you can implement in the following way.</p> <pre><code>a, b = b, a\n</code></pre> <p>The left side is a tuple of variables; the right side is a tuple of expressions. Each value is assigned to its respective variable. All the expressions on the right side are evaluated before any of the assignments.</p>"},{"location":"Learning/Python/#tuples-as-return-values","title":"Tuples as return values","text":"<p>Actually a tuple is a list of value separated by comma and we can omit the parentheses although not recommand. </p> <p>In some functions, return values are often separated by comma, which are actually tuples.</p> <pre><code>def min_max(t):\n    return min(t), max(t)\n</code></pre>"},{"location":"Learning/Python/#variable-length-argument-tuples","title":"Variable-length argument tuples","text":"<p>Functions can take a variable number of arguments. A parameter name that begins with a * gathers arguments into a tuple. For example, the following print out arguments as a tuple.</p> <pre><code>def print(*args):\n    print(args)\n</code></pre> <p>The complement of gather is scatter. If you have a sequence of values and you want to pass it to a function as multiple arguments, you can use the * operator. For example:</p> <pre><code>&gt;&gt;&gt; t = (1, 2)\n&gt;&gt;&gt; divmod(t)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: divmod expected 2 arguments, got 1\n&gt;&gt;&gt; divmod(*t)\n(0, 1)\n</code></pre>"},{"location":"Learning/Python/#lists-and-tuples","title":"Lists and tuples","text":"<p><code>zip()</code> is a built-in function that takes two or more sequences and interleaves them.</p> <p>The result is a zip object that knows how to iterate through the pairs. The most common use of <code>zip</code> is in a for loop:</p> <pre><code>&gt;&gt;&gt; l = 'abc'\n&gt;&gt;&gt; n = [1, 2, 3]\n&gt;&gt;&gt; z = zip(l ,n)\n&gt;&gt;&gt; for pair in z:\n...     print(pair)\n...\n('a', 1)\n('b', 2)\n('c', 3)\n</code></pre> <p>A zip object is a kind of iterator, which is any object that iterates through a sequence. Iterators are similar to lists in some ways, but unlike lists, you can\u2019t use an index to select an element from an iterator.</p> <p>If you want to use list operators and methods, you can use a zip object to make a list:</p> <pre><code>&gt;&gt;&gt; list(zip(l, n))\n[('a', 1), ('b', 2), ('c', 3)]\n</code></pre> <p>If you need to traverse the elements of a sequence and their indices, you can use the built-in function <code>enumerate</code>: <pre><code>&gt;&gt;&gt; list(zip(l, n))\n[('a', 1), ('b', 2), ('c', 3)]\n&gt;&gt;&gt; for index, element in enumerate('abc'):\n...     print(index, element)\n...\n0 a\n1 b\n2 c\n</code></pre></p>"},{"location":"Learning/Python/#dicts-and-tuples","title":"Dicts and tuples","text":"<p>Dictionary have a method called <code>items</code> returns a sequence of tuples with key-value pairs. The result is a <code>dist_items</code> object, which is also an iterator.</p> <pre><code>&gt;&gt;&gt; d = {'a':1, 'b':2, 'c':3}\n&gt;&gt;&gt; t = d.items()\n&gt;&gt;&gt; t\ndict_items([('a', 1), ('b', 2), ('c', 3)])\n</code></pre> <p>Combining <code>dict</code> with <code>zip</code> yields a concise way to create a dictionary</p> <pre><code>&gt;&gt;&gt; d = dict(zip('abc', range(3)))\n&gt;&gt;&gt; d\n{'a': 0, 'b': 1, 'c': 2}\n</code></pre>"},{"location":"Learning/Python/#files","title":"Files","text":"<ul> <li>transient program: data disappears when programs end</li> <li>persistent program: some of their data is kept in permanent storage.</li> <li>reading and writing text files or store the state of program in a database are basic way to maintain data.</li> </ul>"},{"location":"Learning/Python/#reading-and-writing","title":"Reading and writing","text":"<p>The built-in function <code>open</code> takes the name of the file as a parameter and returns a file object. </p> <p><code>readline</code> reads a line each time and returns the result as a string. Or you can use <code>readlines</code> to return all lines in a list</p> <pre><code>&gt;&gt;&gt; fin = open('demo.txt')\n&gt;&gt;&gt; line = fin.readline()\n&gt;&gt;&gt; line\n'This is a title.\\n'\n</code></pre> <p>You can also use a file object as part of a for loop</p> <pre><code>fin = open('demo.txt')\nfor line in fin:\n    print(line.strip())     # strip() removes '\\n' in the end of a str\n</code></pre> <p>Open a file with mode <code>w</code> as a second parameter to write a file. If the file already exists, opening it in write mode clears out the old data and starts fresh. You can use mode <code>a</code> if you don't want to erase old thing and just append.</p> <p><code>write</code> write str into the file. Return value is the number of characters that were written.</p> <pre><code>&gt;&gt;&gt; fout = open('demo.txt', 'w')\n&gt;&gt;&gt; line1 = \"This is a title.\\n\"\n&gt;&gt;&gt; fout.write(line1)\n17\n&gt;&gt;&gt; line2 = \"this is a p\"\n&gt;&gt;&gt; fout.write(line2)\n11\n</code></pre> <p>Close the file when operating is completed <pre><code>&gt;&gt;&gt; fout.close()\n</code></pre></p> <p>To avoid forgetting to close the file, you can use the keyword <code>with</code> to do that for you. The following code assign <code>open(\"names.txt\", \"a\")</code> to <code>file</code> and close it when the statement in with is done. <pre><code>with open(\"names.txt\", \"a\") as file:\n    file.write(f\"{name}\\n\")\n</code></pre></p>"},{"location":"Learning/Python/#csv","title":"csv","text":"<p><code>csv</code> stands for Comma-Separated Values. It's a very common convention to store multiple pieces of information that are related in the same file. See the following example.</p> mygo.csvband.py <pre><code>tomori,vocal\nanon,guitar\nrana,guitar\nrikki,drum\nsoyo,bass\n</code></pre> <pre><code>with open(\"mygo.csv\") as file:\n    for line in file:\n        name, pos = line.rstrip().split(\",\")\n        print(f\"{name} is {pos}\")\n</code></pre> <p>We can use functions in <code>csv</code> module to handle this case. <code>csv.reader</code> act similar to above.</p> <pre><code>import csv\n\nwith open(\"mygo.csv\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(f\"{row[0]} is {row[1]}\")\n</code></pre> <p>Now we insert some hints as to what these columns are in the first row and use <code>csv.DistReader</code> to handle it.</p> mygo.csvband.py <pre><code>name,position\ntomori,vocal\nanon,guitar\nrana,guitar\nrikki,drum\nsoyo,bass\n</code></pre> <pre><code>import csv\n\nwith open(\"mygo.csv\") as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        print(f\"{row['name']} is {row['position']}\")\n</code></pre> <p>It makes our code more robust by associating our data with its column title</p> <p>We can also write to <code>csv</code>.</p> writerDictWriter <pre><code>import csv\n\nname = input(\"What's your name? \")\nhome = input(\"Where's your home? \")\n\nwith open(\"student.csv\", \"a\") as file:\n    writer = csv.writer(file)\n    writer.writerow([name, home])\n</code></pre> <pre><code>import csv\n\nname = input(\"What's your name? \")\nhome = input(\"Where's your home? \")\n\nwith open(\"student.csv\", \"a\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"name\", \"home\"])\n    writer.writerow({\"name\": name, \"home\": home})\n</code></pre> <p><code>DictWriter</code> takes two parameters: the <code>file</code> being written to and the <code>fieldnames</code> to write. Further, notice how the <code>writerow</code> function takes a dictionary as its parameter. Quite literally, we are telling the compiler to write a row with two fields called name and home.</p>"},{"location":"Learning/Python/#filenames-and-paths","title":"Filenames and paths","text":"<p><code>os</code> module provides functions for working with files and directories.</p> <p><code>os.getcwd</code> returns the name of the current directory <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.getcwd()\n'C:\\\\Users\\\\86156\\\\Desktop'\n</code></pre></p> <p><code>os.path</code> provides other functions for working with filenames and paths. <pre><code>&gt;&gt;&gt; os.path.abspath('demo.txt')         # find absolute path\n'C:\\\\Users\\\\86156\\\\Desktop\\\\demo.txt'\n&gt;&gt;&gt; os.path.exists('demo.txt')          # check is the file exist\nTrue\n&gt;&gt;&gt; os.path.isdir('demo.txt')           # check is it a directory\nFalse\n</code></pre></p>"},{"location":"Learning/Python/#database","title":"Database","text":"<p>A database is a file that is organized for storing data. Many databases are organized like a dictionary in the sense that they map from keys to values, and it is on disk.</p> <p>The module <code>dbm</code> provides an interface for creating and updating database files.</p> <pre><code>&gt;&gt;&gt; import dbm\n&gt;&gt;&gt; db = dbm.open('MyGO', 'c')\n</code></pre> <p>Mode <code>c</code> means that database should be created if it doesn\u2019t already exist. The result is a database object that can be used (for most operations) like a dictionary.</p> <pre><code>&gt;&gt;&gt; db['vocal'] = 'tomori'      # create a new item\n&gt;&gt;&gt; db['vocal']                 # access the item\nb'tomori'\n</code></pre> <p>The result of access is bytes object.</p> <p>You can iterate it with a for loop</p> <pre><code>for key in db.keys():\n    print(key, db[key])\n</code></pre> <p>Close it when you are done <pre><code>&gt;&gt;&gt; db.close()\n</code></pre></p> <p>A limitation of <code>dbm</code> is that the keys and values have to be strings or bytes.</p> <p>You can use <code>pickle</code> module to transilate any type of object into a string, and then translate the string back into objects.</p> <ul> <li><code>pickle.dumps</code> takes an object and returns a string representation.</li> <li><code>pickle.loads</code> reconstitues the object.</li> </ul> <pre><code>&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; t = [1, 2 ,3]\n&gt;&gt;&gt; t1 = pickle.dumps(t)\n&gt;&gt;&gt; t1\nb'\\x80\\x04\\x95\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00]\\x94(K\\x01K\\x02K\\x03e.'\n&gt;&gt;&gt; t2 = pickle.loads(t1)\n&gt;&gt;&gt; t2\n[1, 2, 3]\n</code></pre>"},{"location":"Learning/Python/#writing-modules","title":"Writing modules","text":"<p>Any file that contains Python code can be imported as a module.</p> <p>Suppose you have a file like this</p> demo.py<pre><code>def add(a, b):\n    return a+b\n\nprint(\"This is a add function\")\n</code></pre> <p>You can import it like this. </p> <pre><code>&gt;&gt;&gt; import demo\nThis is a add function\n</code></pre> <p>Now you have a module object <code>demo</code> and you can use function in it.</p> <pre><code>&gt;&gt;&gt; demo\n&lt;module 'demo' from 'C:\\\\Users\\\\86156\\\\Desktop\\\\demo.py'&gt;\n&gt;&gt;&gt; demo.add(3, 4)\n7\n</code></pre> <p>The test code in the module will run when you import it, but normally we only want the defination of functions. We can add the following idiom:</p> <pre><code>if __name__ == '__main__':\n    print(\"This is a add function\")\n</code></pre> <p><code>__name__</code> is a built-in variable that is set when the program starts. If the program is running as a script, <code>__name__</code> has the value <code>'__main__'</code>; in that case, the test code runs. Otherwise, if the module is being imported, the test code is skipped.</p>"},{"location":"Learning/Python/#other","title":"Other","text":""},{"location":"Learning/Python/#some-operators","title":"Some operators","text":""},{"location":"Learning/Python/#floor-division-and-modulus","title":"Floor division and modulus","text":"<p>The division sign <code>/</code> divides two numbers to a float, while floor division <code>//</code> returns an integer rounded down by that float. The modulus operator <code>%</code> divides two numbers and return s the remainder. <pre><code>&gt;&gt;&gt; 32 / 10\n3.2\n&gt;&gt;&gt; 32 // 10\n3\n&gt;&gt;&gt; 32 % 10\n2\n</code></pre></p>"},{"location":"Learning/Python/#logical-operator","title":"Logical operator","text":"<p>Python use <code>and</code>, <code>or</code> and <code>not</code> these three keywords to do logical operation.</p>"},{"location":"Learning/Python/#truncation-imprecision-and-overflow","title":"truncation, imprecision and overflow","text":"<p>Python won't truncate the result if it has fractional component. It will convert type automatically. calculator.py<pre><code>x = int(input(\"x: \"))\ny = int(input(\"y: \"))\nz = x / y\nprint(z)\n</code></pre> Run the code <pre><code>python .\\calculator.py\nx: 1\ny: 3\n0.3333333333333333\n</code></pre> We can use f string to show more digits after the decimal point <pre><code>x = int(input(\"x: \"))\ny = int(input(\"y: \"))\nz = x / y\nprint(f\"{z:.50f}\")\n</code></pre> The result below shows that the floating point imprecision problem remains in Python <pre><code>python .\\calculator.py\nx: 1\ny: 3\n0.33333333333333331482961625624739099293947219848633\n</code></pre> In Python, integer won't overflow no matter how big it is because Python will reserve more and more memeory for that integer to fit it.</p>"},{"location":"Learning/Python/#sys","title":"sys","text":"<p>The sys library has system-related functionality.  In C, we got access to command-line arguments with <code>main()</code>, <code>argc</code> and <code>argv</code>. You can do command-line arguments in Python with the help of <code>sys</code> <pre><code>from sys import argv\n\nif len(argv) == 2:\n    print(f\"hello, {argv[1]}\")\nelse:\n    print(\"hello, world\")\n</code></pre> If you give one argument, the output is <code>hello, world</code>. If two, then different.  Note that the command <code>python</code> is ignored from <code>argv</code> <pre><code>$ python greet.py\nhello, world\n$ python greet.py David\nhello, David\n</code></pre> You can also exit the program with <code>sys</code> <pre><code>import sys\n\nif len(sys.argv) != 2:\n    print(\"Missing command-line argument\")\n    sys.exit(1)\n\nprint(f\"hello, {sys.argv[1]}\")\nsys.exit(0)\n</code></pre></p>"},{"location":"Learning/Python/#pip","title":"pip","text":"<p>Using <code>pip</code> to install third-party libraries.</p>"},{"location":"Learning/CV/4DGS/","title":"4 Dimension Gaussian Splatting","text":""},{"location":"Learning/CV/4DGS/#gaussian-distribution","title":"Gaussian Distribution","text":""},{"location":"Learning/CV/4DGS/#covariance","title":"Covariance","text":"<p>For a given random vector \\(X:\\Omega \\rightarrow R^{n}\\), its covariance matrix \\(\\Sigma\\) is the \\(n\\times n\\) square matrix whose extries are given by \\(\\Sigma_{ij}=Cov[X_{i},X_{j}]=E[(X_{i}-E[X_{i}])(X_{j}-E[X_{j}])]\\)</p> \\[ \\Sigma=E[(X-E[X])(X-E[X])^{T}]  \\] <p>Proposition: Suppose that \\(\\Sigma\\) is the covariance matrix corresponding to some random vector \\(X\\). Then \\(\\Sigma\\) is symmetric positive semidefinite</p> <p>positive semidefinite</p> <p>A symmetric matrix \\(A\\in S^{n}\\) is positive semidefinite (PSD) if for all vectors \\(x^{T}Ax\\geq 0\\)</p> <p>\\(\\textit{proof}\\text{: For any vector }z\\in R^{n} \\text{, observe that}\\)</p> \\[ z^{T}\\Sigma z= \\sum_{i=1}^{n}\\sum_{j=1}^{n}\\Sigma_{ij}z_{i}z_{j}\\\\ =E[\\sum_{i=1}^{n}\\sum_{j=1}^{n}(X_{i}-E[X_{i}])(X_{j}-E[X_{j}])z_{i}z_{j}] \\] <p>\\(\\text{Observe that the quantity inside the brackets is of the form}\\) </p> \\[ \\sum_{i=1}^{n}\\sum_{j=1}^{n}x_{i}x_{j}z_{i}z_{j}=(x^{T}z)^{2}\\geq 0\\\\ \\] <p>\\(\\text{, which complete the proof}\\)</p>"},{"location":"Learning/CV/4DGS/#gaussian-distribution_1","title":"Gaussian Distribution","text":"<ul> <li>\\(X\\sim Normal(\\mu,\\sigma^{2})\\): also known as univariate Gaussian distribution</li> </ul> \\[ f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2}} \\] <ul> <li>\\(X\\sim \\mathcal{N}(\\mu,\\Sigma)\\) For multivariate Gaussian, where mean \\(\\mu\\in R^{d}\\) and covariance matrix \\(\\Sigma\\) is symmetric positive de\ufb01nite \\(d\\times d\\) matrix </li> </ul> \\[ f_{X_{1},X_{2},\\dots,X_{d}}(x_1,x_2,\\dots,x_{d};\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)) \\] <p>Proposition: \\(\\Sigma=E[(X-\\mu)(X-\\mu)^{T}]=E[X^{2}]-\\mu\\mu^{T}\\)</p> <p>Note</p> <p>In the definition of multivariate Gaussians, we required that the covariance matrix \\(\\Sigma\\) be symmetric positive definite. That's because we need \\(\\Sigma^{-1}\\) exists, which means \\(\\Sigma\\) is full rank. Since any full rank symmetric positive semidefinite matrix is necessarily symmetric positive definite, it follows that \\(\\Sigma\\) must be symmetric positive definite.</p>"},{"location":"Learning/CV/4DGS/#isocontour","title":"Isocontour","text":"<p>Note</p> <p>For a function \\(f : R^2 \u2192 R\\), an isocontour is a set of the form \\(\\{x\\in R^{2}:f(x)=c\\}\\) for some \\(c\\in R\\)</p> <p>Consider the case where \\(d=2\\) and \\(\\Sigma\\) is diagonal. For some constant \\(c\\in R\\) and all \\(x1,x2\\in R\\), we have</p> \\[ \\begin{aligned} c&amp;=\\frac{1}{2\\pi \\sigma_{1}\\sigma_{2}}\\exp{(-\\frac{1}{2\\sigma_{1}^{2}}(x_{1}-\\mu_{1})^{2}-\\frac{1}{2\\sigma_{2}^{2}}(x_{2}-\\mu_{2})^{2})} \\\\ 1&amp;=\\frac{(x_{1}-\\mu_{1})^{2}}{2\\sigma_{1}^{2}\\log{\\frac{1}{2\\pi c\\sigma_{1}\\sigma_{2}}}}+\\frac{(x_{2}-\\mu_{2})^{2}}{2\\sigma_{2}^{2}\\log{\\frac{1}{2\\pi c\\sigma_{1}\\sigma_{2}}}} \\end{aligned} \\] <p>Defining</p> \\[ r_{1}=\\sqrt{2\\sigma_{1}^{2}\\log{(\\frac{1}{2\\pi c\\sigma_{1}\\sigma_{2}})}}\\\\ r_{2}=\\sqrt{2\\sigma_{2}^{2}\\log{(\\frac{1}{2\\pi c\\sigma_{1}\\sigma_{2}})}}\\\\ \\] <p>It follows that</p> \\[ 1=\\left(\\frac{x_{1}-\\mu_{1}}{r_{1}}\\right)^{2}+\\left(\\frac{x_{2}-\\mu_{2}}{r_{2}}\\right)^{2} \\] <p>When \\(\\Sigma\\) is diagonal, it is the equation of an axis-aligned ellipse, while a rotated ellipses when not diagonal.</p> <p>In \\(d\\) dimensional case, the level sets form geometrical structures known as ellipsoid in \\(R^{d}\\)</p>"},{"location":"Learning/CV/4DGS/#closure-properties","title":"Closure properties","text":"<p>\\(\\textbf{Theorem:}\\text{ Suppose that }y\\sim\\mathcal{N}(\\mu,\\Sigma)\\text{ and }z\\sim\\mathcal{N}(\\mu',\\Sigma')\\text{ are independent }\\\\ \\text{Gaussian distributed random variables, where }\\mu,\\mu'\\in R^{d}\\text{ and }\\Sigma,\\Sigma'\\in S^{d}_{++}\\text{. Then, their sum is also Gaussian:}\\)</p> \\[ y+z\\sim \\mathcal{N}(\\mu+\\mu',\\Sigma+\\Sigma')\\\\ \\] <p>\\(\\textbf{Theorem:}\\text{ Suppose that }\\)</p> \\[ \\begin{bmatrix} x_{A} \\\\ x_{B} \\end{bmatrix} \\sim\\mathcal{N} \\left( \\begin{bmatrix} x_{A} \\\\ x_{B} \\end{bmatrix} , \\begin{bmatrix} \\Sigma_{AA} &amp; \\Sigma_{AB}\\\\ \\Sigma_{BA} &amp; \\Sigma_{BB} \\end{bmatrix} \\right) \\] <p>\\(\\text{where }x_{A}\\in R^{n},x_{B}\\in R^{d}\\text{ and the dimensions of the mean vectors and covariance matrix subblocks are chosen }\\\\ \\text{to match }x_{A}\\text{ and }x_{B}\\text{. Then, the marginal densities are Gaussian:}\\)</p> \\[ \\begin{aligned} p(x_A)&amp;=\\int_{x_{B}\\in R^{d}}p(x_{A},x_{B};\\mu,\\Sigma)dx_{B}\\\\ p(x_B)&amp;=\\int_{x_{A}\\in R^{n}}p(x_{A},x_{B};\\mu,\\Sigma)dx_{A}\\\\ \\\\ x_{A}&amp;\\sim \\mathcal{N}(\\mu_{A},\\Sigma_{AA})\\\\ x_{B}&amp;\\sim \\mathcal{N}(\\mu_{B},\\Sigma_{BB}) \\end{aligned} \\] <p>\\(\\textbf{Theorem:}\\text{ Suppose that }\\)</p> \\[ \\begin{bmatrix} x_{A}\\\\ x_{B}\\\\ \\end{bmatrix} \\sim\\mathcal{N} \\begin{pmatrix} \\begin{bmatrix} x_{A}\\\\ x_{B}\\\\ \\end{bmatrix} , \\begin{bmatrix} \\Sigma_{AA}&amp;\\Sigma_{AB}\\\\ \\Sigma_{BA}&amp;\\Sigma_{BB}\\\\ \\end{bmatrix} \\end{pmatrix} \\] <p>\\(\\text{where }x_{A}\\in R^{n},x_{B}\\in R^{d}\\text{ and the dimensions of the mean vectors and covariance matrix subblocks are chosen }\\\\ \\text{to match }x_{A}\\text{ and }x_{B}\\text{. Then, the conditional densities are Gaussian:}\\)</p> \\[ \\begin{aligned} p(x_{A}|x_{B})&amp;=\\frac{p(x_{A},x_{B};\\mu,\\Sigma)}{\\int_{x_{A}\\in R^{n}}p(x_{A},x_{B};\\mu,\\Sigma)dx_{A}}\\\\ p(x_B|x_{A})&amp;=\\frac{p(x_{A},x_{B};\\mu,\\Sigma)}{\\int_{x_{B}\\in R^{d}}p(x_{A},x_{B};\\mu,\\Sigma)dx_{B}}\\\\ \\\\ x_{A}|x_{B}\\sim \\mathcal{N}(\\mu_{A}+\\Sigma_{AB}&amp;\\Sigma_{BB}^{-1}(x_{B}-\\mu_{B}),\\Sigma_{AA}-\\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})\\\\ x_{B}|x_{A}\\sim \\mathcal{N}(\\mu_{B}+\\Sigma_{BA}&amp;\\Sigma_{AA}^{-1}(x_{A}-\\mu_{A}),\\Sigma_{BB}-\\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB}) \\end{aligned} \\]"},{"location":"Learning/CV/4DGS/#decomposition","title":"Decomposition","text":"<p>Given that the covariance matrix is positive definite, it can be diagonalize in the following format:</p> \\[ \\begin{aligned} \\Sigma&amp;=U\\Lambda U^{T}\\\\ &amp;=(U\\Lambda^{1/2})(\\Lambda^{1/2}U)^{T}\\\\ &amp;=AA^{T} \\end{aligned} \\] <p>\\(\\Lambda\\) here is a diagonal matrix. We can consider it as a scaling matrix \\(S\\) and \\(U\\) as a rotation matrix \\(R\\).</p> \\[ \\Sigma=RSS^{T}T^{T} \\] <p>When optimization \\(\\Sigma\\), the gradient descent may not preserve the property of positive definite. We can equivalently optimize \\(R\\) and \\(S\\) separately, using a \\(3D\\) vecter \\(s\\) for scaling and a quaternion \\(q\\) to represent rotation. Making sure to normalize \\(q\\) to obtain a valid unit quaternion.</p> \\[ \\mathbf{q}=q_r+q_i\\cdot i+q_j\\cdot j+q_k\\cdot k\\\\ ~\\\\ \\mathbf{R(q)}=2 \\begin{pmatrix} \\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\\\ (q_{i}q_{j}+q_{r}q_{k})&amp;\\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\\\ (q_{i}q_{k}-q_{r}q_{j})&amp;(q_{j}q_{k}+q_{r}q_{i})&amp;\\frac{1}{2}-(q_{i}^{2}+q_{j}^{2}) \\end{pmatrix} \\]"},{"location":"Learning/CV/4DGS/#volumn-rendering","title":"Volumn Rendering","text":"<p>backward mapping algorithm: shoot rays through pixels on the image plane into the volume data</p> <p>forward mapping alogorithm: map the data onto the image plane</p>"},{"location":"Learning/CV/4DGS/#rendering-pipeline","title":"rendering pipeline","text":""},{"location":"Learning/CV/4DGS/#splatting-alogorithm","title":"Splatting Alogorithm","text":"<p>We denote a point in ray space by a column vector of three coordinates \\(\\mathbf{x}=(x_0,x_1,x_2)^{T}\\). The coordinates \\(x_0\\) and \\(x_1\\) specify a point on the projection plane and \\(x_2\\) specifes the Euclidean distance from the center of projection to a point on the viewing ray. Here we use the abbreviation \\(\\mathbf{\\hat{x}}=(x_0,x_1)^{T}\\).</p> <p>The volume rendering equation describes the light intensity \\(I_{\\lambda}(\\hat{\\mathbf{x}})\\) at wavelength \\(\\lambda\\) that reaches the center of projection along the ray \\(\\hat{\\mathbf{x}}\\) with length \\(L\\):</p> \\[ I_{\\lambda}(\\hat{\\mathbf{x}})=\\int_{0}^{L}c_{\\lambda}(\\hat{\\mathbf{x}},\\xi)g(\\hat{\\mathbf{x}},\\xi)e^{-\\int_{0}^{\\xi}g(\\hat{\\mathbf{x}},\\mu)d\\mu}d\\xi \\] <ul> <li> <p>\\(g(\\mathbf{x})\\) is the extinction function that de\ufb01nes the rate of light occlusion, and \\(c_{\\lambda}(\\mathbf{x})\\) is an emission coef\ufb01cient. </p> </li> <li> <p>\\(c_{\\lambda}(\\mathbf{x})g(\\mathbf{x})\\) describes the light intensity scattered in the direction of the ray \\(\\hat{\\mathbf{x}}\\) at the point \\(x_2\\). </p> </li> <li> <p>The exponential term can be interpreted as an attenuation factor.</p> </li> </ul> <p>Now let's make several assumptions</p> <ul> <li> <p>The volume consists of individual particles that absorb and emit light. Given weight cofficient \\(g_k\\) and reconstruction kernels \\(r_{k}(\\mathbf{x})\\), we have \\(g(\\mathbf{x})=\\sum_{k}g_kr_{k}(\\mathbf{x})\\)</p> </li> <li> <p>Local support areas do not overlap along a ray \\(\\hat{\\mathbf{x}}\\), and the reconstruction kernels are ordered front to back. </p> </li> <li> <p>The emission coef\ufb01cient is constant in the support of each reconstruction kernel along a ray, hence we use the notation \\(c_{\\lambda k}(\\hat{\\mathbf{x}})=c_{\\lambda}(\\hat{\\mathbf{x}},\\xi)\\)</p> </li> <li> <p>Approximate the exponential function with the \ufb01rst two terms of its Taylor expansion, thus \\(e^{x}\\approx 1-x\\)</p> </li> <li> <p>Ignore self-occlusion</p> </li> </ul> <p>Exploiting these assumptions, yielding: </p> \\[ I_{\\lambda}(\\hat{\\mathbf{x}})=\\sum_{k}c_{\\lambda k}(\\hat{\\mathbf{x}})g_kr_{k}(\\mathbf{x})\\prod_{j=0}^{k-1}(1-g_{j}q_{j}(\\hat{\\mathbf{x}})) \\] <p>where \\(q_{j}(\\hat{\\mathbf{x}})\\) denotes an integrated reconstruction kernel, hence:</p> \\[ q_{j}(\\hat{\\mathbf{x}})=\\int_{\\mathbb{R}}r_{k}(\\hat{\\mathbf{x}},x_{2})dx_{2} \\]"},{"location":"Learning/CV/4DGS/#the-viewing-transformation","title":"The Viewing Transformation","text":"<p>Denote the Gaussian reconstruction kernels in object space by \\(r_{k}''(\\mathbf{t})=\\mathcal{G}_{\\mathbf{V''}}(\\mathbf{t}-\\mathbf{t_k})\\), where \\(\\mathbf{t_k}\\) are the voxel positions of center of kernel.</p> <p>Denote camera coordinates by a vector \\(\\mathbf{u}=(u_0,u_1,u_2)^{T}\\). Object coordinates are transformed to camera coordinates using an af\ufb01ne mapping \\(\\mathbf{u}=\\varphi(\\mathbf{t})=\\mathbf{Wt+d}\\), called viewing tranformation.</p> <p>Now we can transform the reconstruction kernels \\(\\mathcal{G}_{\\mathbf{V''}}(\\mathbf{t}-\\mathbf{t_k})\\) to camera space: </p> \\[ \\mathcal{G}_{\\mathbf{V''}}(\\varphi^{-1}(\\mathbf{u})-\\mathbf{t_k})=\\frac{1}{|\\mathbf{W}^{-1}|}\\mathcal{G}_{\\mathbf{V'}_{k}}(\\mathbf{u}-\\mathbf{u_k})=r_{k}'(\\mathbf{u}) \\] <p>where \\(\\mathbf{u_k}=\\varphi(\\mathbf{t_k})\\) is the center of the Gaussian in camera coordinates and \\(\\mathbf{V'}_{k}=\\mathbf{W}\\mathbf{V''}_{k}\\mathbf{W}^{T}\\) is the variance matrix in camera coordinates.</p>"},{"location":"Learning/CV/4DGS/#the-projective-transformation","title":"The Projective Transformation","text":"<p>In camera space, The ray intersecting the center of projection and the point \\((x_0, x_1)\\) on the projection plane is called a viewing ray.</p> <p>To facilitate analiytical integration of volumn function, we need to transform the camera space to ray space such that the viewing rays are parallel to a coordinate axis. The projective transformation converts camera coordinates to ray coordinates.</p> <p>Camera space is de\ufb01ned such that the origin of the camera coordinate system is at the center of projection and the projection plane is the plane \\(u_{2}=1\\). Camera space and ray space are related by the mapping \\(\\mathbf{x=m(u)}\\). </p> \\[ \\begin{pmatrix} x_0\\\\ x_1\\\\ x_2\\\\ \\end{pmatrix} =\\mathbf{m(u)}= \\begin{pmatrix} u_{0}/u_{2}\\\\ u_{1}/u_{2}\\\\ \\Vert(u_{0},u_{1},u_{2})^{T}\\Vert\\\\ \\end{pmatrix}\\\\ \\begin{pmatrix} u_0\\\\ u_1\\\\ u_2\\\\ \\end{pmatrix} =\\mathbf{m^{-1}(u)}= \\begin{pmatrix} x_{0}/l\\cdot x_2\\\\ x_{1}/l\\cdot x_2\\\\ 1/l\\cdot x_2\\\\ \\end{pmatrix} \\] <p>where \\(l=\\Vert(x_{0},x_{1},1)^{T}\\Vert\\).</p> <p>Unfortunately, these mappings are not affine. The Gaussian after the transformation may not still Gaussian. To solve this problem, we introduce the local affine approximation \\(m_{uk}\\) of the projective transformation. It is defined by the first two terms of the Taylor expansion of \\(\\mathbf{m}\\) at the point \\(\\mathbf{u}_k\\):</p> \\[ \\mathbf{m_{u_k}(u)=x_k+J_{u_k}\\cdot (u-u_k)}\\\\ \\mathbf{J_{u_k}}={\\frac{\\partial \\mathbf{m}}{\\partial \\mathbf{u}}}(\\mathbf{u}_k) \\] <p>where \\(\\mathbf{x}_k=\\mathbf{m(u_k)}\\) is the center of a Gaussian in ray space and the Jacobian \\(\\mathbf{J_{u_k}}\\) is given by the partial derivatives of \\(\\mathbf{m}\\) at the point \\(\\mathbf{u}_k\\).</p> <p>This yields the local affine approximation of reconstruction kernels to ray space:</p> \\[ \\begin{aligned} r_{k}(\\mathbf{x})&amp;=\\frac{1}{|\\mathbf{W}^{-1}|}\\mathcal{G}_{\\mathbf{V'}_{k}}(\\mathbf{m^{-1}(x)}-\\mathbf{u_k})\\\\ &amp;=\\frac{1}{|\\mathbf{W}^{-1}||\\mathbf{J}^{-1}|}\\mathcal{G}_{\\mathbf{V}_{k}}(\\mathbf{x}-\\mathbf{x_k}) \\end{aligned} \\] <p>where \\(\\mathbf{V}_k\\) is the variance matrix in ray coordinates:</p> \\[ \\begin{aligned} \\mathbf{V}_{k}&amp;=\\mathbf{J}\\mathbf{V'}_{k}\\mathbf{J}^{T}\\\\ &amp;=\\mathbf{JW}\\mathbf{V''}_{k}\\mathbf{W}^{T}\\mathbf{J}^{T} \\end{aligned} \\]"},{"location":"Learning/CV/4DGS/#spherical-harmonic-functions","title":"Spherical Harmonic Functions","text":"<p>Spherical harmonics form an orthogonal basis for functions defined over the sphere, with low degree harmonics encoding smooth (more Lambertian) changes in color and higher degree harmonics encoding higher-frequency (more specular) effects.</p> \\[ f(t)\\approx \\sum_{l}\\sum_{m=-l}^{l}c_{l}^{m}Y_{l}^{m}(\\theta,\\phi) \\] <ul> <li> <p>\\(l\\): the degree (non-negative integer)</p> </li> <li> <p>\\(m\\): the order (integer such that \\(\u2212l\\leq m\\leq l\\))</p> </li> <li> <p>\\(c_{l}^{m}\\): SH coefficients</p> </li> </ul> \\[ c_{l}^{m}=\\int_{\\Omega}f(w)Y_{l}^{m}(w)dw \\] <ul> <li>\\(Y_{l}^{m}\\): SH functions, where \\(P_{l}^{m}\\) are the associated Legendre polynomials, \\(\\theta\\) is the colatitude(0 to \\(\\pi\\)), and \\(\\phi\\) is the longitude(0 to \\(2\\pi\\))</li> </ul> \\[ Y_{l}^{m}=\\sqrt{\\frac{(2l+1)(l-m)!}{4\\pi (l+m)!}}P_{l}^{m}(cos\\theta)e^{im\\phi} \\]"},{"location":"Learning/CV/4DGS/#structural-similarity-index-measure","title":"Structural Similarity Index Measure","text":"<p>The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. It is also used for measuring the similarity between two images. </p>"},{"location":"Learning/CV/4DGS/#algorithm","title":"Algorithm","text":"<p>The SSIM index is calculated on various windows of an image. The SSIM formula is based on three comparison measurements between two window \\(x\\) and \\(y\\) of common size \\(N\\times N\\) is: luminance (\\(l\\)), contrast (\\(c\\)) and structure (\\(s\\)). The individual comparison functions are:</p> \\[ l(x,y)=\\frac{2\\mu_{x}\\mu_{y}+c_1}{\\mu_{x}^{2}+\\mu_{y}^{2}+c_1}\\\\ c(x,y)=\\frac{2\\sigma_{x}\\sigma_{y}+c_2}{\\sigma_{x}^{2}+\\sigma_{y}^{2}+c_2}\\\\ s(x,y)=\\frac{\\sigma_{xy}+c_3}{\\sigma_{x}\\sigma_{y}+c_3} \\] <p>where:</p> <ul> <li> <p>\\(\\mu_{x}\\) the pixel sample mean of \\(x\\)</p> </li> <li> <p>\\(\\mu_{y}\\) the pixel sample mean of \\(y\\)</p> </li> <li> <p>\\(\\sigma_{x}^{2}\\) the variance of \\(x\\)</p> </li> <li> <p>\\(\\sigma_{y}^{2}\\) the variance of \\(x\\)</p> </li> <li> <p>\\(\\sigma_{xy}\\) the covariance of \\(x\\) and \\(y\\)</p> </li> <li> <p>\\(c_{1}=(k_1L)^{2},c_{2}=(k_2L)^2=2c_3\\) two variables to stabilize the division with weak denominator</p> </li> <li> <p>\\(L\\) the dynamic range of the pixel-values (typically is \\(2^{\\#bits per pixel}-1\\))</p> </li> <li> <p>\\(k_1=0.01\\) and \\(k_2=0.03\\) by default.</p> </li> </ul> <p>SSIM is then a weighted combination of those comparative measures:</p> \\[ SSIM(x,y)=l(x,y)^{\\alpha}\\cdot c(x,y)^{\\beta} \\cdot s(x,y)^{\\gamma} \\] <p>Setting the weights \\(\\alpha,\\beta,\\gamma\\) to 1, the formula can be reduced to the form below:</p> \\[ SSIM(x,y)=\\frac{(2\\mu_{x}\\mu_{y}+c_1)(2\\sigma_{xy}+c_2)}{(\\mu_{x}^{2}+\\mu_{y}^{2}+c_1)(\\sigma_{x}^{2}+\\sigma_{y}^{2}+c_2)} \\]"},{"location":"Learning/CV/4DGS/#structural-dissimilarity","title":"Structural Dissimilarity","text":"<p>Structural dissimilarity (DSSIM) may be derived from SSIM, though it does not constitute a distance function as the triangle inequality is not necessarily satisfied.</p> \\[ DSSIM(x,y)=\\frac{1-SSIM(x,y)}{2} \\]"},{"location":"Learning/CV/ML/","title":"ML4360 - Computer vision","text":""},{"location":"Learning/CV/ML/#direct-linear-transform","title":"Direct Linear Transform","text":"\\[ Let \\ \\mathcal X =\\{\\tilde{x}_{i},\\tilde{x}^{'}_{i}\\} \\ denote \\ a \\ set \\ of \\ 2D-2D \\ correspondences. \\\\ \\] <p>Concatenate \\(A_{i}\\) into single \\(2n\\times 9\\) matrix \\(A\\) leads to the following constrained least squares problem</p> \\[ \\begin{aligned} \\tilde{h}^{*}&amp;=\\mathop{\\arg\\min}\\limits_{\\tilde{h}} \\ ||A\\tilde{h}||_{2}^{2}+\\lambda(||\\tilde{h}||_{2}^{2}-1) \\\\ &amp;=\\mathop{\\arg\\min}\\limits_{\\tilde{h}} \\ \\tilde{h}^{T}A^{T}A\\tilde{h}+\\lambda(\\tilde{h}^{T}\\tilde{h}-1) \\end{aligned} \\] <p>where we have fixed \\(||\\tilde{h}||_{2}^{2}=1\\) as \\(\\tilde{H}\\) is is homogeneous (i.e., de\ufb01ned only up to scale).</p> <p>The solution to the above optimization problem is the singular vector corresponding to the smallest singular value of A. The resulting algorithm is called Direct Linear Transformation.</p>"},{"location":"Learning/CV/ML/#singular-value-decomposition","title":"Singular Value Decomposition","text":"<p>The SVD of \\(m \\times n\\) matrix \\(A\\) is given by \\(A=U\\Sigma V^{T}=\\sum_{i=1}^{r}u_{i}\\sigma _{i}v_{i}^{T}\\)</p> <p>where:</p> <ul> <li> <p>\\(U\\): \\(m\\times m\\) orthogonal  matrix of the orthonormal eigenvectors of \\(AA^{T}\\)</p> </li> <li> <p>\\(V^{T}\\): transpose of a \\(n\\times n\\) orthogonal matrix containing the orthonormal eigenvectors of \\(A^{T}A\\)</p> </li> <li> <p>\\(\\Sigma\\): a \\(m\\times n\\) matrix, actually a diagonal matrix with \\(r\\) elements equal to the root of the positive eigenvalues of \\(AA^{T}\\) or \\(A^{T}A\\) (both matrics have the same positive eigenvalues anyway) and \\(m-r\\) extra zero rows and \\(n-r\\) new zero columns.</p> </li> </ul>"},{"location":"Learning/CV/ML/#pseudoinverse-matrix","title":"Pseudoinverse Matrix","text":""},{"location":"Learning/CV/ML/#definition","title":"Definition","text":"<p>For \\(A \\in \\mathbb{K}^{m\\times n}\\), a pseudoinverse of \\(A\\) is defined as a matrix  \\({\\displaystyle A^{+}\\in \\mathbb {K} ^{n\\times m}}\\) satisfying all of the following four criteria, known as the Moore\u2013Penrose conditions:</p> <ul> <li>\\({\\displaystyle AA^{+}}\\) need not be the general identity matrix, but it maps all column vectors of A to themselves:</li> </ul> \\[ {\\displaystyle AA^{+}A=\\;A.} \\] <ul> <li>\\({\\displaystyle A^{+}}\\) acts like a weak inverse:</li> </ul> \\[ {\\displaystyle A^{+}AA^{+}=\\;A^{+}.} \\] <ul> <li>\\({\\displaystyle AA^{+}}\\) is Hermitian. Hermitian matrix is a complex square matrix that is equal to its own conjugate transpose.</li> </ul> \\[ A\\ Hermitian\\Longleftrightarrow A=\\overline{A^{T}}=A^{H} \\] <ul> <li>\\({\\displaystyle A^{+}A}\\) is also Hermitian.</li> </ul>"},{"location":"Learning/CV/ML/#linear-least-squares","title":"Linear least-squares","text":"<p>The pseudoinverse provides a least squares solution to a system of linear equations.For \\({\\displaystyle A\\in \\mathbb {K} ^{m\\times n}}\\), given a system of linear equations \\(Ax=b\\), in general, a vector \\({\\displaystyle x}\\) that solves the system may not exist, or if one does exist, it may not be unique. More specifically, a solution exists if and only if \\({\\displaystyle b}\\) is in the image of \\({\\displaystyle A}\\), and is unique if and only if \\({\\displaystyle A}\\) is injective. The pseudoinverse solves the \"least-squares\" problem as follows:</p> <ul> <li> <p>\\(\\forall x \\in \\mathbb{K}^{n}\\), we want to find a \\(z\\) satisfying \\(\\Vert{Az-b}\\Vert_{2}\\leq \\Vert{Ax-b}\\Vert_{2}\\), where \\(z=A^{+}b\\) and \\(\\Vert \\cdot \\Vert_{2}\\) denotes the Euclidean norm.</p> </li> <li> <p>This weak inequality holds with equality if and only if \\({\\displaystyle x=A^{+}b+\\left(I-A^{+}A\\right)w}\\) for any vector \\({\\displaystyle w}\\); this provides an infinitude of minimizing solutions unless \\({\\displaystyle A}\\) has full column rank, in which case \\({\\displaystyle \\left(I-A^{+}A\\right)}\\) is a zero matrix.</p> </li> </ul> <p>Assume we want to solve \\(n\\) in \\(Sn=I\\) where S is a real matrix. A solution may not unique. We can use pseudoinverse to solve it.</p> \\[ S^{T}I=S^{T}Sn \\\\ n=(S^{T}S)^{-1}S^{T}I \\] <p>\\((S^{T}S)^{-1}S^{T}\\) here is pesudoinverse. * Reference: wikipedia</p>"},{"location":"Learning/CV/ML/#chamfer-distance","title":"Chamfer Distance","text":"<p>Chamfer Distance is widely used in 3D reconstruction to judge how close one point cloud is on average to the other.</p> <p>Given two point clouds \\(X\\) and \\(Y\\), Chamfer Distance is defined as follows:</p> \\[ d=0.5\\cdot (\\frac{1}{\\vert{X}\\vert}\\sum_{x_{i}\\in X}min_{y_{j}\\in Y}\\Vert x_{i}-y_{j}\\Vert ^{2}+\\frac{1}{\\vert{Y}\\vert}\\sum_{y_{j}\\in Y}min_{x_{i}\\in X}\\Vert x_{i}-y_{j}\\Vert ^{2}) \\] <p>In the following code, we use the KDTree to get the nearest neighbors of one point cloud to the other.</p> Chamfer Distance<pre><code>def chamfer_distance(pcl_0, pcl_1):     # pcl_0 and pcl_1 are two point clouds in 3D space\n    assert pcl_1.shape[-1] == 3\n    assert pcl_0.shape[-1] == 3\n    kd_pcl_0 = KDTree(pcl_0)\n    kd_pcl_1 = KDTree(pcl_1)\n    mindis_x2y, nearest_x2y= kd_pcl_1.query(pcl_0)\n    mindis_y2x, nearest_y2x= kd_pcl_0.query(pcl_1)\n    chamfer_dist = 0.5 * float(mindis_x2y.mean() + mindis_y2x.mean())\n    assert type(chamfer_dist) == float\n    return chamfer_dist\n</code></pre>"},{"location":"Learning/DL4CV/CNN/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/CNN/#convolutional-network","title":"Convolutional Network","text":"<ul> <li> <p>Problem: So far our classifiers don't respect the spatial structure of images!</p> </li> <li> <p>Solution: Define new computational nodes that operate on images!</p> </li> </ul> <p></p>"},{"location":"Learning/DL4CV/CNN/#convolutional-layer","title":"Convolutional Layer","text":"<ul> <li>There can be many filters generating many activation maps.</li> <li>Each filter must have same depth as the input image.</li> <li>Each filter also has a bias.</li> <li>Activation map can be considered as a feature vector, telling us the structure of the input.</li> </ul> <ul> <li>The input is often a batch of images</li> <li>The output channel \\(C_{out}\\) is usually different from input channel \\(C_{in}\\)</li> </ul>"},{"location":"Learning/DL4CV/CNN/#pooling-layer","title":"Pooling Layer","text":"<ul> <li>Compute one unique number to represent the region to downsampling.</li> </ul>"},{"location":"Learning/DL4CV/CNN/#convolutional-network-architecture","title":"Convolutional Network Architecture","text":""},{"location":"Learning/DL4CV/CNN/#normalization","title":"Normalization","text":"<ul> <li>Problem: Deep network is hard to train</li> <li>Normalization is introduced to accelerate optimization.</li> </ul> <ul> <li>The batch normalization get its name for it doing normalization in batch dimension \\(N\\). </li> <li>It's a stiff constraint for those layers to exactly fit zero mean and unit variance. So in practice, we introduce learnable scale and shift \\(\\gamma,\\beta\\) to make our network able to learn mean and variance.</li> </ul> <ul> <li>It is wired that the result of one image is influenced by other images, so the model process differently in training time and test time<ul> <li>Training Time: empirically compute mean and variance.</li> <li>Test Time: use the average of historical mean and variance computed during training to compute.</li> </ul> </li> </ul>"},{"location":"Learning/DL4CV/CNN/#cnn-architecture","title":"CNN Architecture","text":""},{"location":"Learning/DL4CV/CNN/#alexnet","title":"AlexNet","text":"<ul> <li>Output channels \\(=\\) number of filters \\(=\\) 64</li> <li>Output size \\(W_{out}=(W_{in}-K+2P)/S+1=56\\)</li> <li>Memory that output feature consume<ul> <li>Number of output elements \\(=C_{out}\\times H_{out}\\times W_{out}=200704\\)</li> <li>Bytes per element \\(=4\\) (for 32-bit floating point)</li> <li>KB \\(=\\) (number of elements)\\(\\times\\) (bytes per element)/1024=784 </li> </ul> </li> <li>Number of learnable parameters<ul> <li>Weight shape \\(=C_{out}\\times C_{in}\\times K \\times K\\)</li> <li>Bias shape \\(=C_{out}=64\\)</li> <li>Number of parameter \\(=\\) weight \\(+\\) bias \\(=\\) 23296</li> </ul> </li> </ul> <ul> <li>Flatten output size \\(=C_{in}\\times H\\times W\\)</li> <li>FCparams \\(=C_{in}\\times C_{out}+C_{out}\\)</li> <li>FC flops \\(=C_{in}\\times C_{out}\\)</li> </ul>"},{"location":"Learning/DL4CV/CNN/#vgg","title":"VGG","text":"<ul> <li>By stacking 3 by 3 conv, we throw away kernal size as a hyperparam and only need to thing about how many 3x3 conv do we need.</li> <li>We can also arbitaryly insert ReLU between these 3x3 conv, allowing more nonlinear computation.</li> </ul>"},{"location":"Learning/DL4CV/CNN/#googlenet","title":"GoogLeNet","text":"<ul> <li>Many innovation for efficiency: reduce parameter count, memory usage, and computation.</li> </ul>"},{"location":"Learning/DL4CV/CNN/#residual-network","title":"Residual Network","text":"<p>When training deeper model, we found that deeper model does worse than shallow model. The initial guess is that Deep model is overfitting since it is much bigger than the other model. </p> <p></p>"},{"location":"Learning/DL4CV/CNN/#other","title":"Other","text":""},{"location":"Learning/DL4CV/Classifier/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/Classifier/#image-classifier-nearest-neighbor-classifier","title":"Image Classifier - Nearest Neighbor Classifier","text":"<p>Compare a new image with each image in training set using some similarity function. Assign the new image a label as same as the most similar image in training set.</p> <p></p> <ul> <li>Using more neighbors helps reduce the effect of outliers.</li> <li>When \\(K &gt; 1\\) there can be ties between classes, which needed to be break somehow.</li> </ul> <p></p> <ul> <li>The idea #2 is bad because when we adjust the hyperparameters according to the test set, we pollute it and it has little differences with training set.</li> <li>Remember that test set can be used only once in the final test.</li> </ul> <p></p> <p>If we want to have a good prediction on a new image, we need a large amount of examples on training set, which grows exponentially with dimension.</p> <p></p>"},{"location":"Learning/DL4CV/Classifier/#linear-classifier","title":"Linear Classifier","text":"<p>Bias Trick is less common to use in practice because when we separate the weight and the bias into separate parameters, we can treat them differently on how they are initialized or regularized.</p> <p></p> <p> </p> <p>We can visualize the \"template\" for each category</p> <p> </p> <p> </p> <p>The score goes linearly with respect to the change of pixels. We can extend this to high dimensions. The changes of scores are consistent with those hyperplanes. </p> <p> </p> <p> </p> <p>This also account for why perceptron, actually a linear classifier, couldn't learn XOR.</p> <p>Now we can compute class scores for an image with given \\(W\\). To get a good \\(W\\), we need to do:</p> <ul> <li> <p>Use a loss function to quantiy how good a value of \\(W\\) is.</p> </li> <li> <p>Find a \\(W\\) that minimizes the loss function.</p> </li> </ul> <p> </p> <p>Different regularization functions give the model extra hints about what types of classifier we'd like them to learn.</p> <p>For example, L2 regularization likes to spread out the weight while L1 regularization does in the opposite way.</p> <p> </p> <p>It's important to notice the loss function on random values, if your model get a worse loss value after training, there must be sth bad.</p> <p> </p> <p> </p> <p>SVM Loss is easy to get to zero point while Cross-Entropy nearly impossible to get to zero.</p>"},{"location":"Learning/DL4CV/NN/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/NN/#optimization","title":"Optimization","text":"<ul> <li>goal: find \\(w^{*}=argmin_{w}L(w)\\)</li> </ul> <p>How to evaluate gradient?</p> <ul> <li>Numeric gradient<ul> <li>We can approximate the gradient by defination. Increasing one slot of the weight matrix by a small step and recompute loss function. Use the defination of derivative to get a approximation of gradient.</li> <li>Takes lots of time and not accurate.</li> </ul> </li> </ul> <p></p> <ul> <li>Analytic gradient<ul> <li>exact, fast, error-prone</li> </ul> </li> </ul> <p></p> <ul> <li>In practice, always use analytic gradient, but check implementation with numerical gradienet. This is called a gradient check</li> </ul>"},{"location":"Learning/DL4CV/NN/#gradient-descent","title":"Gradient Descent","text":""},{"location":"Learning/DL4CV/NN/#stochastic-gradient-descend","title":"Stochastic Gradient Descend","text":"<ul> <li>We don't use the full training data but some small subsamples of it  to approximate loss function and gradient, for computing on the whole set is expansive. These small subsamples are called minibatches</li> </ul> <ul> <li>SGD + Momentum may overshoot in the bottom for its historical speed and will come back.</li> </ul> <ul> <li> <p>Progress along \"steep\" direction is damped. Progress along \"flat\" directions is accelerated.</p> </li> <li> <p>The learning rate will continuely decay since <code>grad_squared</code> gets larger and larger.</p> </li> </ul> <p></p> <p></p> <ul> <li>We initialize <code>moment2</code> as 0 and if we take <code>beta2</code> closely to 0, the <code>moment2</code> will also close to 0, which may make our first gradient step very large. This could cause bad results.</li> </ul> <p></p> <ul> <li>Second-order optimization is better to use in low dimension. </li> </ul> <p></p>"},{"location":"Learning/DL4CV/NN/#neural-network","title":"Neural Network","text":"<p>To solve the limitation of linear classifier, we can apply feature transformation.</p> <p></p>"},{"location":"Learning/DL4CV/NN/#activation-function","title":"Activation function","text":""},{"location":"Learning/DL4CV/NN/#neuron","title":"Neuron","text":""},{"location":"Learning/DL4CV/NN/#space-warping","title":"Space Warping","text":"<p>The more the layer, the more complex the model is. Try to adjust regularization parameter to solve it.</p> <p></p>"},{"location":"Learning/DL4CV/NN/#universal-approximation","title":"Universal Approximation","text":"<p>A two layer neural network can approximate any function. But it may need a large size to get high fidelity.</p> <p></p>"},{"location":"Learning/DL4CV/NN/#convex-functions","title":"Convex Functions","text":"<p>Taking any two points in the input, the secant line will always lie above the function between that two points.</p> <p></p> <p>However, most neural networks need nonconvex optimization</p>"},{"location":"Learning/DL4CV/NN/#backpropagation","title":"Backpropagation","text":""},{"location":"Learning/DL4CV/NN/#computation-graph","title":"Computation Graph","text":""},{"location":"Learning/DL4CV/NN/#backprop-implementation","title":"Backprop Implementation","text":"<ul> <li>You can define your own node object in computation graph using pytorch API</li> </ul>"},{"location":"Learning/DL4CV/NN/#backprop-with-vectors","title":"Backprop with Vectors","text":""},{"location":"Learning/DL4CV/NN/#backprop-with-matrics","title":"Backprop with Matrics","text":"<ul> <li>\\(dL/dx\\) must have the same shape as \\(x\\) since loss \\(L\\) is a scalar</li> </ul> <p>Assume we want to derive \\(dL/dx_{i,j}\\)</p> <ul> <li>Note that only the \\(ith\\) row of \\(y\\) is formed by \\(x_{i,j}\\) and corresponding coefficients are the \\(jth\\) row of \\(w\\).</li> <li>So \\(dL/dx_{i,j}\\) is just the inner product of the \\(ith\\) row of \\(dL/dy\\) and the \\(jth\\) column of \\(w^{T}\\), which leads to the result of \\(dL/dx=(dL/dy)w^{T}\\)</li> </ul> <p></p>"},{"location":"Learning/DL4CV/NN/#high-order-derivatives","title":"High-Order Derivatives","text":"<p>\\(\\frac{\\partial ^{2}L}{\\partial x_{0}^{2}}\\) is \\(D_{0} \\times D_{0}\\) dimension for it's the derivative of \\(\\frac{\\partial L}{\\partial x_{0}}\\), which is a \\(D_{0}\\) dimension vector.</p> <p></p>"},{"location":"Learning/DL4CV/OD/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/OD/#object-detection","title":"Object Detection","text":"<p>Input: Single RGB Image</p> <p>Output: A set of detected objects;</p> <p>For each object predict:</p> <ul> <li>Category label (from fixed, known set of categories)</li> <li>Bounding box (four numbers: x, y, width, height)</li> </ul> <p>Challenges:</p> <ul> <li>Multiple outputs: Need to output variable numbers of objects per image</li> <li>Multiple types of output: Need to predict \"what\" (category label) as well as \"where\" (bounding box)</li> <li>Large images: Classification works at 224x224; need higher resolution for detection, often ~800x600</li> </ul>"},{"location":"Learning/DL4CV/OD/#detecting-a-single-object","title":"Detecting a single object","text":""},{"location":"Learning/DL4CV/OD/#detecting-multiple-objects","title":"Detecting Multiple Objects","text":"<p>Need different numbers of outputs per image</p>"},{"location":"Learning/DL4CV/OD/#sliding-wingdow","title":"Sliding Wingdow","text":"<p>Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.</p> <p>Question: There are many possible boxes in an image of size \\(H\\times W\\). Total possible boxes: \\(\\frac{H(H+1)}{2}\\frac{W(W+1)}{2}\\)</p>"},{"location":"Learning/DL4CV/OD/#region-proposals","title":"Region Proposals","text":"<ul> <li>Find a small set of boxes that are likely to cover all objects</li> <li>Often based on heuristics: e.g. look for \"blob-like\" image regions</li> <li>Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU</li> </ul>"},{"location":"Learning/DL4CV/OD/#r-cnn-region-based-cnn","title":"R-CNN: Region-Based CNN","text":"<p>We can use region proposal method like selective search to get some region proposals in the image with different sizes and different aspect ratios.</p> <p>Then we warp these region to a same size and forward them throusgh ConvNet.</p> <p>Here if we only output the predicted class in each ConvNet, the box be may not accurate. So we introduce bounding box regression to predict \"tranform\" to correct the box: 4 numbers \\((t_{x},t_{y},t_{h},t_{w})\\). Now our CNN is going to output an additional thing, which is a transformation that will transform the region proposal box into the final box that we want.</p> <p>Region proposal: \\((p_{x},p_{y},p_{h},p_{w})\\)</p> <p>Transform: \\((t_{x},t_{y},t_{h},t_{w})\\), which is one of the output of convnet</p> <p>Final bex: \\((b_{x},b_{y},b_{h},b_{w})\\)</p> <p>Translate relative to box size: \\(b_{x}=p_{x}+p_{w}t_{x}\\qquad b_{y}=p_{y}+p_{h}t_{y}\\)</p> <p>Log-space scale transform: \\(b_{w}=p_{w}exp(t_{w})\\qquad b_{h}=p_{h}exp(t_{h})\\)</p> <p>The way we parameterize the transformation is invariant to the location and the scale of the box.</p> <p>Test-time:</p> <ul> <li>Run region proposal method to compute ~2000 region proposals</li> <li>Resize each region to 224x224 and run independently through CNN to predict class scores and bbox transform</li> <li>Use scores to select a subset of region proposals to output (Many choices here: threshold on background, or per-category? Or take top K proposals per image?)</li> <li>Compare with ground-truth boxes</li> </ul>"},{"location":"Learning/DL4CV/OD/#comparing-boxes-intersection-over-unioniou","title":"Comparing Boxes: Intersection over Union(IoU)","text":"<p>How can we compare our prediction to the ground-truth box?</p> <p>Intersection over Union (IoU)(Also called \"Jaccard similarity\" or \"Jaccard index\"): \\(\\frac{Area of Intersection}{Area of Union}\\)</p> <p>IoU &gt; 0.5 is \"decent\". IoU &gt; 0.7 is \"pretty good\". IoU &gt; 0.9 is \"almost perfect\".</p>"},{"location":"Learning/DL4CV/OD/#overlapping-boxes-non-max-supperssionnms","title":"Overlapping Boxes: Non-Max Supperssion(NMS)","text":"<p>Problem: Object detectors often output many overlapping detections, which are detecting the same object but in different boxes.</p> <p>Solution: Post-process raw detections using Non-Max Suppression (NMS)</p> <ol> <li> <p>Select next highest-scoring box</p> </li> <li> <p>Eliminate lower-scoring boxes with loU &gt; threshold (e.g. 0.7)</p> </li> <li> <p>If any boxes remain, GOTO 1</p> </li> </ol> <p>Problem: NMS may eliminate \"good\" boxes when objects are highly overlapping ... no good solution </p>"},{"location":"Learning/DL4CV/OD/#evaluating-object-detectors-mean-average-precisionmap","title":"Evaluating Object Detectors: Mean Average Precision(mAP)","text":"<ul> <li> <p>Run object detector on all test images (with NMS)</p> </li> <li> <p>For each category, compute Average Precision (AP) = area under Precision vs Recall Curve</p> <ul> <li> <p>For each detection (highest score to lowest score)</p> <ul> <li> <p>If it matches some GT box with loU &gt; 0.5, mark it as positive and eliminate the GT</p> </li> <li> <p>Otherwise mark it as negative</p> </li> <li> <p>Plot a point on PR Curve</p> </li> </ul> </li> <li> <p>Average Precision (AP) = area under PR curve</p> </li> </ul> </li> <li> <p>Mean Average Precision (mAP) = average of AP for each category</p> </li> <li> <p>For \"COCO mAP\": Compute mAP@thresh for each loU threshold (0.5, 0.55, 0.6, ... , 0.95) and take average</p> </li> </ul> <p>How to get AP = 1.0: Hit all GT boxes with loU &gt; 0.5, and have no \"false positive\" detections ranked above any \"true positives\"</p>"},{"location":"Learning/DL4CV/OD/#fast-r-cnn","title":"Fast R-CNN","text":""},{"location":"Learning/DL4CV/OD/#cropping-features-rol-pool","title":"Cropping Features: Rol Pool","text":"<p>First, we project proposal onto features. The projected region may not align with the grid cells so we have to \"snap\" them to fit the grid cells.</p> <p>Then we divide these regions into 2x2 grid of (roughly) equal subregions. Max-pool within each subregion to get the region features. So region features always the same size even if input regions have different sizes.</p> <p>Problem:</p> <ul> <li> <p>Misaligned features due to snapping</p> </li> <li> <p>Can't backprop to box coordinates</p> </li> </ul>"},{"location":"Learning/DL4CV/OD/#cropping-features-rol-align","title":"Cropping Features: Rol Align","text":"<p>We still divide the projected proposal into equal-size subregion. But instead of snapping, we're going to sample a fixed number of regularly-spaced points inside each subregion. These points may not align with the grid. Here we use bilinear interpolation.</p> <p></p> <p>After sampling, max-pool in each subregion.</p> <p>Output features now aligned to input box! And we can backprop to box coordinates!</p>"},{"location":"Learning/DL4CV/OD/#faster-r-cnn-learnable-region-proposals","title":"Faster R-CNN: Learnable Region Proposals","text":"<p>We eliminate the heuristic algorithm called selective search and instead train a CNN to predict region proposals.</p> <p>Compared to Fast R-CNN, we insert Region Proposal Network(RPN) after feature map to predict porposals from features.</p> <p>Otherwise same as Fast R-CNN: Crop features for each proposal, classify each one.</p>"},{"location":"Learning/DL4CV/OD/#region-proposal-networkrpn","title":"Region Proposal Network(RPN)","text":"<p>Now in Faster R-CNN, we jointly train with 4 losses:</p> <ul> <li>RPN classification: anchor box is object / not an object</li> <li>RPN regression: predict transform from anchor box to proposal box</li> <li>Object classification: classify proposals as background / object class</li> <li>Object regression: predict transform from proposal box to object box</li> </ul> <p>Faster R-CNN is a Two-stage object detector</p> <p>First stage: Run once per image * Backbone network * Region proposal network</p> <p>Second stage: Run once per region * Crop features: Rol pool / align * Predict object class * Prediction bbox offset</p>"},{"location":"Learning/DL4CV/OD/#single-stage-object-detection","title":"Single-Stage Object Detection","text":"<p>Just like the RPN in faster R-CNN except that rather than classfying the anchor boxes as object or not object, instead we'll just make a full classification decision for the category of the object right here.</p>"},{"location":"Learning/DL4CV/SS/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/SS/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Label each pixel in the image with a category label. Don't differentiate instances, only care about pixels</p>"},{"location":"Learning/DL4CV/SS/#sliding-window","title":"Sliding Window","text":"<p>For every pixel, extract a patch centered it and input to CNN to get the label for that pixel.</p> <p>Problem: Very inefficient! Not reusing shared features between overlapping patches.</p>"},{"location":"Learning/DL4CV/SS/#fully-convolutional-network","title":"Fully Convolutional Network","text":""},{"location":"Learning/DL4CV/SS/#in-network-upsampleing-unpooling","title":"In-Network Upsampleing: \"Unpooling\"","text":"<p>Bilinear Interpolation </p> <p>Bicubic Interpolation</p> <p></p> <p>Max Unpooling</p> <p>Pair each downsampling layer with an upsampling layer</p> <p>When Max Pooling, Remember which position had the max</p> <p>When Max Unpooling, Place the value into remembered positions, while filling other positions with 0.</p> <p>Transposed Convolution</p> <p>Convolution with stride &gt; 1 is \"Learnable Downsampling\"</p> <p>Can we use stride &lt; 1 for \"Learnable Upsampling\"?</p> <p></p> <p></p> <p></p>"},{"location":"Learning/DL4CV/Training/","title":"Deep Learning for Computer Vision","text":""},{"location":"Learning/DL4CV/Training/#train-neural-network","title":"Train Neural Network","text":""},{"location":"Learning/DL4CV/Training/#one-time-setup","title":"One time setup","text":""},{"location":"Learning/DL4CV/Training/#activation-functions","title":"Activation functions","text":"<p>Sigmoid: \\(\\sigma(x)=1/(1+e^{-x})\\)</p> <ul> <li>Squashes numbers to range \\([0,1]\\)</li> <li>problems:<ul> <li>Saturated neurons \"kill\" the gradients</li> <li>Sigmoid outputs are not zero-centered</li> <li>exp() is a bit compute expensive</li> </ul> </li> </ul> <p>Tanh</p> <ul> <li>Squashes numbers to range \\([-1,1]\\)</li> <li>zero centered(nice)</li> <li>still kill gradients when saturated</li> </ul> <p>ReLU: \\(f(x)=max(0,x)\\)</p> <ul> <li>Does not saturate in positive region</li> <li>Very computationally efficient</li> <li>COnverges much faster than sigmoid/tanh in practice</li> <li>Problems:<ul> <li>Not zero-centered output</li> <li>When x &lt; 0, the gradient will also be zero, which may cause it never update.</li> </ul> </li> </ul> <p>Leaky ReLU: \\(f(x)=max(0.01x,x)\\)</p> <ul> <li>Does not saturate</li> <li>Very computationally efficient</li> <li>COnverges much faster than sigmoid/tanh in practice</li> <li>will not \"die\" like ReLU</li> <li>We can introduce learnable parameter \\(\\alpha\\), which is call PReLU(Parametric Rectifier): \\(f(x)=max(\\alpha x,x)\\)</li> </ul> <p></p>"},{"location":"Learning/DL4CV/Training/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Subtract the mean image(AlexNet)</li> <li>Subtract per-channel mean(VGGNet)</li> <li>Subtract per-channel mean and divide by per-channel std(ResNet)</li> <li>Others<ul> <li>PCA: data has diagonal covariance matrix</li> <li>whitening: covariance matrix is the identity matrix</li> </ul> </li> </ul>"},{"location":"Learning/DL4CV/Training/#weight-initialization","title":"Weight Initialization","text":"<p>Q: What happens if we initialize all \\(W=0, b=0\\)?</p> <p>A: All outputs are zero and weights will never update.</p> <p>Small random values</p> <p></p> <p>Big random values</p> <p></p> <p>Xavier Initialization</p> <p></p> <ul> <li>Derivation: Varience of output = Variance of input</li> </ul> \\[ y=Wx\\qquad \\qquad \\qquad \\qquad y_{i}=\\sum_{j=1}^{Din}x_{j}w_{j} \\\\ \\begin{align} Var(y_{i})&amp;=Din\\times Var(x_{i}w_{i}) \\qquad &amp;\\text{Assume x, w are iid}\\\\ &amp;=Din\\times (E[x_{i}^{2}]E[w_{i}^{2}]-E[x_{i}]^{2}E[w_{i}]^{2}) \\qquad &amp;\\text{Assume x, w independent}\\\\ &amp;=Din\\times Var(x_{i})\\times Var(w_{i})\\qquad &amp;\\text{Assume x, w are zero-mean} \\\\ \\end{align} \\\\ \\text{If }Var(w_{i})=1/Din\\text{ then }Var(y_{i})=Var(x_{i}) \\] <p>Unfortunately, this initialization tends to be bad when we use ReLU as activation function.</p> <p>Kaiming / MSRA Initialization * ReLU correction: std = sqrt(2 / Din)</p> <p>\"Just right\" - activations nicely scaled for all layers.</p> <p>Not do well in Residual Networks.</p> <p>If we initialize with MSRA: then \\(Var(F(x))=Var(x)\\). But then \\(Var(F(x)+x)&gt;Var(x)\\) - variance grows with each block!</p> <ul> <li>Solution: Initialize first conv with MSRA, initialize second conv to zero. Then \\(Var(x+F(x))=Var(x)\\)</li> </ul>"},{"location":"Learning/DL4CV/Training/#regularization","title":"Regularization","text":"<p>In common use: L2 regularization, L1 regularization ...</p> <p>Dropout In each forward pass, randomly set some neurons to zero. Probability of dropping is a hyperparameter. 0.5 is common</p> <p>Forces the network to have a redundant representation. Prevents co-adaptation of features.</p> <p>Another interpretation: Dropout is training a large ensemble of models (that share parameters). Each binary mask is one model</p> <ul> <li>Test Time: We need to take another way to compute output when testing or our output will be random.<ul> <li>We want to \"average out\" the randomness at test-time. That is, multiply the result without dropout by dropout probobility.</li> </ul> </li> </ul> <p>Inverted Dropout Drop and scale layer during training, </p>"},{"location":"Learning/DL4CV/Training/#data-augmentation","title":"Data Augmentation","text":"<p>Tranform input training image before feeding it into the network. Horizontal flips is one example. This augments the amount of data and keep their label unchanged.</p> <p>Random Crops and Scales</p> <ul> <li> <p>Training: sample random crops / scales</p> <ul> <li>Pick random L in range [256, 480]</li> <li>Resize training image, short side = L</li> <li>Sample random 224 x 224 patch</li> </ul> </li> <li> <p>Testing: average a fixed set of crops</p> <ul> <li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li> <li>For each size, use 10 224 x 224 crops: 4 corners + center, + flips</li> </ul> </li> </ul>"},{"location":"Learning/DL4CV/Training/#training-dynamics","title":"Training Dynamics","text":""},{"location":"Learning/DL4CV/Training/#learning-rate-schedules","title":"Learning rate schedules","text":"<p>We can Start with large learning rate and decay over time.</p> <p>Step Schedule</p> <p>Step: Reduce learning rate at a few fixed points. </p> <p>E.g. for ResNets, multiply LR by 0.1 after epochs 30, 60, and 90.</p> <ul> <li>Problem: introduce additional hyperparameters we have to tune.</li> </ul> <p>Cosine Schedule: \\(\\alpha_{t}=\\frac{1}{2}\\alpha_{0}(1+cos(t\\pi /T))\\)</p> <p></p> <p>Linear Schedule: \\(\\alpha_{t}=\\alpha_{0}(1-t/T)\\)</p> <p>Inverse Sqrt: \\(\\alpha_{t}=\\alpha_{0}/\\sqrt{t}\\)</p> <p>Constant: \\(\\alpha_{t}=\\alpha_{0}\\)</p> <ul> <li>Early Stopping: Stop training the model when accuracy on the validation set decreases. Or train for a long time, but always keep track of the model snapshot that worked best on val. Always a good idea to do this!</li> </ul>"},{"location":"Learning/DL4CV/Training/#hyperparameter-optimization","title":"hyperparameter optimization","text":"<p>Grid Search</p> <p>Choose several values for each hyperparameter(Often space choices log-linearly).</p> <p>Example:</p> <p>Weight decay: [1x10-4, 1x10-3, 1x10-2, 1x10-1], Learning rate: [1x10-4, 1x10-3, 1x10-2, 1x10-1]</p> <p>Evaluate all possible choices on this hyperparameter grid.</p> <p>Random Search</p> <p>Randomly choose several values for each hyperparameter in a given range(Often space choices log-linearly)</p> <p>Example:</p> <p>Weight decay: log-uniform on [1x10-4, 1x10-1], Learning rate: log-uniform on [1x10-4, 1x10-1]</p> <ul> <li>Jump out of the grid and may have a better performance when some parameters are not important.</li> </ul> <p>Choosing hyperparameters:</p> <p>Step 1: Check initial loss</p> <p>Turn off weight decay, sanity check loss at initialization</p> <p>Step 2: Overfit a small sample</p> <p>Try to train to 100% training accuracy on a small sample of training data (~5-10 minibatches); fiddle with architecture, learning rate, weight initialization. Turn off regularization.</p> <p>Loss not going down? LR too low, bad initialization</p> <p>Loss explodes to Inf or NaN? LR too high, bad initialization</p> <p>Step 3: Find LR that makes loss go down</p> <p>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within ~100 iterations</p> <p>Step 4: Coarse grid, train for ~1-5 epochs</p> <p>Step 5: Refine grid, train longer</p> <p>Step 6: Look at loss curves</p> <p>Step 7: GOTO step 5</p>"},{"location":"Learning/DL4CV/Training/#after-training","title":"After training","text":""},{"location":"Learning/DL4CV/Training/#model-ensembles","title":"Model ensembles","text":"<ol> <li> <p>Train multiple independent models</p> </li> <li> <p>At test time average their results</p> </li> </ol>"},{"location":"Learning/DL4CV/Training/#transfer-learning","title":"Transfer learning","text":"<ol> <li> <p>Train on Imagenet</p> </li> <li> <p>Use CNN as a feature extractor. Remove those fully connected layers in the last and freeze other layers.</p> </li> <li> <p>Bigger dataset: Fine-Tuning</p> </li> </ol> <p>Reinitialize those last layer to new layers and continue training CNN for new task.</p> <p>Some tricks:</p> <ul> <li> <p>Train with feature extraction first before fine-tuning</p> </li> <li> <p>Lower the learning rate: use ~1/10 of LR used in original training</p> </li> <li> <p>Sometimes freeze lower layers to save computation</p> </li> </ul>"},{"location":"Learning/DL4CV/Training/#large-batch-training","title":"Large-batch training","text":"<p>How to scale up on data-parallel training on K GPUs?</p> <p>Goal: Train for same number of epochs, but use larger minibatches. We want model to train K times faster</p> <p>Single-GPU model: batch size N, learning rate \\(\\alpha\\)</p> <p>K-GPU model: batch size KN, learning rate \\(K\\alpha\\)</p>"},{"location":"Learning/DeepLearning/NNandDL/","title":"Neural Networks and Deep Learning","text":""},{"location":"Learning/DeepLearning/NNandDL/#introduction","title":"Introduction","text":""},{"location":"Learning/DeepLearning/NNandDL/#neural-network","title":"Neural Network","text":"<ul> <li>Use the given input x to predict output y. Inputs are called input layer.</li> <li>the circles are called hidden units, use input features to output another features. The units in a column are called a hidden layer. </li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#supervised-learning","title":"Supervised Learning","text":"<ul> <li>Structured Data: Basically databases of data. Each of the features has a defined meaning.</li> <li>Unstructured Data: Data like audio, image and text.</li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#drivers-behind-deep-learning","title":"Drivers behind deep learning","text":"<ul> <li>Scale: Large NN performs much better with a large amount of labeled data. </li> <li>Algorithms: From sigmoid to ReLU to solve gradient descend.</li> <li>Computation</li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#neural-network-basics","title":"Neural Network Basics","text":""},{"location":"Learning/DeepLearning/NNandDL/#binary-classification","title":"Binary Classification","text":"<p>Learn a classifier that can input a feature vector x and predict the corresponding label y is 1 or 0.</p>"},{"location":"Learning/DeepLearning/NNandDL/#notation","title":"Notation","text":"<ul> <li> <p>\\((x,y)\\): A single training example where \\(x\\) is a \\(n_{x}\\) dimension feature vector and \\(y\\), the label, is either 0 or 1.</p> </li> <li> <p>\\(m\\): Number of training examples. Training examples are denoted from \\((x^{(1)},y^{(1)})\\) to \\((x^{(m)},y^{(m)})\\)</p> </li> <li> <p>matrix \\(X\\) represents training examples in a compact way.</p> </li> </ul> \\[ X= \\begin{bmatrix} \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ x^{(1)} &amp; x^{(2)} &amp; \\dots &amp; x^{(m)} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\end{bmatrix} \\] <p>The width and height of the matrix are \\(m\\) and \\(n_{x}\\) respectively.</p> <ul> <li>Label \\(y\\) can also be denoted by stacking \\(y\\) in columns.</li> </ul> \\[ Y= \\begin{bmatrix} y^{(1)} &amp; y^{(2)} &amp; \\dots &amp; y^{(m)} \\end{bmatrix} \\]"},{"location":"Learning/DeepLearning/NNandDL/#logistic-regression","title":"Logistic Regression","text":"<p>Given an input feature vector \\(x \\in R^{n_{x}}\\), we want to get \\(\\hat{y}=P(y=1|x)\\) as the probability of the chance that \\(y\\) is equal to 1.</p> <p>When  implementing logistic regression, our job is to learn the parameters \\(w \\in R^{n_{x}}\\) and \\(b \\in R\\), so that \\(\\hat{y}\\) can be got by the following way:</p> \\[ \\hat{y}=\\sigma (w^{T}x+b) \\] <p>\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\), the sigmoid function, is used to map the result in the parenthesis to \\([0,1]\\) for \\(\\hat{y}\\) is the probability.</p> <p></p>"},{"location":"Learning/DeepLearning/NNandDL/#logistic-regression-cost-function","title":"Logistic Regression cost function","text":"<ul> <li> <p>Denote \\(z^{(i)}=w^{T}x^{(i)}+b\\) and \\(\\hat{y}^{(i)}=\\sigma(z^{(i)})\\)</p> </li> <li> <p>Goal: Given \\(\\{(x^{(1)},y^{(1)}),\\dots,(x^{(m)},y^{(m)})\\}\\), want \\(\\hat{y}^{(i)}\\approx y^{i}\\).</p> </li> <li> <p>Loss(error) function: A function to measure how good our prediction \\(\\hat{y}\\) is on a single example. The small the loss function value, the better our prediction. Denoted by \\(\\mathcal L(\\hat{y},y)\\). Some common functions are shown below.</p> <ul> <li>\\(\\mathcal L(\\hat{y},y)=\\frac{1}{2}(\\hat{y}-y)^{2}\\)</li> <li>\\(\\mathcal L(\\hat{y},y)=-(ylog\\hat{y}+(1-y)log(1-\\hat{y}))\\)</li> </ul> </li> <li> <p>Cost function: measures how well you are doing on the entire training set.</p> </li> </ul> \\[ J(w, b)=\\frac{1}{m}\\sum^{m}_{i=1}\\mathcal L(\\hat{y}^{(i)},y^{(i)}) \\] <p>The function here is a convex function with global optimal.</p>"},{"location":"Learning/DeepLearning/NNandDL/#gradient-descent","title":"Gradient Descent","text":"<ul> <li> <p>Goal: Find \\(w,b\\) that minimize \\(J(w,b)\\). </p> </li> <li> <p>Gradient Descent: initialize parameters with some values and repeatedly update them in the opposite gradient direction. Take \\(w\\) for example:</p> </li> </ul> \\[ Repeat\\{     w:=w-\\alpha\\frac{dJ(w)}{dw} \\} \\] <ul> <li>\\(\\alpha\\): Learning rate, controls how big a step we take on each iteration.</li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#derivatives-with-a-computation-graph","title":"Derivatives with a Computation Graph","text":"<ul> <li>Computation Graph: Assume wew want to compute \\(J=3(a+bc)\\) </li> <li>\\(\\frac{dJ}{db}=\\frac{dJ}{dv}\\frac{dv}{du}\\frac{du}{db}\\)</li> <li>Logistic regression derivatives: <ul> <li>In programming, we use <code>dx</code> to represent \\(\\frac{dL}{dx}\\). <code>x</code> can be any variable here such as <code>a</code>, <code>z</code> and so on.</li> <li><code>da</code>\\(=\\frac{dL}{da}=-\\frac{y}{a}+\\frac{1-y}{1-a}\\)</li> <li><code>dz</code>\\(=\\frac{dL}{da}\\frac{da}{dz}=(-\\frac{y}{a}+\\frac{1-y}{1-a})a(1-a)=a-y\\)</li> <li><code>dw1</code>\\(=\\frac{dL}{dw_{1}}=\\)<code>dz</code>\\(\\frac{dz}{dw_{1}}=x_{1}\\)<code>dz</code></li> <li><code>db</code>\\(=\\)<code>dz</code></li> </ul> </li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#logistic-regression-on-m-examples","title":"Logistic regression on m examples","text":"<p>We know that </p> \\[ J(w, b)=\\frac{1}{m}\\sum^{m}_{i=1}\\mathcal L(\\hat{y}^{(i)},y^{(i)}) \\] <p>So when computing derivatives on \\(J\\), we just need to take the average of derivatives on \\(\\mathcal L^{(i)}\\). i.e. <code>dw1</code>\\(=\\frac{1}{m}\\sum_{i=1}^{m}\\)<code>dw1</code>\\(^{(i)}\\), <code>db</code>\\(=\\frac{1}{m}\\sum_{i=1}^{m}\\)<code>db</code>\\(^{(i)}\\)</p> <p>The algorithm is shown below:</p> \\[ \\begin{aligned} J=0;dw&amp;1=0;dw2=0;db=0; \\\\ for \\ \\ i=1&amp; \\ \\ to \\ \\ m: \\\\ &amp;z^{i}=w^{T}x^{i}+b\\\\ &amp;a^{i}=\\sigma(z^{(i)})\\\\ &amp;J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})]\\\\ &amp;dz^{(i)}=a^{(i)}-y^{(i)}\\\\ &amp;dw1+=x_{1}^{(i)}dz^{(i)}\\\\ &amp;dw2+=x_{2}^{(i)}dz^{(i)}\\\\ &amp;db+=dz^{(i)}\\\\ J/=m;d&amp;w1/=m;dw2/=m;db/=m; \\nonumber \\end{aligned} \\]"},{"location":"Learning/DeepLearning/NNandDL/#python-and-vectorization","title":"Python and Vectorization","text":""},{"location":"Learning/DeepLearning/NNandDL/#vectorization","title":"Vectorization","text":"<p>Vectorization can significantly speed up calculation compared to using explicit <code>for</code> loops.</p> VectorizationFor Loop <pre><code>import numpy as np\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nc = np.dot(a, b)\n</code></pre> <pre><code>import numpy as np\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nc = 0\nfor i in range(1000000):\n    c += a[i] * b[i]\n</code></pre> <p>If we use <code>time()</code> function in <code>time</code> library to check the time comsuming, we will find that the Vectorization is hundreds of times faster than for loop.</p> <p>One criterian is that always using built-in functions in Python or Numpy instead of explicit for loop.</p> <p>Now we can apply vectorization to logistic regression.</p> <p>When it comes to compute the prediction, recap the defination of matrix \\(X\\).</p> <p>Here we define matrix \\(Z\\) and \\(A\\). Note that \\(z^{(i)}=w^{T}x^{(i)}+b\\) and \\(a^{(i)}=\\sigma(z^{(i)})\\)</p> \\[ Z= \\begin{bmatrix} z^{(1)} &amp; z^{(2)} &amp; \\dots &amp; z^{(m)}  \\end{bmatrix} \\\\ =w^{T}X+ \\begin{bmatrix} b &amp; b &amp; \\dots &amp; b  \\end{bmatrix} \\\\ A=\\begin{bmatrix} a^{(1)} &amp; a^{(2)} &amp; \\dots &amp; a^{(m)}  \\end{bmatrix} \\] <p>If writen in Python: <pre><code>Z = np.dot(w.T, X) + b\nA = sigmoid(Z)\ndZ = A - Y\ndw = np.dot(X, dZ.T) / m\ndb = np.sum(dZ) / m\nw = w - alpha * dw\nb = b - alpha * db\n</code></pre> A for loop is still needed to control the iteration times.</p>"},{"location":"Learning/DeepLearning/NNandDL/#shallow-neural-networks","title":"Shallow Neural Networks","text":""},{"location":"Learning/DeepLearning/NNandDL/#neural-network-representation","title":"Neural Network Representation","text":"<p> * Each column is call a layer. There are three layers here     * Input layer with \\(x_{1},x_{2},x_{3}\\)     * Hidden layer is the second column with 4 circles. The \"hidden\" means the value in this layer is invisible in the training set.     * Output layer, which output the final result. * Layers are denoted by \\(a^{[i]}\\).</p> <p>$$ a^{[0]}= \\begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\ \\end{bmatrix} \\quad a^{[1]}= \\begin{bmatrix} a_{1}^{[1]} \\ a_{2}^{[1]} \\ a_{3}^{[1]} \\ a_{4}^{[1]} \\ \\end{bmatrix} \\quad a^{[2]}=\\hat{y} $$ * \\(a\\) means activate here. This example is a two layer NN since we don't count input layer.</p>"},{"location":"Learning/DeepLearning/NNandDL/#computation","title":"Computation","text":"<p>In each node in hidden layer, we repeat the same computing way.</p> <p></p> <p>For the entire network,</p> <p></p> <p>We can also vectorize it</p> <p></p> \\[ W^{[1]}= \\begin{bmatrix} w_{1}^{[1]T} \\\\ w_{2}^{[1]T} \\\\ w_{3}^{[1]T} \\\\ w_{4}^{[1]T} \\\\ \\end{bmatrix} ,a \\ 4\\times3 \\ matrix \\] <p>We can extend vectorization to multiple examples</p> <p></p> <p>Note that actually \\(X=A^{[0]}\\)</p>"},{"location":"Learning/DeepLearning/NNandDL/#activation-functions","title":"Activation functions","text":"<p>There are different activation functions you can use in NN and different layers may have different activation functions. Now we can redefine that \\(a^{[i]}=g^{[i]}(z^{[i]})\\). \\(g^{[i]}\\) represents the activation function in the ith layer. * sigmoid function: \\(a=\\sigma(z)\\). Mostly used in the output layer in binary classification. </p> <ul> <li> <p>tanh function: \\(a=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\). Always behaving better than sigmoid. A problem is when \\(|z|\\) is too large, the gradient of both sigmoid and tanh goes to 0 </p> </li> <li> <p>ReLU function: \\(a=max(0, z)\\). When z is negative, the gradient is 0. The gradient is 1 otherwise. Widely used.</p> </li> </ul> <p></p> <ul> <li>Leaky ReLU: \\(a=max(0.01z, z)\\).</li> </ul> <p></p>"},{"location":"Learning/DeepLearning/NNandDL/#why-a-non-linear-activation-function-needed","title":"Why a Non-Linear Activation Function needed?","text":"<p>If we use Linear function as activation function or simply don't use activation function, the output \\(y\\) is just the linear combination of input \\(X\\). That means the result won't be better with more hidden layers.</p>"},{"location":"Learning/DeepLearning/NNandDL/#derivatives","title":"Derivatives","text":"<ul> <li>sigmoid: \\(g'(z)=g(z)(1-g(z))\\)</li> <li>tanh: \\(g'(z)=1-g(z)^{2}\\)</li> <li>ReLU: \\(g'(z)=0, z&lt;0 \\ ; \\ g'(z)=1,z\\geq 0\\)</li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#neural-network-gradients","title":"Neural network gradients","text":"<p>Similar to logistic regression.  </p> <p>The left one is formula for single trainging example and the right one is for the entire training set. Note the \\(*\\) in the forth line means element-wise product.</p>"},{"location":"Learning/DeepLearning/NNandDL/#random-initialization","title":"Random initialization","text":"<p>Take the following NN for example: </p> <p>If we initialize \\(W^{[1]}\\) as all zero matrix, then \\(a^{[1]}_{1}\\) and \\(a^{[1]}_{2}\\) are same. The back propagation will also be the same, which turns out the hidden units in that layer are same. </p> <p>We can initializate randomly. <pre><code>W1 = np.random.rand((2, 2)) * 0.01\nb1 = np.zero((2, 1))\nW2 = np.random.rand((2, 2)) * 0.01\nb2 = np.zero((2, 1))\n</code></pre></p> <p>\\(b\\) is ok to initialize to zero. We also times a \\(0.01\\) to make the initial value small for bigger gradient if we use sigmoid or tanh.</p>"},{"location":"Learning/DeepLearning/NNandDL/#deep-neural-network","title":"Deep neural network","text":"<p>Deep neural network has more hidden layers. </p> <ul> <li> <p>\\(L\\) denotes the number of layers. </p> </li> <li> <p>\\(n^{[l]}\\) denotes the numbers of unit in layer \\(l\\).</p> </li> </ul>"},{"location":"Learning/DeepLearning/NNandDL/#forward-propagation","title":"Forward propagation","text":"\\[ \\begin{aligned} Z^{[l]}&amp;=W^{[l]}A^{[l-1]}+b^{[l]} \\\\ A^{[l]}&amp;=g^{[l]}(Z^{[l]}) \\\\ ca&amp;che \\ Z^{[l]} \\end{aligned} \\] <p>Here we store \\(Z^{[l]}\\) for computing backward propagation.</p> <p>The dimension of \\(W^{[l]}\\) is \\((n^{[l]},n^{[l-1]})\\). The dimension of \\(b^{[l]}\\) is \\((n^{[l]},1)\\).</p> <p>Although we always try to get rid of explicit for loop, a for loop is still necessary to traverse the \\(L\\) layers.</p>"},{"location":"Learning/DeepLearning/NNandDL/#backward-propagation","title":"Backward propagation","text":"\\[ \\begin{aligned} dZ^{[l]}&amp;=dA^{[l]}*g^{[l]'}(Z^{[l]}) \\\\ dW^{[l]}&amp;=\\frac{1}{m}dZ^{[l]}\\cdot A^{[l-1]T} \\\\ db^{[l]}&amp;=\\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True) \\\\ dA^{[l-1]}&amp;=W^{[l]T}\\cdot dZ^{[l]} \\end{aligned} \\] <p>In an iteration, we first use forward propagation to get \\(\\hat{y}\\), compute \\(\\mathcal L(\\hat{y},y)\\) and then use back propagation to get those gradients.</p>"},{"location":"Learning/DeepLearning/NNandDL/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>Parameters: \\(W^{[1]},b^{[1]},W^{[2]},b^{[2]}\\dots\\)</li> <li>Hyperparameters: parameters that control the above ultimate parameters<ul> <li>learning rate \\(\\alpha\\)</li> <li>iteration times</li> <li>number of hidden layers \\(L\\)</li> <li>number of hidden units \\(n^{[1]},n^{[2]}\\dots\\)</li> <li>choice of activation functions</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Animation/","title":"Animation","text":"<p>Animation bring things to life</p> <ul> <li> <p>Communication tool </p> </li> <li> <p>Aesthetic issues often dominate technical issues </p> </li> </ul> <p>An extension of modeling </p> <ul> <li>Represent scene models as a function of time </li> </ul> <p>Output: sequence of images that when viewed sequentially provide a sense of motion </p> <ul> <li> <p>Film: 24 frames per second </p> </li> <li> <p>Video (in general): 30 fps </p> </li> <li> <p>Virtual reality: 90 fp</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#keyframe-animation","title":"Keyframe Animation","text":"<ul> <li> <p>Animator (e.g. lead animator) creates keyframes </p> </li> <li> <p>Assistant (person or computer) creates in-between frames (\"tweening\")</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#keyframe-interpolation","title":"Keyframe Interpolation","text":"<ul> <li> <p>Think of each frame as a vector of parameter values</p> </li> <li> <p>For each parameters, we can use, for example, splines for smooth / controllable interpolation</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#physical-simulation","title":"Physical Simulation","text":"<p>Construct the phisical model, using phisical formulas to calculate the motion of objects</p> <p></p> <p>For fluids, </p> <ul> <li> <p>First simulate the motion to get the position, shape of fluids</p> </li> <li> <p>Then render the scene to get what it looks like</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#mass-spring-system","title":"Mass Spring System","text":"<p>Using a set of connected mass and spring to simulate the motion of rope, hair, cloth, etc...</p>"},{"location":"Learning/GAMES101/Animation/#a-simple-spring","title":"A Simple Spring","text":"<p>Idealized Spring: </p> <ul> <li> <p>Force pulls points together</p> </li> <li> <p>Strength proportional to displacement (Hooke's Law)  </p> </li> <li> <p>\\(k_s\\) is a spring coefficient: stiffness  </p> </li> <li> <p>Problem: this spring wants to have zero length</p> </li> </ul> \\[\\boldsymbol{f}_{a\\rightarrow b}=k_s(\\boldsymbol{b}-\\boldsymbol{a}), \\quad \\boldsymbol{f}_{b\\rightarrow a}=-\\boldsymbol{f}_{a\\rightarrow b}\\] <p></p> <p>For Non-Zero Length Spring:</p> \\[ \\boldsymbol{f}_{a \\rightarrow b} = k_s \\frac{\\boldsymbol{b} - \\boldsymbol{a}}{\\|\\boldsymbol{b} - \\boldsymbol{a}\\|} (\\|\\boldsymbol{b} - \\boldsymbol{a}\\| - l) \\] <ul> <li> <p>where \\(l\\) is the non-zero rest length</p> </li> <li> <p>Problem: it will oscillates forever</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#energy-loss","title":"Energy Loss","text":"<p>Here we use the dot notation for derivatives: </p> <ul> <li>If \\(\\boldsymbol{x}\\) is a vector for the position of a point of interest, we will use  dot notation for velocity and acceleration: \\(\\dot{\\boldsymbol{x}}=\\boldsymbol{v} \\quad\\ddot{\\boldsymbol{x}}=\\boldsymbol{a}\\)</li> </ul> <p>We can introduce the damping force to make the motion stop</p> <ul> <li> <p>Simple motion damping: \\(\\boldsymbol{f}=-k_d\\dot{\\boldsymbol{b}}\\)</p> <ul> <li> <p>Behaves like viscous drag on motion </p> </li> <li> <p>Slows down motion in the direction of velocity </p> </li> <li> <p>\\(k_d\\) is a damping coefficient </p> </li> </ul> </li> <li> <p>Problem: slows down all motion</p> <ul> <li>we want a rusty spring's oscillations to slow down, but should it also fall to the ground more slowly? </li> </ul> </li> </ul> <p></p> <p>Here comes Internal Damping for Spring, Damp only the internal, spring-driven motion</p> \\[ \\boldsymbol{f}_{\\boldsymbol{b}}=-k_d\\left({\\frac{\\boldsymbol{b}-\\boldsymbol{a}}{||\\boldsymbol{b}-\\boldsymbol{a}||}\\cdot(\\dot{\\boldsymbol{b}}-\\dot{\\boldsymbol{a}})}\\right)\\frac{\\boldsymbol{b}-\\boldsymbol{a}}{||\\boldsymbol{b}-\\boldsymbol{a}||} \\] <ul> <li> <p>where: </p> <ul> <li> <p>\\(\\boldsymbol{f}_{\\boldsymbol{b}}\\) is the damping force applied on \\(b\\)</p> </li> <li> <p>\\(\\frac{\\boldsymbol{b}-\\boldsymbol{a}}{||\\boldsymbol{b}-\\boldsymbol{a}||}\\) is the direction from \\(a\\) to \\(b\\)</p> </li> <li> <p>\\({\\frac{\\boldsymbol{b}-\\boldsymbol{a}}{||\\boldsymbol{b}-\\boldsymbol{a}||}\\cdot(\\dot{\\boldsymbol{b}}-\\dot{\\boldsymbol{a}})}\\) is the relative velocity projected to the direction from \\(a\\) to \\(b\\) (scalar)</p> </li> </ul> </li> <li> <p>Viscous drag only on change in spring length</p> <ul> <li>Won't slow group motion for the spring system  (e.g. global translation or rotation of the group)</li> </ul> </li> <li> <p>Note: This is only one specific type of damping</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#particle-system","title":"Particle System","text":"<p>Model dynamical systems as collections of large numbers of particles </p> <p>Each particle's motion is defined by a set of physical (or non-physical) forces </p> <p>Popular technique in graphics and games </p> <ul> <li> <p>Easy to understand, implement </p> </li> <li> <p>Scalable: fewer particles for speed, more for higher complexity </p> </li> </ul> <p>Challenges </p> <ul> <li> <p>May need many particles (e.g. fluids) </p> </li> <li> <p>May need acceleration structures (e.g. to find nearest particles for interactions)</p> </li> </ul> <p>For each frame in animation </p> <ul> <li> <p>[If needed] Create new particles  </p> </li> <li> <p>Calculate forces on each particle </p> </li> <li> <p>Update each particle's position and velocity </p> </li> <li> <p>[If needed] Remove dead particles </p> </li> <li> <p>Render particles</p> </li> </ul> <p>Particle System Forces: </p> <ul> <li> <p>Attraction and repulsion forces </p> <ul> <li> <p>Gravity, electromagnetism, ...</p> </li> <li> <p>Springs, propulsion, ...</p> </li> </ul> </li> <li> <p>Damping forces </p> <ul> <li>Friction, air drag, viscosity, ...</li> </ul> </li> <li> <p>Collisions </p> <ul> <li> <p>Walls, containers, fixed objects, ...</p> </li> <li> <p>Dynamic objects, character body parts, ...</p> </li> </ul> </li> </ul> <p>Simulated Flocking as an ODE</p> <ul> <li> <p>Model each bird as a particle </p> </li> <li> <p>Subject to very simple forces: </p> <ul> <li> <p>attraction to center of neighbors </p> </li> <li> <p>repulsion from individual neighbors </p> </li> <li> <p>alignment toward average trajectory of neighbors </p> </li> </ul> </li> </ul> <p>Simulate evolution of large particle system numerically </p> <p>Emergent complex behavior (also seen in fish, bees, ...)</p> <p></p>"},{"location":"Learning/GAMES101/Animation/#kinematics","title":"Kinematics","text":"<p>Articulated skeleton </p> <ul> <li> <p>Topology (what's connected to what) </p> </li> <li> <p>Geometric relations from joints </p> </li> <li> <p>Tree structure (in absence of loops) </p> </li> </ul> <p>Joint types </p> <ul> <li> <p>Pin (1D rotation) </p> </li> <li> <p>Ball (2D rotation) </p> </li> <li> <p>Prismatic joint (translation)</p> </li> </ul> <p></p> <p></p>"},{"location":"Learning/GAMES101/Animation/#forwawrd-kinematics","title":"Forwawrd Kinematics","text":"<p>Animator provides angles, and computer determines position \\(p\\) of end effector</p> <p></p> <p>Animation is described as angle parameter values as a function of time</p> <p></p> <p>Strengths </p> <ul> <li> <p>Direct control is convenient </p> </li> <li> <p>Implementation is straightforward </p> </li> </ul> <p>Weaknesses </p> <ul> <li> <p>Animation may be inconsistent with physics </p> </li> <li> <p>Time consuming for artists</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#inverse-kinematics","title":"Inverse Kinematics","text":"<p>Animator provides position of end-effector, and computer must determine joint angles that satisfy constraints</p> <p>Direct inverse kinematics: for two-segment arm, can solve for parameters analytically</p> <p></p> <p>Why is the problem hard? </p> <ul> <li> <p>Multiple solutions in configuration space</p> </li> <li> <p>Solutions may not always exist</p> </li> </ul> <p>Numerical solution to general N-link IK problem </p> <ul> <li> <p>Choose an initial configuration </p> </li> <li> <p>Define an error metric (e.g. square of distance between goal and current position) </p> </li> <li> <p>Compute gradient of error as function of configuration </p> </li> <li> <p>Apply gradient descent (or Newton's method, or other optimization procedure)</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#rigging","title":"Rigging","text":"<p>Rigging is a set of higher level controls on a character that allow more rapid &amp; intuitive modification of pose, deformations, expression, etc.</p> <ul> <li> <p>Like strings on a puppet </p> </li> <li> <p>Captures all meaningful character changes </p> </li> <li> <p>Varies from character to character </p> </li> </ul> <p>Expensive to create </p> <ul> <li> <p>Manual effort </p> </li> <li> <p>Requires both artistic and technical training</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#blend-shapes","title":"Blend Shapes","text":"<p>Instead of skeleton, interpolate directly between surfaces </p> <p>E.g., model a collection of facial expressions: </p> <p>Simplest scheme: take linear combination of vertex positions </p> <p>Spline used to control choice of weights over time</p> <p></p>"},{"location":"Learning/GAMES101/Animation/#motion-capture","title":"Motion Capture","text":"<p>Data-driven approach to creating animation sequences </p> <ul> <li> <p>Record real-world performances (e.g. person executing an activity) </p> </li> <li> <p>Extract pose as a function of time from the data collected</p> </li> </ul> <p>Strengths </p> <ul> <li> <p>Can capture large amounts of real data quickly </p> </li> <li> <p>Realism can be high </p> </li> </ul> <p>Weaknesses </p> <ul> <li> <p>Complex and costly set-ups </p> </li> <li> <p>Captured animation may not meet artistic needs, requiring alterations</p> </li> </ul> <p></p> <p>Optical Motion Capture:</p> <ul> <li> <p>Markers on subject  </p> </li> <li> <p>Positions by triangulation from multiple cameras  </p> </li> <li> <p>8+ cameras, 240 Hz, occlusions are difficult</p> </li> </ul> <p></p> <p>Challenges of Facial Animation:  Uncanny valley</p> <ul> <li> <p>In robotics and graphics </p> </li> <li> <p>As artificial character appearance approaches human realism, our emotional response goes negative, until it achieves a sufficiently convincing level of realism in expression</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#the-production-pipeline","title":"The Production Pipeline","text":""},{"location":"Learning/GAMES101/Animation/#single-particle-simulation","title":"Single Particle Simulation","text":"<p>First study motion of a single particle. Later, generalize to a multitude of particles</p> <p>To start, assume motion of particle determined by a velocity vector field that is a function of position and time: \\(v(x,y)\\)</p> <p></p> <p>We wants to compute the position of particle over time.</p> <ul> <li>This requires  solving a first order ordinary differential equation: </li> </ul> \\[ \\frac{dx}{dt}=\\dot{x}=v(x,t) \\] <ul> <li> <p>\"First-order\" refers to the first derivative being taken</p> </li> <li> <p>\"Ordinary\" means no \"partial\" derivatives, i.e. \\(x\\) is just a function of \\(t\\)</p> </li> </ul> <p>We can solve the ODE, subject to a given initial particle position \\(x_0\\), by using forward numerical integration</p> <p></p>"},{"location":"Learning/GAMES101/Animation/#eulers-method","title":"Euler's Method","text":"<p>Euler's Method (a.k.a. Forward Euler, Explicit Euler) </p> <ul> <li> <p>Simple iterative method </p> </li> <li> <p>Commonly used </p> </li> <li> <p>Very inaccurate </p> </li> <li> <p>Most often goes unstable</p> </li> </ul> \\[ \\begin{align*} \\boldsymbol{x}^{t+\\Delta{t}}=\\boldsymbol{x}^{t}+\\Delta t\\dot{\\boldsymbol{x}}^{t}\\\\ \\dot{\\boldsymbol{x}}^{t+\\Delta{t}}=\\dot{\\boldsymbol{x}}^{t}+\\Delta t\\ddot{\\boldsymbol{x}}^{t} \\end{align*} \\] <p>Errors: </p> <ul> <li> <p>With numerical integration, errors accumulate</p> </li> <li> <p>Euler integration is particularly bad</p> </li> <li> <p>Large time step has large error</p> </li> <li> <p>Errors at each time step accumulate. Accuracy decreases as simulation proceeds </p> </li> <li> <p>Accuracy may not be critical in graphics applications </p> </li> </ul> <p></p> <p>Instability: </p> <ul> <li> <p>Inaccuracies increase as time step \\(\\Delta t\\) increases </p> </li> <li> <p>Instability is a common, serious problem that can cause simulation to diverge</p> </li> <li> <p>Errors can compound, causing the simulation to diverge even when the underlying system does not </p> </li> <li> <p>Lack of stability is a fundamental problem in simulation, and cannot be ignored</p> </li> <li> <p>For a ripple-like velocity field, the correct behavior of a particle should be to continuously rotate around the center, but Euler's method causes it to move outward in a centrifugal motion.</p> <ul> <li>This will always happen no matter how small the time step is.</li> </ul> </li> <li> <p>For the velocity field as right, the particle will oscillate with incresing amplitude in Euler Method</p> </li> </ul> <p></p> <p></p>"},{"location":"Learning/GAMES101/Animation/#combating-instability","title":"Combating Instability","text":"<p>Midpoint method / Modified Euler </p> <ul> <li>Average velocities at start and endpoint  </li> </ul> <p>Adaptive step size </p> <ul> <li>Compare one step and two half-steps, recursively, until error is acceptable </li> </ul> <p>Implicit methods </p> <ul> <li>Use the velocity at the next time step (hard) </li> </ul> <p>Position-based / Verlet integration </p> <ul> <li>Constrain positions and velocities of particles after time step</li> </ul>"},{"location":"Learning/GAMES101/Animation/#midpoint-method","title":"Midpoint Method","text":"<p>Steps:</p> <ul> <li> <p>Compute Euler step (a) </p> </li> <li> <p>Compute derivative at midpoint of Euler step (b) </p> </li> <li> <p>Update position using midpoint derivative (c) </p> </li> </ul> \\[ \\begin{aligned} x_{\\mathrm{mid}} &amp; =x(t)+\\Delta t/2\\cdot v(x(t),t) \\\\ x(t+\\Delta t) &amp; =x(t)+\\Delta t\\cdot v(x_{\\mathrm{mid}},t) \\end{aligned} \\] <p></p>"},{"location":"Learning/GAMES101/Animation/#modified-euler","title":"Modified Euler","text":"<ul> <li> <p>Average velocity at start and end of step </p> </li> <li> <p>Actually a local quadratic model</p> </li> <li> <p>Better results</p> </li> </ul> \\[ \\begin{aligned}  &amp; \\boldsymbol{x}^{t+\\Delta t}=\\boldsymbol{x}^{t}+\\frac{\\Delta t}{2}(\\dot{\\boldsymbol{x}}^{t}+\\dot{\\boldsymbol{x}}^{t+\\Delta t}) \\\\  &amp;  \\dot{\\boldsymbol{x}}^{t+\\Delta t}=\\dot{\\boldsymbol{x}}^t+\\Delta t\\ddot{\\boldsymbol{x}}^t \\\\  &amp; \\boldsymbol{x}^{t+\\Delta t}=\\boldsymbol{x}^t+\\Delta t\\dot{\\boldsymbol{x}}^t+\\frac{(\\Delta t)^2}{2}\\ddot{\\boldsymbol{x}}^t \\end{aligned} \\]"},{"location":"Learning/GAMES101/Animation/#adaptive-step-size","title":"Adaptive Step Size","text":"<p>Adaptive step size </p> <ul> <li> <p>Technique for choosing step size based on error estimate </p> </li> <li> <p>Very practical technique </p> </li> <li> <p>But may need very small steps! </p> </li> </ul> <p>Repeat until error is below threshold: </p> <ul> <li> <p>Compute \\(x_T\\) an Euler step, size \\(T\\) </p> </li> <li> <p>Compute \\(x_{T/2}\\) two Euler steps, size \\(T/2\\) </p> </li> <li> <p>Compute error \\(\\|x_T-x_{T/2}\\|\\) </p> </li> <li> <p>If (error &gt; threshold) reduce step size and try again</p> </li> </ul> <p>We may have different step size at different position </p> <p></p> <p></p>"},{"location":"Learning/GAMES101/Animation/#implicit-euler-method","title":"Implicit Euler Method","text":"<ul> <li> <p>Informally called backward methods </p> </li> <li> <p>Use derivatives in the future, for the current step</p> <ul> <li>We don't know the velocity and acceleration of next step in current step</li> </ul> </li> </ul> \\[ \\begin{align*} &amp;\\boldsymbol{x}^{t+\\Delta t}=\\boldsymbol{x}^t+\\Delta t\\boldsymbol{\\dot{x}}^{t+\\Delta t}\\\\ &amp;\\boldsymbol{\\dot{x}}^{t+\\Delta t}=\\boldsymbol{\\dot{x}}^t+\\Delta t\\boldsymbol{\\ddot{x}}^{t+\\Delta t} \\end{align*} \\] <ul> <li> <p>Need to solve nonlinear problem for \\(\\boldsymbol{x}^{t+\\Delta t}\\) and \\(\\dot{\\boldsymbol{x}}^{t+\\Delta t}\\)</p> </li> <li> <p>Use root-finding algorithm, e.g. Newton's method </p> </li> <li> <p>Offers much better stability</p> </li> </ul> <p>How to determine / quantize \"stability\"?</p> <ul> <li> <p>We use the local truncation error (every step) / total accumulated error (overall)</p> </li> <li> <p>Absolute values do not matter, but the orders w.r.t. step</p> </li> <li> <p>Implicit Euler has order 1, which means that</p> <ul> <li> <p>Local truncation error: \\(O(h^2)\\) and</p> </li> <li> <p>Global truncation error: \\(O(h)\\) (\\(h\\) is the step, i.e. \\(\\Delta t\\))</p> </li> </ul> </li> <li> <p>Understanding of \\(O(h)\\)</p> <ul> <li>If we halve \\(h\\), we can expect the error to halve as well</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#runge-kutta-families","title":"Runge-Kutta Families","text":"<p>A family of advanced methods for solving ODEs </p> <ul> <li> <p>Especially good at dealing with non-linearity </p> </li> <li> <p>It's order-four version is the most widely used, a.k.a. RK4</p> </li> </ul> <p>Initial Condition: </p> \\[ \\frac{dy}{dt}=f(t,y),\\quad y(t_0)=y_0. \\] <p>RK4 solution: </p> \\[ \\begin{aligned}  &amp; y_{n+1}=y_n+\\frac{1}{6}h\\left(k_1+2k_2+2k_3+k_4\\right), \\\\  &amp; t_{n+1}=t_{n}+h \\end{aligned} \\] <p>where: </p> \\[ \\begin{aligned} k_{1} &amp; =f(t_n,y_n), &amp; k_3 &amp; =f\\left(t_n+\\frac{h}{2},y_n+h\\frac{k_2}{2}\\right), \\\\ k_{2} &amp; =f\\left(t_n+\\frac{h}{2},y_n+h\\frac{k_1}{2}\\right), &amp; k_4 &amp; =f\\left(t_n+h,y_n+hk_3\\right). \\end{aligned} \\]"},{"location":"Learning/GAMES101/Animation/#position-based-verlet-integration","title":"Position-based / Verlet integration","text":"<p>Idea:</p> <ul> <li> <p>After modified Euler forward-step, constrain positions of particles to prevent divergent, unstable behavior</p> </li> <li> <p>Use constrained positions to calculate velocity</p> </li> <li> <p>Both of these ideas will dissipate energy, stabilize</p> </li> </ul> <p>Pros / Cons</p> <ul> <li> <p>Fast and simple</p> </li> <li> <p>Not physically based, dissipates energy (error)</p> </li> </ul>"},{"location":"Learning/GAMES101/Animation/#rigid-body-simulation","title":"Rigid Body Simulation","text":"<p>Simple case</p> <ul> <li> <p>Similar to simulating a particle </p> </li> <li> <p>Just consider a bit more properties</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#fluid-simulation","title":"Fluid Simulation","text":""},{"location":"Learning/GAMES101/Animation/#a-simple-position-based-method","title":"A Simple Position-Based Method","text":"<p>Key idea </p> <ul> <li> <p>Assuming water is composed of small rigid-body spheres </p> </li> <li> <p>Assuming the water cannot be compressed (i.e. const. density) </p> </li> <li> <p>So, as long as the density changes somewhere, it should be \u201ccorrected\u201d via changing the positions of particles </p> <ul> <li>Change the position of a particle may effect the position of others</li> </ul> </li> <li> <p>You need to know the gradient of the density anywhere w.r.t. each particle's position </p> </li> <li> <p>Update? Just gradient descent!</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Animation/#eulerian-vs-lagrangian","title":"Eulerian vs. Lagrangian","text":"<p>Two different views to simulating large collections of matters</p> <p></p>"},{"location":"Learning/GAMES101/Animation/#material-point-method-mpm","title":"Material Point Method (MPM)","text":"<p>Hybrid, combining Eulerian and Lagrangian views </p> <ul> <li> <p>Lagrangian: consider particles carrying material properties </p> </li> <li> <p>Eulerian: use a grid to do numerical integration </p> </li> <li> <p>Interaction: particles transfer properties to the grid, grid performs update, then interpolate back to particles</p> </li> </ul>"},{"location":"Learning/GAMES101/GAMES101/","title":"GAMES101","text":""},{"location":"Learning/GAMES101/GAMES101/#transformation","title":"Transformation","text":"<p>\u4e3a\u4ec0\u4e48\u5f15\u5165\u9f50\u6b21\u53d8\u6362\uff1f\u628a\u6240\u6709\u53d8\u6362\u90fd\u5199\u4e3a\u4e00\u4e2a\u77e9\u9635\u548c\u4e00\u4e2a\u5411\u91cf\u76f8\u4e58\u7684\u5f62\u5f0f \u65cb\u8f6c\u4e00\u822c\u90fd\u662f\u7ed5\u539f\u70b9\u65cb\u8f6c \u5199\u6210\u4e00\u4e2a\u77e9\u9635\u662f\u5148\u7ebf\u6027\u53d8\u6362\u518d\u5e73\u79fb</p>"},{"location":"Learning/GAMES101/GAMES101/#viewing-transformation","title":"Viewing Transformation","text":"<p>Define camera first: </p> <ul> <li> <p>Position \\(\\vec{e}\\)</p> </li> <li> <p>Look-at / gaze direction \\(\\hat{g}\\)</p> </li> <li> <p>Up direction \\(\\hat{t}\\)</p> </li> </ul> <p>How to take a photo?(MVP)</p> <ul> <li> <p>Find a good place and arrange people (model transformation)</p> </li> <li> <p>Find a good \"angle\" to put the camera (view transformation)</p> </li> <li> <p>Cheese! (projection transformation)</p> </li> </ul>"},{"location":"Learning/GAMES101/GAMES101/#view-transformation","title":"View Transformation","text":"<p>Key observation</p> <ul> <li>If the camera and all objects move together, the \"photo\" will be the same</li> </ul> <p>How about that we always transform the camera using \\(M_{view}\\) to</p> <ul> <li> <p>The origin, up at Y, look at -Z</p> </li> <li> <p>And transform the objects along with the camera</p> </li> </ul> <p>Let's write \\(M_{view}=R_{view}T_{view}\\)</p> <ul> <li>\\(T_{view}\\) translate \\(e\\) to origin</li> </ul> \\[ T_{view}= \\begin{bmatrix} 1&amp;0&amp;0&amp;-x_{e}\\\\ 0&amp;1&amp;0&amp;-y_{e}\\\\ 0&amp;0&amp;1&amp;-z_{e}\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix} \\] <ul> <li> <p>\\(R_{view}\\) rotate \\(g\\) to \\(-Z\\), \\(t\\) to \\(Y\\) and \\(g\\times t\\) to \\(X\\)</p> <ul> <li>Consider the inverse transformation: rotate \\(Z\\) to \\(-g\\), \\(Y\\) to \\(t\\) and \\(X\\) to \\(g\\times t\\)</li> </ul> </li> </ul> \\[ R_{view}^{-1}= \\begin{bmatrix} x_{\\hat{g}\\times \\hat{t}}&amp;x_{t}&amp;x_{-g}&amp;0\\\\ y_{\\hat{g}\\times \\hat{t}}&amp;y_{t}&amp;y_{-g}&amp;0\\\\ z_{\\hat{g}\\times \\hat{t}}&amp;z_{t}&amp;z_{-g}&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix} \\stackrel{inverse(transpose)}{\\longrightarrow} R_{view}= \\begin{bmatrix} x_{\\hat{g}\\times \\hat{t}}&amp;y_{\\hat{g}\\times \\hat{t}}&amp;z_{\\hat{g}\\times \\hat{t}}&amp;0\\\\ x_{t}&amp;y_{t}&amp;z_{t}&amp;0\\\\ x_{-g}&amp;y_{-g}&amp;z_{-g}&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix} \\]"},{"location":"Learning/GAMES101/GAMES101/#projection-transformation","title":"Projection Transformation","text":""},{"location":"Learning/GAMES101/GAMES101/#orthographic-projection","title":"Orthographic Projection","text":"<p>A simple way of understanding</p> <ul> <li> <p>Camera located at origin, looking at -Z, up at Y</p> </li> <li> <p>Drop Z coordinate</p> </li> <li> <p>Translate and scale the resulting rectangle to \\([-1, 1]^2\\)</p> </li> </ul> <p>In general</p> <ul> <li> <p>We want to map a cuboid \\([I, r] \\times [b, t] \\times [f, n]\\) to the canonical cube \\([-1,1]^3\\)</p> <ul> <li> <p>Center cuboid by translating</p> </li> <li> <p>Scale into \"canonical\" cube</p> </li> </ul> </li> </ul> <p></p> <p>Transformation matrix</p> <ul> <li>Translate (center to origin) first, then scale (length/width/height to 2)</li> </ul> \\[ M_{ortho}= \\begin{bmatrix} \\frac{2}{r-l}&amp;0&amp;0&amp;0\\\\ 0&amp;\\frac{2}{t-b}&amp;0&amp;0\\\\ 0&amp;0&amp;\\frac{2}{n-f}&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix} \\begin{bmatrix} 1&amp;0&amp;0&amp;-\\frac{r+l}{2}\\\\ 0&amp;1&amp;0&amp;-\\frac{t+b}{2}\\\\ 0&amp;0&amp;1&amp;-\\frac{n+f}{2}\\\\ 0&amp;0&amp;0&amp;1 \\end{bmatrix} \\] <p>Caveat</p> <ul> <li>Looking at / along -Z is making near and far not intuitive (n &gt; f)</li> <li>FYI: that's why OpenGL (a Graphics API) uses left hand coords.</li> </ul>"},{"location":"Learning/GAMES101/GAMES101/#perspective-projection","title":"Perspective Projection","text":"<p>How to do perspective projection</p> <ul> <li> <p>First \"squish\" the frustum into a cuboid (\\(n \\rightarrow n, f \\rightarrow f\\)) (\\(M_{persp-&gt;ortho}\\))</p> </li> <li> <p>Do orthographic projection (\\(M_{ortho}\\), already known!)</p> </li> </ul> <p></p> <p>In order to find a transformation</p> <ul> <li>Find the relationship between transformed points \\((x', y', z')\\) and the original points \\((x, y, z)\\)</li> </ul> <p></p> \\[ y'=\\frac{n}{z}y \\qquad x'=\\frac{n}{z}x \\] <p>In homogeneous coordinates,</p> \\[ M_{persp\\rightarrow ortho} \\begin{pmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{pmatrix} \\,= \\begin{pmatrix} nx/z\\\\ ny/z\\\\ unknown\\\\ 1\\\\ \\end{pmatrix} \\, == \\begin{pmatrix} nx\\\\ ny\\\\ unknown\\\\ z\\\\ \\end{pmatrix}\\\\ M_{persp\\rightarrow ortho}= \\begin{pmatrix} n&amp;0&amp;0&amp;0\\\\ 0&amp;n&amp;0&amp;0\\\\ ?&amp;?&amp;?&amp;?\\\\ 0&amp;0&amp;1&amp;0 \\end{pmatrix} \\] <p>Observation: the third row is responsible for \\(z'\\)</p> <ul> <li> <p>Any point on the near plane will not change</p> </li> <li> <p>Any point's \\(z\\) on the far plane will not change</p> </li> </ul> <p>From the two property we have: </p> \\[ M_{persp\\rightarrow ortho} \\begin{pmatrix} x\\\\ y\\\\ n\\\\ 1\\\\ \\end{pmatrix} \\,= \\begin{pmatrix} x\\\\ y\\\\ n\\\\ 1\\\\ \\end{pmatrix} \\,== \\begin{pmatrix} nx\\\\ ny\\\\ n^2\\\\ n\\\\ \\end{pmatrix}\\\\ M_{persp\\rightarrow ortho} \\begin{pmatrix} 0\\\\ 0\\\\ f\\\\ 1\\\\ \\end{pmatrix} \\,= \\begin{pmatrix} 0\\\\ 0\\\\ f\\\\ 1\\\\ \\end{pmatrix} \\,== \\begin{pmatrix} 0\\\\ 0\\\\ f^2\\\\ f\\\\ \\end{pmatrix} \\] <p>Now we can get the entire matrix</p> \\[ M_{persp\\rightarrow ortho}= \\begin{pmatrix} n&amp;0&amp;0&amp;0\\\\ 0&amp;n&amp;0&amp;0\\\\ 0&amp;0&amp;n+f&amp;-nf\\\\ 0&amp;0&amp;1&amp;0 \\end{pmatrix} \\] <p>What's next?</p> <ul> <li>Do orthographic projection (\\(M_{ortho}\\)) to finish</li> <li>\\(M_{persp} = M_{ortho} M_{persp\\rightarrow ortho}\\)</li> </ul> <p>What's near plane's \\(l, r, b, t\\)(left, right, bottom, top) then?</p> <ul> <li> <p>If explicitly specified, good</p> </li> <li> <p>Sometimes people prefer: vertical field-of-view (\\(fovY\\)) and aspect ratio(assume symmetry i.e. I = -r, b = -t)</p> </li> </ul> <p></p> \\[ tan\\frac{fovY}{2}=\\frac{t}{|n|}\\\\ tan\\frac{fovX}{2}=\\frac{r}{|n|}\\\\ aspect=\\frac{r}{t} \\] <p>What's after MVP?</p>"},{"location":"Learning/GAMES101/GAMES101/#viewport-transformation","title":"Viewport Transformation","text":"<p>What is a screen?</p> <ul> <li> <p>An array of pixels</p> </li> <li> <p>Size of the array: resolution</p> </li> <li> <p>A typical kind of raster display</p> </li> </ul> <p>Raster == screen in German</p> <ul> <li>Rasterize == drawing onto the screen</li> </ul> <p>Pixel (FYI, short for \"picture element\")</p> <ul> <li> <p>For now: A pixel is a little square with uniform color</p> </li> <li> <p>Color is a mixture of (red, green, blue)</p> </li> </ul> <p>Defining the screen space</p> <ul> <li> <p>Pixels' indices are in the form of \\((x, y)\\), where both \\(x\\) and \\(y\\) are integers</p> </li> <li> <p>Pixels' indices are from \\((0, 0)\\) to \\((width - 1, height - 1)\\)</p> </li> <li> <p>Pixel \\((x, y)\\) is centered at \\((x+ 0.5, y + 0.5)\\)</p> </li> <li> <p>The screen covers range \\((0, 0)\\) to \\((width, height)\\)</p> </li> </ul> <p>Canonical Cube to Screen:</p> <ul> <li> <p>Irrelevant to \\(z\\)</p> </li> <li> <p>Transform in \\(xy\\) plane: \\([-1, 1]^2\\) to \\([0, width] \\times [0, height]\\)</p> </li> <li> <p>Viewport transform matrix:</p> </li> </ul> \\[ M_{viewport}= \\begin{pmatrix} \\frac{width}{2}&amp;0&amp;0&amp;\\frac{width}{2}\\\\ 0&amp;\\frac{height}{2}&amp;0&amp;\\frac{height}{2}\\\\ 0&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{pmatrix} \\]"},{"location":"Learning/GAMES101/Geometry/","title":"Geometry","text":""},{"location":"Learning/GAMES101/Geometry/#ways-to-represent-geometry","title":"Ways to Represent Geometry","text":""},{"location":"Learning/GAMES101/Geometry/#implicit","title":"Implicit","text":"<ul> <li> <p>algebraic surface</p> </li> <li> <p>level sets</p> </li> <li> <p>distance functions</p> </li> <li> <p>...</p> </li> </ul> <p>Based on classifying points</p> <ul> <li> <p>It tells you the specified relationships that the points satisfy, but not the coordinates.</p> </li> <li> <p>E.g. sphere: all points in 3D, where \\(x_2+y_2+z_2 = 1\\)</p> </li> <li> <p>More generally, \\(f(x,y,z) = 0\\)</p> </li> <li> <p>Easy to decide whether the given point is inside/on/outside the surface</p> </li> <li> <p>Hard to find what points lie on it</p> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#constructive-solid-geometry","title":"Constructive Solid Geometry","text":"<p>Combine implicit geometry via Boolean operations</p> <p></p>"},{"location":"Learning/GAMES101/Geometry/#distance-function","title":"Distance Function","text":"<p>Instead of Booleans, gradually blend surfaces together using Distance functions:</p> <ul> <li> <p>Distance functions: given a position, return the minimum distance to object</p> </li> <li> <p>The distance can be signed (for example, negative means the point is inside the object)</p> </li> <li> <p>blend, for example, can be linear interpolation</p> </li> </ul> <p></p> <ul> <li>Can blend any two distance functions \\(d1, d2\\): </li> </ul> <p></p> <ul> <li>The surface can be found via \\(d(\\mathbf{x})=0\\)</li> </ul>"},{"location":"Learning/GAMES101/Geometry/#level-set-method","title":"Level Set Method","text":"<p>Closed-form equations are hard to describe complex shapes</p> <p>Alternative: store a grid of values approximating function</p> <ul> <li> <p>Surface is found where (bilinear) interpolated values equal zero</p> </li> <li> <p>Provides much more explicit control over shape (like a texture)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#fractals","title":"Fractals","text":"<ul> <li> <p>Exhibit self-similarity, detail at all scales</p> </li> <li> <p>\"Language\" for describing natural phenomena</p> </li> <li> <p>Hard to control shape!</p> </li> </ul> <p></p> <p>Level set encodes distance to air-liquid boundary</p> <p></p>"},{"location":"Learning/GAMES101/Geometry/#prons-cons","title":"Prons &amp; Cons","text":"<p>Pros:</p> <ul> <li> <p>compact description (e.g., a function)</p> </li> <li> <p>certain queries easy (inside object, distance to surface)</p> </li> <li> <p>good for ray-to-surface intersection (more later)</p> </li> <li> <p>for simple shapes, exact description / no sampling error</p> </li> <li> <p>easy to handle changes in topology (e.g., fluid)</p> </li> </ul> <p>Cons:</p> <ul> <li>difficult to model complex shapes</li> </ul>"},{"location":"Learning/GAMES101/Geometry/#explicit","title":"Explicit","text":"<ul> <li> <p>point cloud</p> </li> <li> <p>polygon mesh</p> </li> <li> <p>subdivision, NURBS</p> </li> <li> <p>...</p> </li> </ul> <p>All points are given directly or via parameter mapping</p> <ul> <li> <p>Genernally, \\(f:\\mathbb{R}^2\\to\\mathbb{R}^3;(u,v)\\mapsto(x,y,z)\\)</p> </li> <li> <p>Easy to find what points lie on this surface: Just plug in \\((u,v)\\) values!</p> </li> <li> <p>Hard to decide whether the point is inside or outside</p> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#point-cloud","title":"Point Cloud","text":"<ul> <li> <p>Easiest representation: list of points \\((x,y,z)\\)</p> </li> <li> <p>Easily represent any kind of geometry</p> </li> <li> <p>Useful for LARGE datasets (&gt;&gt;1 point/pixel)</p> </li> <li> <p>Often converted into polygon mesh</p> </li> <li> <p>Difficult to draw in undersampled regions</p> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#polygon-mesh","title":"Polygon Mesh","text":"<ul> <li> <p>Store vertices &amp; polygons (often triangles or quads)</p> </li> <li> <p>Easier to do processing / simulation, adaptive sampling</p> </li> <li> <p>More complicated data structures</p> </li> <li> <p>Perhaps most common representation in graphics</p> </li> <li> <p>Often stored in Wavefront Object File Format(.obj)</p> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#curves","title":"Curves","text":""},{"location":"Learning/GAMES101/Geometry/#bezier-curve","title":"Bezier Curve","text":"<p>The Bezier Curve is controlled by \\(n\\) control points.</p> <ul> <li> <p>The curve starts at the first point and ends at the last point.</p> </li> <li> <p>The tangent line at the start passes through the first two points, and the tangent line at the end passes through the last two points.</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#evaluating-bezier-curvesde-casteljau-algorithm","title":"Evaluating Bezier Curves(de Casteljau Algorithm)","text":"<p>Consider three points (quadratic Bezier), we can use a parameter \\(t\\in[0,1]\\) to control the drawing procedure</p> <ul> <li> <p>Each point in curve corresponds to a \\(t\\)</p> </li> <li> <p>For the image below, \\(\\frac{\\mathbf{b}_0\\mathbf{b}_0^1}{\\mathbf{b}_0\\mathbf{b}_1}=\\frac{\\mathbf{b}_1\\mathbf{b}_1^1}{\\mathbf{b}_1\\mathbf{b}_2}=\\frac{\\mathbf{b}_0^1\\mathbf{b}_0^2}{\\mathbf{b}_0^1\\mathbf{b}_1^1}=t\\). </p> </li> <li> <p>\\(b_0^2\\) is on curve</p> </li> <li> <p>Compute \\(b_0^2\\) for every \\(t\\) to draw the curve</p> </li> </ul> <p></p> <p>For four input points (Cubic Bezier), same recursive linear interpolations</p> <p></p>"},{"location":"Learning/GAMES101/Geometry/#algebratic-formula","title":"Algebratic Formula","text":"<p>de Casteljau algorithm gives a pyramid of coefficients</p> <ul> <li>Every rightward arrow is multiplication by \\(t\\), Every leftward arrow by \\((1-t)\\)</li> </ul> <p></p> <p>The coefficient for each point is actually binomial coefficient.</p> <p>For a quadratic Bezier curve, </p> \\[ \\mathbf{b}_0^2(t)=(1-t)^2\\mathbf{b}_0+2t(1-t)\\mathbf{b}_1+t^2\\mathbf{b}_2 \\] <p>The Bernstein form of a Bezier curve of order \\(n\\): </p> \\[ \\mathbf{b}^n(t)=\\mathbf{b}_0^n(t)=\\sum_{j=0}^n\\mathbf{b}_jB_j^n(t)  \\] <ul> <li> <p>\\(\\mathbf{b}^n(t)\\) is the Bezier curve with order \\(n\\) (vector polynomial of degree \\(n\\))</p> </li> <li> <p>\\(\\mathbf{b}_j \\in \\mathbb{R}^N\\) are Bezier control points</p> </li> <li> <p>\\(B_i^n(t)={\\binom{n}{i}}t^i(1-t)^{n-i}\\) are Bernstein polynomial (scalar polynomial of degree \\(n\\))</p> </li> <li> <p>The points on Bezier curve are actually a weighted sum of control points, since \\(\\sum_{j=0}^nB_j^n(t)=1\\)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#properties-of-bezier-curves","title":"Properties of B\u00e9zier Curves","text":"<p>Interpolates endpoints</p> <ul> <li>For cubic B\u00e9zier: \\(\\mathbf{b}(0)=\\mathbf{b}_0;\\quad\\mathbf{b}(1)=\\mathbf{b}_3\\)</li> </ul> <p>Tangent to end segments</p> <ul> <li>Cubic case: \\(\\mathbf{b}^{\\prime}(0)=3(\\mathbf{b}_1-\\mathbf{b}_0);\\quad\\mathbf{b}^{\\prime}(1)=3(\\mathbf{b}_3-\\mathbf{b}_2)\\)</li> </ul> <p>Affine transformation property</p> <ul> <li>Transform curve by transforming control points</li> </ul> <p>Convex hull property</p> <ul> <li> <p>Curve is within convex hull of control points</p> </li> <li> <p>Convex hull: the smallest convex shape that can enclose a set of points in a plane or space</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#piecewise-bezier-curves","title":"Piecewise B\u00e9zier Curves","text":"<p>Higher-Order Bezier Curves are hard to control</p> <ul> <li>The middle doesn't twist as much as we expected.</li> </ul> <p></p> <ul> <li> <p>Instead, chain many low-order B\u00e9zier curve</p> </li> <li> <p>Piecewise cubic Bezier is the most common technique</p> </li> </ul> <p></p> <p>Below are two B\u00e9zier curves, let's call the left one \\(\\mathbf{a}\\) and the right one \\(\\mathbf{b}\\)</p> <ul> <li> <p>\\(C^0\\) continuity: \\(\\mathbf{a}_n=\\mathbf{b}_0\\)</p> </li> <li> <p>\\(C^1\\) continuity: \\(\\mathbf{a}_n=\\mathbf{b}_0=\\frac{1}{2}(\\mathbf{a}_{n-1}+\\mathbf{b}_1)\\)</p> </li> <li> <p>where \\(\\mathbf{a}_{n}\\) is the last control point of \\(\\mathbf{a}\\) and \\(\\mathbf{b}_{0}\\) is the first control point of \\(\\mathbf{b}\\)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#other-types-of-splines","title":"Other Types of Splines","text":"<ul> <li> <p>Spline</p> <ul> <li> <p>a continuous curve constructed so as to pass through a given setof points and have a certain number of continuous derivatives.</p> </li> <li> <p>In short, a curve under control</p> </li> </ul> </li> <li> <p>B-splines</p> <ul> <li> <p>Short for basis splines</p> </li> <li> <p>Require more information than Bezier curves</p> </li> <li> <p>Satisfy all important properties that B\u00e9zier curves have (i.e. superset)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#surface","title":"Surface","text":""},{"location":"Learning/GAMES101/Geometry/#bezier-surfaces","title":"B\u00e9zier Surfaces","text":"<p>Extend B\u00e9zier curves to surfaces</p>"},{"location":"Learning/GAMES101/Geometry/#evaluating-bezier-surfaces","title":"Evaluating B\u00e9zier Surfaces","text":"<p>We can evaluating surface position for parameters \\((u,v)\\)</p> <p>For bi-cubic Bezier surface patch,</p> <ul> <li> <p>Input: 4x4 control points</p> </li> <li> <p>Output: 2D surface parameterized by \\((u,v)\\) in \\([0,1]^2\\)</p> </li> </ul> <p></p> <ul> <li>Goal: Evaluate surface position corresponding to \\((u,v)\\)</li> </ul> <p>(u,v)-separable application of de Casteljau algorithm</p> <ul> <li> <p>Use de Casteljau to evaluate point \\(u\\) on each of the 4 Bezier curves in \\(u\\). This gives 4 control points for the \"moving\" Bezier curve</p> </li> <li> <p>Use 1D de Casteljau to evaluate point \\(v\\) on the \"moving\" curve</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#mesh","title":"Mesh","text":"<p>Mesh Operations: </p> <ul> <li> <p>Mesh subdivision: Increase the number of triangles to improve resolution</p> </li> <li> <p>Mesh simplification: Decrease resolution while preserving shape/appearance</p> </li> <li> <p>Mesh regularization: Modify sample distribution to improve quality with same #triangles</p> </li> </ul>"},{"location":"Learning/GAMES101/Geometry/#mesh-subdivision","title":"Mesh Subdivision","text":""},{"location":"Learning/GAMES101/Geometry/#loop-subdivision","title":"Loop Subdivision","text":"<p>Loop Subdivision is a common subdivision rule for triangle mesh</p> <ul> <li>First, create more triangles (vertices)<ul> <li>Split each triangle into four</li> </ul> </li> </ul> <p></p> <ul> <li>Second, tune their positions<ul> <li>Assign new vertex positions according to weights</li> <li>New / old vertices updated differently</li> </ul> </li> </ul> <p>For new vertices, update to \\(\\frac{3}{8}(A+B)+\\frac{1}{8}(C+D)\\)</p> <p></p> <p>For old vertices, update to: \\((1-n\\cdot u)\\cdot position_{original}+u\\cdot sum(position_{neighbors})\\)</p> <ul> <li>\\(n\\): vertex degree</li> <li>\\(u\\): \\(\\frac{3}{16}\\) if \\(n=3,\\frac{3}{8n}\\) otherwise</li> <li>A weighted sum of its original position and the positions of its neighbors.</li> </ul> <p></p> <ul> <li>\\(C^2\\) continuity on regular meshes</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#catmull-clark-subdivision","title":"Catmull-Clark Subdivision","text":"<p>Catmull-Clark Subdivision can be used for general meshes, while Loop Subdivision is only suitable for triangle meshes</p> <p>Define: </p> <ul> <li> <p>Non-quad face: faces that don't have 4 vertices</p> </li> <li> <p>Extraordinary vertex: vertex with \\(degree\\neq 4\\)</p> </li> </ul> <p></p> <p>Subdivision Step: </p> <ul> <li> <p>Add vertex in the middle of each face </p> </li> <li> <p>Add midpoint on each edge</p> </li> <li> <p>Connect all new vertices</p> </li> </ul> <p></p> <p>After one subdivision: </p> <ul> <li> <p>How many extraordinary vertices? </p> <ul> <li>2(original extraordinary point) + 2 (new vertice in the middle of non-quad faces)</li> </ul> </li> <li> <p>How many non-quad faces?</p> <ul> <li>None. Each non-quad face becomes many quad faces with one additional extraordinary point.</li> </ul> </li> </ul> <p></p> <p>Vertex Update Rules (Quad Mesh):</p> <p></p> <p>Convergence: Overall Shape and Creases</p> <p></p>"},{"location":"Learning/GAMES101/Geometry/#mesh-simplification","title":"Mesh simplification","text":"<p>Goal: reduce number of mesh elements while maintaining the overall shape</p> <p></p>"},{"location":"Learning/GAMES101/Geometry/#edge-collapsing","title":"Edge Collapsing","text":"<p>We can simplify a mesh using edge Collapsing</p> <ul> <li>Removing an edge by merging its two vertices into a single vertex.</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Geometry/#quadric-error-metrics","title":"Quadric Error Metrics","text":"<ul> <li> <p>How much geometric error is introduced by simplification?</p> </li> <li> <p>Not a good idea to perform local averaging of vertices</p> </li> <li> <p>Quadric error: new vertex should minimize its sum of square distance (L2 distance) to previously related triangle planes!</p> </li> </ul> <p></p> <p>How to collapse edges? </p> <ul> <li>put the new point on the position that minimizes quadric error</li> </ul> <p>Which edge should we collapse?</p> <ul> <li> <p>assign each edge a score with quadric error metric</p> </li> <li> <p>approximate distance to surface as sum of distances to planes containing triangles</p> </li> <li> <p>iteratively collapse the edge with the smallest score and update the scores of the edges affected by the collapsed edge.</p> <ul> <li>use Heap to maintain scores</li> </ul> </li> <li> <p>actually a greedy algorithm: try to find global optimal with local optimal. But has great results</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/","title":"Miscellaneous","text":""},{"location":"Learning/GAMES101/Miscellaneous/#material","title":"Material","text":"<p>What is material in CG?</p>"},{"location":"Learning/GAMES101/Miscellaneous/#material-brdf","title":"Material == BRDF","text":""},{"location":"Learning/GAMES101/Miscellaneous/#diffuse-lambertian-material","title":"Diffuse / Lambertian Material","text":"<p>Light is equally reflected in each output direction.</p> <p></p> <p>Suppose the incident lighting is uniform and the surface doesn't emit light, then we have: </p> \\[ \\begin{align*} L_o(\\omega_o)&amp;=\\int_{H^2}f_rL_i(\\omega_i)\\cos{\\theta_i}d\\omega_i\\\\ &amp;=f_rL_i\\int_{H^2}\\cos{\\theta_i}d\\omega_i\\\\ &amp;=\\pi f_r L_i \\end{align*} \\] <ul> <li> <p>\\(L_i(\\omega_i)\\) is a constant since the incident lighting is uniform</p> </li> <li> <p>For diffuse material, if the surface doesn't absorb light, then the outgoing irradiance is identical to the incoming irradiance in any direction.</p> </li> <li> <p>\\(f_r=\\frac{\\rho }{\\pi}\\), where \\(\\rho\\in[0,1]\\) is called albedo</p> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#glossy-material","title":"Glossy material","text":""},{"location":"Learning/GAMES101/Miscellaneous/#ideal-reflective-refractive-material-bsdf","title":"Ideal reflective / refractive material (BSDF)","text":""},{"location":"Learning/GAMES101/Miscellaneous/#perfect-specular-reflection","title":"Perfect Specular Reflection","text":"<p>\\(\\theta\\) describes the incident angle with respect to \\(\\vec{n}\\), while \\(\\phi\\) describes the rotation on surface</p> <p>For incoming light and outgoing light \\(\\omega_i\\) and \\(\\omega_o\\), we have:</p> \\[ \\omega_o+\\omega_i=2\\cos{\\theta}\\ \\vec{n}=2(\\omega_i\\cdot\\vec{n})\\vec{n}\\\\ \\omega_o=-\\omega_i+2(\\omega_i\\cdot \\vec{n})\\vec{n} \\] <ul> <li>The BRDF of perfect specular reflection can be described by impulse function, which is nonzero only at outgoing direction.</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#specular-refraction","title":"Specular Refraction","text":"<ul> <li> <p>In addition to reflecting off surface, light may be transmitted through surface.</p> </li> <li> <p>Light refracts when it enters a new medium.</p> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#snells-law-transmitted-angle-depends-on","title":"Snell's Law: Transmitted angle depends on","text":"<ul> <li> <p>index of refraction (IOR) for incident ray</p> </li> <li> <p>index of refraction (IOR) for exiting ray</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#law-of-refraction","title":"Law of Refraction:","text":"\\[ \\begin{aligned} \\eta_{i}\\operatorname{sin}\\theta_{i} &amp; =\\eta_t\\sin\\theta_t \\\\ \\cos\\theta_{t} &amp; =\\sqrt{1-\\sin^2\\theta_t} \\\\  &amp; =\\sqrt{1-\\left(\\frac{\\eta_i}{\\eta_t}\\right)^2\\sin^2\\theta_i} \\\\  &amp; =\\sqrt{1-\\left(\\frac{\\eta_i}{\\eta_t}\\right)^2(1-\\cos^2\\theta_i)} \\\\  &amp;  \\begin{aligned} 1-\\left(\\frac{\\eta_i}{\\eta_t}\\right)^2(1-\\cos^2\\theta_i)&lt;0 \\end{aligned} \\end{aligned} \\] <p>Total internal reflection:</p> <ul> <li>When light is moving from a more optically dense medium to a less optically dense medium, i.e. \\(\\frac{\\eta_i}{\\eta_t}&gt;1\\), Light incident on boundary from large enough angle will not exit medium.</li> </ul> <p>Snell's Window / Circle: </p> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#fresnel-reflection-term","title":"Fresnel Reflection / Term","text":"<p>Reflectance depends on incident angle (and polarization of light)</p> <p></p> <p>Fresnel Term (Dielectric, \\(\\eta=1.5\\)): </p> <p></p> <p>Fresnel Term (Conductor)</p> <p></p> <p>Formulae: </p> <ul> <li>Accurate: need to consider polarization</li> </ul> \\[ \\begin{aligned} R_{s} &amp; =\\left|\\frac{n_1\\cos\\theta_1-n_2\\cos\\theta_t}{n_1\\cos\\theta_1+n_2\\cos\\theta_t}\\right|^2=\\left|\\frac{n_1\\cos\\theta_1-n_2\\sqrt{1-\\left(\\frac{n_1}{n_2}\\sin\\theta_i\\right)^2}}{n_1\\cos\\theta_i+n_2\\sqrt{1-\\left(\\frac{n_1}{n_2}\\sin\\theta_i\\right)^2}}\\right|^2, \\\\ R_{p} &amp; =\\left|\\frac{n_1\\cos\\theta_i-n_2\\cos\\theta_i}{n_1\\cos\\theta_i+n_2\\cos\\theta_i}\\right|^2=\\left|\\frac{n_1\\sqrt{1-\\left(\\frac{n_1}{n_2}\\sin\\theta_i\\right)^2}-n_2\\cos\\theta_i}{n_1\\sqrt{1-\\left(\\frac{n_1}{n_2}\\sin\\theta_i\\right)^2}+n_2\\cos\\theta_i}\\right|^2, \\\\ R_{\\mathrm{eff}}&amp;=\\frac{1}{2}\\left(R_{\\mathrm{s}}+R_{\\mathrm{p}}\\right) \\end{aligned} \\] <ul> <li>Approximate: Schlick's approximation</li> </ul> \\[ \\begin{aligned} R(\\theta) &amp; =R_0+(1-R_0)(1-\\cos\\theta)^5 \\\\ R_{0} &amp; =\\left(\\frac{n_1-n_2}{n_1+n_2}\\right)^2 \\end{aligned} \\]"},{"location":"Learning/GAMES101/Miscellaneous/#microfacet-material","title":"Microfacet Material","text":"<p>Motivation: When looking from a distance, we can't see the fine details. We can only see the overall appearance.</p> <p></p> <p>Theory: </p> <ul> <li> <p>Rough surface</p> <ul> <li> <p>Macroscale: flat &amp; rough</p> </li> <li> <p>Microscale: bumpy &amp; specular</p> </li> </ul> </li> <li> <p>Individual elements of surface act like mirrors</p> <ul> <li> <p>Known as Microfacets</p> </li> <li> <p>Each microfacet has its own normal</p> </li> </ul> </li> </ul> <p></p> <p>We can describe this property using the distribution of microfacets' normals</p> <p></p> <p>Microfacet BRDF: </p> \\[ f(\\mathbf{i},\\mathbf{o})=\\frac{\\mathbf{F}(\\mathbf{i},\\mathbf{h})\\mathbf{G}(\\mathbf{i},\\mathbf{o},\\mathbf{h})\\mathbf{D}(\\mathbf{h})}{4(\\mathbf{n},\\mathbf{i})(\\mathbf{n},\\mathbf{o})} \\] <p>where: </p> <ul> <li> <p>\\(\\mathbf{F}(\\mathbf{i},\\mathbf{h})\\) is the Fresnel term, describing how much light is reflected</p> </li> <li> <p>\\(\\mathbf{G}(\\mathbf{i},\\mathbf{o},\\mathbf{h})\\) is the shadowing masking term, describing the self occlusion between microfacets</p> <ul> <li>This term may have a significant effect when the ray direction is nearly parallel to the surface. We call this kind of angle the grazing angle.</li> </ul> </li> <li> <p>\\(\\mathbf{D}(\\mathbf{h})\\) is the distribution of normals</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#isotropic-anisotropic-materials-brdfs","title":"Isotropic / Anisotropic Materials (BRDFs)","text":"<p>Key: directionality of underlying surface</p> <p></p> <p>Anisotropic BRDFs: </p> <ul> <li> <p>Reflection depends on absolute azimuthal angle \\(\\phi\\). </p> </li> <li> <p>The BRDF may change even when the relative relationship between azimuthal angles \\(\\phi_i\\) and \\(\\phi_r\\) is unchanged</p> <ul> <li>i.e. \\(f_r(\\theta_i,\\phi_i;\\theta_r,\\phi_r)\\neq f_r(\\theta_i,\\theta_r,\\phi_r-\\phi_i)\\)</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#properties-of-brdfs","title":"Properties of BRDFs","text":"<ul> <li> <p>Non-negativity: \\(f_r(\\omega_i\\rightarrow\\omega_r)\\geq 0\\)</p> </li> <li> <p>Linearity: \\(L_r(\\mathrm{p},\\omega_r)=\\int_{H^2}f_r(\\mathrm{p},\\omega_i\\to\\omega_r)L_i(\\mathrm{p},\\omega_i)\\cos\\theta_i\\mathrm{d}\\omega_i\\)</p> </li> </ul> <p></p> <ul> <li> <p>Reciprocity: \\(f_r(\\omega_r\\rightarrow\\omega_i)=f_r(\\omega_i\\rightarrow\\omega_r)\\)</p> </li> <li> <p>Energy conservation: \\(\\forall\\omega_r\\int_{H^2}f_r(\\omega_i\\to\\omega_r)\\cos\\theta_i\\mathrm{d}\\omega_i\\leq1\\)</p> </li> <li> <p>Isotropic vs. anisotropic</p> <ul> <li> <p>If isotropic, \\(f_r(\\theta_i,\\phi_i;\\theta_r,\\phi_r)= f_r(\\theta_i,\\theta_r,\\phi_r-\\phi_i)\\)</p> </li> <li> <p>Then, from reciprocity, \\(f_r(\\theta_i,\\theta_r,\\phi_r-\\phi_i)= f_r(\\theta_r,\\theta_i,\\phi_i-\\phi_r)=f_r(\\theta_i,\\theta_r,|\\phi_r-\\phi_i|)\\)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#measuring-brdfs","title":"Measuring BRDFs","text":"<p>Motivation: </p> <ul> <li> <p>Avoid need to develop / derive models</p> <ul> <li>Automatically includes all of the scattering effects present</li> </ul> </li> <li> <p>Can accurately render with real-world materials</p> <ul> <li>Useful for product design, special effects, ...</li> </ul> </li> <li> <p>Theory vs. practice: </p> </li> </ul> <p></p> <p>Image-Based BRDF Measurement: </p> <p></p> <p>General approach: </p> <pre><code>for each outgoing direction wo\n    move light to illuminate surface with a thin beam from wo\n    for each incoming direction wi\n        move sensor to be at direction wi from surface\n        measure incident radiance\n</code></pre> <p>Improving efficiency:</p> <ul> <li> <p>Isotropic surfaces reduce dimensionality from 4D to 3D</p> </li> <li> <p>Reciprocity reduces # of measurements by half</p> </li> <li> <p>Clever optical systems ...</p> </li> </ul> <p>Challenges in Measuring BRDFs:</p> <ul> <li> <p>Accurate measurements at grazing angles</p> <ul> <li>Important due to Fresnel effects</li> </ul> </li> <li> <p>Measuring with dense enough sampling to capture high frequency specularities</p> </li> <li> <p>Retro-reflection</p> </li> <li> <p>Spatially-varying reflectance, ...</p> </li> </ul> <p>Representing Measured BRDFs, Desirable qualities:</p> <ul> <li> <p>Compact representation</p> </li> <li> <p>Accurate representation of measured data</p> </li> <li> <p>Efficient evaluation for arbitrary pairs of directions</p> </li> <li> <p>Good distributions available for importance sampling</p> </li> </ul> <p>Tabular Representation</p> <ul> <li> <p>Store regularly-spaced samples in \\((\\theta_i,\\theta_r,|\\phi_r-\\phi_i|)\\)</p> <ul> <li>Better: reparameterize angles to better match specularities</li> </ul> </li> <li> <p>Generally need to resample measured values to table</p> </li> <li> <p>Very high storage requirements</p> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#advanced-topics-in-rendering","title":"Advanced Topics in Rendering","text":""},{"location":"Learning/GAMES101/Miscellaneous/#advanced-light-transport","title":"Advanced Light Transport","text":"<ul> <li> <p>Unbiased light transport methods</p> <ul> <li> <p>Bidirectional path tracing (BDPT)</p> </li> <li> <p>Metropolis light transport (MLT)</p> </li> </ul> </li> <li> <p>Biased light transport methods</p> <ul> <li> <p>Photon mapping</p> </li> <li> <p>Vertex connection and merging (VCM)</p> </li> </ul> </li> <li> <p>Instant radiosity (VPL / many light methods)</p> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#biased-vs-unbiased-monte-carlo-estimators","title":"Biased vs. Unbiased Monte Carlo Estimators","text":"<ul> <li> <p>An unbiased Monte Carlo technique does not have any systematic error</p> <ul> <li>The expected value of an unbiased estimator will always be the correct value, no matter how many samples are used</li> </ul> </li> <li> <p>Otherwise, biased</p> <ul> <li>One special case, the expected value converges to the correct value as infinite #samples are used - consistent</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#bidirectional-path-tracing-bdpt","title":"Bidirectional Path Tracing (BDPT)","text":"<ul> <li> <p>Recall: a path connects the camera and the light</p> </li> <li> <p>BDPT:</p> <ul> <li> <p>Traces sub-paths from both the camera and the light</p> </li> <li> <p>Connects the end points from both sub-paths</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Suitable if the light transport is complex on the light's side. For example, indirect light dominates as the following figure</p> </li> <li> <p>Difficult to implement &amp; quite slow</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#metropolis-light-transport-mlt","title":"Metropolis Light Transport (MLT)","text":"<ul> <li> <p>A Markov Chain Monte Carlo (MCMC) application</p> <ul> <li>Jumping from the current sample to the next with some PDF</li> </ul> </li> <li> <p>Very good at locally exploring difficult light paths</p> </li> <li> <p>Key idea</p> <ul> <li>Locally perturb an existing path to get a new path</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Pros</p> <ul> <li> <p>Works great with difficult light paths </p> </li> <li> <p>Also unbiased</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Cons </p> <ul> <li> <p>Difficult to estimate the convergence rate </p> </li> <li> <p>Does not guarantee equal convergence rate per pixel </p> </li> <li> <p>So, usually produces \u201cdirty\u201d results </p> </li> <li> <p>Therefore, usually not used to render animations</p> </li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#photon-mapping","title":"Photon Mapping","text":"<ul> <li> <p>A biased approach &amp; A two-stage method</p> </li> <li> <p>Very good at handling Specular-Diffuse-Specular (SDS) paths and generating caustics (as following figures)</p> </li> </ul> <p></p> <p>Approach (variations apply)</p> <ul> <li> <p>Stage 1 \u2014 photon tracing - Emitting photons from the light source, bouncing them around. Stop and record photons when hitting diffuse surfaces</p> </li> <li> <p>Stage 2 \u2014 photon collection (final gathering) - Shoot sub-paths from the camera, bouncing them around, until they hit diffuse surfaces</p> </li> </ul> <p></p> <p>Calculation \u2014 local density estimation </p> <ul> <li> <p>Idea: areas with more photons should be brighter </p> </li> <li> <p>For each shading point, find the nearest N photons. Take the surface area they over</p> </li> </ul> <p></p> <p>Why the method is biased?</p> <ul> <li> <p>Local density estimation \\(dN/dA\\neq \\Delta N/\\Delta A\\)</p> </li> <li> <p>But in sense of limit, with more photons emitted, the same \\(N\\) photons will cover a smaller \\(\\Delta A\\). \\(\\Delta A\\rightarrow dA\\)</p> </li> <li> <p>So, biased but consistent</p> </li> </ul> <p></p> <p>An easier understanding bias in rendering </p> <ul> <li> <p>Biased == blurry </p> </li> <li> <p>Consistent == not blurry with infinite #samples </p> </li> </ul> <p>Why not do a \u201cconst range\u201d search for density estimation?</p> <ul> <li>Always biased and not consistent since \\(\\Delta A\\) will not converge to \\(dA\\)</li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#vertex-connection-and-merging","title":"Vertex Connection and Merging","text":"<ul> <li> <p>A combination of BDPT and Photon Mapping </p> </li> <li> <p>Key idea </p> </li> <li> <p>Let's not waste the sub-paths in BDPT if their end points cannot be connected but can be merged </p> </li> <li> <p>Use photon mapping to handle the merging of nearby \"photons\"</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#instant-radiosity-ir","title":"Instant Radiosity (IR)","text":"<ul> <li> <p>Sometimes also called many-light approaches</p> </li> <li> <p>Key idea</p> <ul> <li>Lit surfaces can be treated as light sources</li> </ul> </li> <li> <p>Approach</p> <ul> <li> <p>Shoot light sub-paths and assume the end point of each sub-path is a Virtual Point Light (VPL)</p> </li> <li> <p>Render the scene as usual using these VPLs</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Pros: fast and usually gives good results on diffuse scenes</p> </li> <li> <p>Cons</p> <ul> <li> <p>Spikes will emerge when VPLs are close to shading points</p> </li> <li> <p>Cannot handle glossy materials</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#non-surface-models","title":"Non-surface models","text":"<ul> <li> <p>Non-surface models</p> <ul> <li> <p>Participating media</p> </li> <li> <p>Hair / fur / fiber (BCSDF)</p> </li> <li> <p>Granular material</p> </li> </ul> </li> <li> <p>Surface models</p> <ul> <li> <p>Translucent material (BSSRDF)</p> </li> <li> <p>Cloth</p> </li> <li> <p>Detailed material (non-statistical BRDF)</p> </li> </ul> </li> <li> <p>Procedural appearance</p> </li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#participating-media","title":"Participating Media","text":"<p>Participating Media includes fog, cloud, etc </p> <ul> <li>At any point as light travels through a participating medium, it can be (partially) absorbed and scattered.</li> </ul> <p></p> <ul> <li>We use Phase Function to describe the angular distribution of light scattering at any point x within participating media.</li> </ul> <p></p> <p>Rendering: </p> <ul> <li> <p>Randomly choose a direction to bounce </p> </li> <li> <p>Randomly choose a distance to go straight </p> </li> <li> <p>At each 'shading point', connect to the light</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#hair-appearance","title":"Hair Appearance","text":""},{"location":"Learning/GAMES101/Miscellaneous/#kajiya-kay-model","title":"Kajiya-Kay Model","text":"<ul> <li>When the light hitting the hair, it will scatter around the intersection as well as forward into a conical region.</li> </ul>"},{"location":"Learning/GAMES101/Miscellaneous/#marschner-model","title":"Marschner Model","text":"<ul> <li> <p>This model represents the hair as a glass-like cylinder.</p> </li> <li> <p>It defines 3 types of light interactions: \\(R, TT, TRT\\) </p> <ul> <li>\\(R\\): reflection, \\(T\\): transmission</li> </ul> </li> </ul> <p></p> <p></p> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#fur-appearance","title":"Fur Appearance","text":"<p>Rendering as human hair cannot represent diffusive and saturated appearance</p> <p></p> <p>Human Hair vs. Animal Fur</p> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#double-cylinder-model","title":"Double Cylinder Model","text":"<p>Animal Fur has larger medulla, which scatters light a lot</p> <p></p> <ul> <li>The model defines 5 types of light interactions: \\(R, TT, TRT, TT^S, TRT^S\\)</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#surface-model","title":"Surface Model","text":""},{"location":"Learning/GAMES101/Miscellaneous/#subsurface-scattering","title":"Subsurface Scattering","text":"<ul> <li> <p>The scatter happens under surface</p> </li> <li> <p>Visual characteristics of many surfaces caused by light exiting at different points than it enters</p> </li> <li> <p>Violates a fundamental assumption of the BRDF</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#scattering-functions","title":"Scattering Functions","text":"<ul> <li> <p>BSSRDF: </p> <ul> <li> <p>generalization of BRDF</p> </li> <li> <p>exitant radiance at one point due to incident differential irradiance at another point (For BRDF, the light enters and exits at the same point)</p> </li> </ul> </li> </ul> \\[ S(x_i,\\omega_i,x_o,\\omega_o) \\] <p>Generalization of rendering equation: integrating over all points on the surface and all directions</p> \\[ L(x_o,\\omega_o)=\\int_A\\int_{H^2}S(x_i,\\omega_i,x_o,\\omega_o)L_i(x_i,\\omega_i)\\cos\\theta_i\\mathrm{d}\\omega_i\\mathrm{d}A \\] <p></p> <p></p>"},{"location":"Learning/GAMES101/Miscellaneous/#detailed-material","title":"Detailed Material","text":"<p>Problem: Hard to sample a path from light to camera</p> <p></p>"},{"location":"Learning/GAMES101/Rasterization/","title":"Rasterization","text":""},{"location":"Learning/GAMES101/Rasterization/#triangles-fundamental-shape-primitives","title":"Triangles - Fundamental Shape Primitives","text":"<p>Why triangles?</p> <ul> <li> <p>Most basic polygon</p> </li> <li> <p>Break up other polygons</p> </li> </ul> <p>Unique properties</p> <ul> <li> <p>Guaranteed to be planar</p> </li> <li> <p>Well-defined interior</p> </li> <li> <p>Well-defined method for interpolating values at vertices over triangle (barycentric interpolation)</p> </li> </ul> <p>Input: position of triangle vertices projected on screen</p> <p>Output: set of pixel values approximating triangle</p>"},{"location":"Learning/GAMES101/Rasterization/#sampling","title":"Sampling","text":"<p>Evaluating a function at a point is sampling. We can discretize a function by sampling.</p> <p>Define Binary Function: \\(inside(tri,x,y)\\). The value is 1 if point \\((x,y)\\) is inside the triangle, otherwise 0.</p> <p>How to know whether inside?</p> <ul> <li> <p>Three Cross Products</p> </li> <li> <p>If \\(P_{0}P_{1}\\times P_{0}Q &gt; 0\\), then \\(Q\\) is on the left of \\(P_{0}P_{1}\\)</p> </li> <li>When the three cross products have same sign, then \\(Q\\) is inside</li> </ul> <p></p> <p>Do we need check all pixels on the screen?</p> <ul> <li> <p>Using a Bounding Box, confine the triangle in a square and only ckeck the pixels inside it.</p> </li> <li> <p>For each line, find the leftmost and rightmost pixel containing the triangle.</p> </li> </ul>"},{"location":"Learning/GAMES101/Rasterization/#antialiasing","title":"Antialiasing","text":"<p>Artifacts due to sampling - \"Aliasing\"</p> <ul> <li>Jaggies - sampling in space</li> <li>Moire - undersampling images</li> <li>Wagon wheel effect - sampling in time</li> <li>[Many more] ...</li> </ul> <p>Behind the Aliasing Artifacts</p> <ul> <li>Signals are changing too fast (high frequency), but sampled too slowly</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#frequency-domain","title":"Frequency Domain","text":"<p>High-frequency signal is insufficiently sampled: samples erroneously appear to be from a low-frequency signal</p> <p>Two frequencies that are indistinguishable at a given sampling are called \"aliases\"</p>"},{"location":"Learning/GAMES101/Rasterization/#filtering-getting-rid-of-certain-frequency-contents","title":"Filtering = Getting rid of certain frequency contents","text":""},{"location":"Learning/GAMES101/Rasterization/#filtering-convolution-averaging","title":"Filtering = Convolution (= Averaging)","text":"<p>Convolution in the spatial domain is equal to multiplication in the frequency domain, and vice versa</p> <p>Option 1:</p> <ul> <li>Filter by convolution in the spatial domain</li> </ul> <p>Option 2:</p> <ul> <li> <p>Transform to frequency domain (Fourier transform)</p> </li> <li> <p>Multiply by Fourier transform of convolution kernel</p> </li> <li> <p>Transform back to spatial domain (inverse Fourier)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#sampling-repeating-frequency-contents","title":"Sampling = Repeating Frequency Contents","text":"<p>More sparse the sampling is, more dense the repeating in frequency is, which may cause overlap.</p> <p></p> <p>How Can We Reduce Aliasing Error?</p> <p>Option 1: Increase sampling rate</p> <ul> <li> <p>Essentially increasing the distance between replicas in the Fourier domain</p> </li> <li> <p>Higher resolution displays, sensors, framebuffers ...</p> </li> <li> <p>But: costly &amp; may need very high resolution</p> </li> </ul> <p>Option 2: Antialiasing</p> <ul> <li> <p>Making Fourier contents \"narrower\" before repeating</p> </li> <li> <p>i.e. Filtering out high frequencies before sampling</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#antiallased-sampling","title":"Antiallased Sampling","text":"<p>Antialiasing By Averaging Values in Pixel Area</p> <ul> <li>Convolve f(x,y) by a 1-pixel box-blur<ul> <li>Recall: convolving = filtering = averaging</li> </ul> </li> <li>Then sample at every pixel's center</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#antialiasing-by-supersampling-msaa","title":"Antialiasing By Supersampling (MSAA)","text":"<p>Supersampling: Approximate the effect of the 1-pixel box filter by sampling multiple locations within a pixel and averaging their values:</p> <ul> <li> <p>Step 1: Take \\(N\\times N\\) samples in each pixel.</p> </li> <li> <p>Step 2: Average the NxN samples \"inside\" each pixel.</p> </li> </ul> <p></p> <p>MSAA reduce aliasing by approximate the blur operation but not by increasing resolution.</p>"},{"location":"Learning/GAMES101/Rasterization/#visibility-occlusion","title":"Visibility &amp; Occlusion","text":""},{"location":"Learning/GAMES101/Rasterization/#painters-algorithm","title":"Painter's Algorithm","text":"<p>Sort every primitives in depth(\\(O(n\\log{n})\\) for \\(n\\) triangles) and paint from back to front, overwrite in the framebuffer.</p> <p>But the depth order is unresolvable.</p> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#z-buffer","title":"Z-Buffer","text":"<p>Idea: </p> <ul> <li> <p>Store current \\(\\min({value_z})\\) for each sample(pixel)</p> </li> <li> <p>Needs an additional buffer for depth values</p> <ul> <li>framebuffer stores color values</li> <li>depth buffer(z-buffer) stores depth</li> </ul> </li> </ul> <p>Note</p> <p>For simplicity, we suppose \\(z\\) is always positive(smaller \\(z\\) for closer sample)</p> <p></p> Z-Buffer Algorithm<pre><code>Initialize depth buffer to infinity\n\nDuring rsterization:\n\n    for (each triangle T):\n        for (each sample (x, y, z) in T):\n            if (z &lt; zbuffer[x, y]):     // the sample is closer\n                framebuffer[x, y] = rgb // update color\n                zbuffer[x, y] = z       // update depth\n            else:\n                continue                // do nothing, the sample is occluded\n</code></pre> <ul> <li> <p>Complexity is \\(O(n)\\) for \\(n\\) triangles (assuming constant coverage)</p> </li> <li> <p>Drawing triangles in different orders brings the same result</p> </li> <li> <p>The most important visibility algorithm, Implemented in hardware for all GPUs</p> </li> </ul>"},{"location":"Learning/GAMES101/Rasterization/#shadow-mapping","title":"Shadow Mapping","text":"<p>How to draw shadows using rasterization?  Shadow Mapping</p> <ul> <li> <p>An Image-space Algorithm</p> <ul> <li>no knowledge of scene's geometry during shadow computation</li> <li>must deal with aliasing artifacts</li> <li>applicable to point light sources</li> </ul> </li> <li> <p>Key idea:</p> <ul> <li>the points NOT in shadow must be seen both by the light and by the camera</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Rasterization/#steps","title":"Steps","text":"<ul> <li> <p>Pass 1: Render the scene from the light's perspective to obtain the depth image.</p> </li> <li> <p>Pass 2A: Render the standard image with depth from eye</p> </li> <li> <p>Pass 2B: Reproject visible points in the eye view back to the light source. If the depth matches the light's depth map, the point can be seen by both the eye and the light.</p> </li> </ul> <p></p> <ul> <li>Compare with and without shadows</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Rasterization/#problems","title":"Problems","text":"<ul> <li> <p>Hard shadows (point lights only)</p> <ul> <li> <p>hard shadow: created by point light sources and have sharp, well-defined edges, since the area is either visible or invisible.</p> </li> <li> <p>soft shadow: produced by area light sources and have blurred, gradual edges due to the varying degrees of occlusion.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Quality depends on shadow map resolution (general problem with image-based techniques)</p> </li> <li> <p>Involves equality comparison of floating-point depth values, which introduces issues of scale, bias, and tolerance.</p> <ul> <li>Noisy near the boundary</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/","title":"Ray Tracing","text":"<p>Why Ray Tracing?</p> <ul> <li> <p>Rasterization couldn't handle global effects well</p> <ul> <li> <p>(Soft) shadows</p> </li> <li> <p>And especially when the light bounces more than once</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Rasterization is fast, but quality is relatively low</p> </li> <li> <p>Ray tracing is accurate, but is very slow</p> <ul> <li> <p>Rasterization: real-time, ray tracing: offline</p> </li> <li> <p>~10K CPU core hours to render one frame in production</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#basic-ray-tracing-algorithm","title":"Basic Ray-Tracing Algorithm","text":""},{"location":"Learning/GAMES101/RayTracing/#light-rays","title":"Light Rays","text":"<p>Three ideas about light rays</p> <ol> <li> <p>Light travels in straight lines (though this is wrong)</p> </li> <li> <p>Light rays do not \"collide\" with each other if they cross (though this is still wrong)</p> </li> <li> <p>Light rays travel from the light sources to the eye (but the physics is invariant under path reversal - reciprocity).</p> </li> </ol>"},{"location":"Learning/GAMES101/RayTracing/#ray-casting","title":"Ray Casting","text":"<ol> <li> <p>Generate an image by casting one eye ray per pixel.</p> </li> <li> <p>Find the closest scene intersection point</p> </li> <li> <p>Check for shadows by sending a ray to the light</p> </li> <li> <p>Perform shading calculation to compute color of pixel</p> </li> </ol> <p></p> <ul> <li>Local only, without reflection and refraction</li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#recursive-whitted-style-ray-tracing","title":"Recursive (Whitted-Style) Ray Tracing","text":"<p>An improved illumination model for shaded display</p> <p></p> <ul> <li> <p>When a ray hits a glass-like material, both reflection and refraction occur.</p> </li> <li> <p>When the primary ray hits the surface, continuely computing the intersection point between refracted/reflected rays and surface</p> </li> <li> <p>For each intersection point, shoot a ray to light source to check shadow</p> </li> <li> <p>Compute shading result on each intersection point</p> </li> <li> <p>Sum these results to get the color</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#ray-surface-intersection","title":"Ray-Surface Intersection","text":""},{"location":"Learning/GAMES101/RayTracing/#ray-equation","title":"Ray Equation","text":"<p>Ray is defined by its origin and a direction vector.</p> <p>Ray equation: </p> \\[ \\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\quad0\\leq t\\leq \\infty \\] <ul> <li> <p>\\(\\mathbf{r}\\): point along ray</p> </li> <li> <p>\\(t\\): \"time\"</p> </li> <li> <p>\\(\\mathbf{o}\\): origin</p> </li> <li> <p>\\(\\mathbf{d}\\): normalized direction</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#ray-intersection-with-implicit-surface","title":"Ray Intersection With Implicit Surface","text":"<p>Ray: \\(\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\quad0\\leq t\\leq \\infty\\)</p> <p>General implicit surface: \\(\\mathbf{p}:f(\\mathbf{p})=0\\)</p> <p>Substitute ray equation: \\(f(\\mathbf{o}+t\\mathbf{d})=0\\)</p> <ul> <li>Solve for real, positive roots</li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#ray-intersection-with-triangle-mesh","title":"Ray Intersection With Triangle Mesh","text":"<p>Why?</p> <ul> <li> <p>Rendering: visibility, shadows, lighting ...</p> </li> <li> <p>Geometry: inside/outside test</p> <ul> <li>Shoot a ray from the point, if #intersection between the ray and surface is odd, then the point is inside the surface, outside otherwise.</li> </ul> </li> </ul> <p>How to compute?</p> <p>Let's break this down:</p> <ul> <li> <p>Simple idea: just intersect ray with each triangle</p> </li> <li> <p>Simple, but slow </p> </li> <li> <p>Note: can have 0, 1 intersections (ignoring multiple intersections)</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#ray-intersection-with-triangle","title":"Ray Intersection With Triangle","text":"<p>Triangle is in a plane</p> <ul> <li> <p>First find ray-plane intersection</p> </li> <li> <p>Then test if hit point is inside triangle</p> </li> </ul> <p>Plane is defined by normal vector and a point on plane</p> <p>Plane Equation (if p satisfies it, then p is on the plane):</p> \\[ \\mathbf{p}:(\\mathbf{p}-\\mathbf{p}^{\\prime})\\cdot\\mathbf{N}=0 \\] <p>Solve for intersection: Set \\(\\mathbf{p}=\\mathbf{r}(t)\\) and solve for \\(t\\)</p> \\[ (\\mathbf{p}-\\mathbf{p}^{\\prime})\\cdot\\mathbf{N}=(\\mathbf{o}+t\\mathbf{d}-\\mathbf{p}^{\\prime})\\cdot\\mathbf{N}=0 \\\\ t=\\frac{(\\mathbf{p}^{\\prime}-\\mathbf{o})\\cdot\\mathbf{N}}{\\mathbf{d}\\cdot\\mathbf{N}} \\] <ul> <li>Check: \\(0\\leq t\\leq \\infty\\)</li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#moller-trumbore-algorithm","title":"M\u00f6ller Trumbore Algorithm","text":"<p>A faster approach, giving barycentric coordinate directly</p> \\[ \\vec{\\mathbf{O}}+t\\vec{\\mathbf{D}}=(1-b_1-b_2)\\vec{\\mathbf{P}}_0+b_1\\vec{\\mathbf{P}}_1+b_2\\vec{\\mathbf{P}}_2\\\\ \\begin{bmatrix} t \\\\ b_1 \\\\ b_2 \\end{bmatrix}=\\frac{1}{\\vec{\\mathbf{S}}_1\\bullet\\vec{\\mathbf{E}}_1} \\begin{bmatrix} \\vec{\\mathbf{S}}_2\\bullet\\vec{\\mathbf{E}}_2 \\\\ \\vec{\\mathbf{S}}_1\\bullet\\vec{\\mathbf{S}} \\\\ \\vec{\\mathbf{S}}_2\\bullet\\vec{\\mathbf{D}} \\end{bmatrix} \\] <p>Where: </p> \\[ \\begin{aligned}  &amp; \\mathbf{\\vec{E}}_1=\\mathbf{\\vec{P}}_1-\\mathbf{\\vec{P}}_0 \\\\  &amp; \\mathbf{\\vec{E}}_{2}=\\mathbf{\\vec{P}}_{2}-\\mathbf{\\vec{P}}_{0} \\\\  &amp; \\vec{\\mathbf{S}}=\\vec{\\mathbf{O}}-\\vec{\\mathbf{P}}_{0} \\\\  &amp; \\vec{\\mathbf{S}}_1=\\vec{\\mathbf{D}}\\times\\vec{\\mathbf{E}}_2 \\\\  &amp; \\vec{\\mathbf{S}}_{2}=\\vec{\\mathbf{S}}\\times\\vec{\\mathbf{E}}_{1} \\end{aligned} \\] <ul> <li> <p>\\(Cost = (1 div, 27 mul, 17 add)\\)</p> </li> <li> <p>The intersection is inside the triangle if \\(b_1&gt;0,b_2&gt;0,b_1+b_2&lt;1\\)</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#accelerating-ray-surface-intersection","title":"Accelerating Ray-Surface Intersection","text":""},{"location":"Learning/GAMES101/RayTracing/#performance-challenges","title":"Performance Challenges","text":"<p>Simple ray-scene intersection</p> <ul> <li>Exhaustively test ray-intersection with every triangle</li> <li>Find the closest hit (i.e. minimum t)</li> </ul> <p>Problem:</p> <ul> <li>Naive algorithm = \\(\\#pixels \\times \\# traingles \\times \\#bounces\\)</li> <li>Very slow!</li> </ul> <p>For generality, we use the term objects instead of triangles later (but doesn't necessarily mean entire objects)</p>"},{"location":"Learning/GAMES101/RayTracing/#bounding-volumns","title":"Bounding Volumns","text":"<p>Quick way to avoid intersections: bound complex object with a simple volume</p> <ul> <li> <p>Object is fully contained in the volume</p> </li> <li> <p>If it doesn't hit the volume, it doesn't hit the object</p> </li> <li> <p>So test BVol first, then test object if there is a hit</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#ray-intersection-with-box","title":"Ray-Intersection With Box","text":"<p>Box is the intersection of 3 pairs of slabs</p> <ul> <li> <p>Specifically, we often use an Axis-Aligned Bounding Box (AABB)</p> <ul> <li>i.e. any side of the BB is along either x, y, or z axis</li> </ul> </li> </ul> <p></p> <p>For 2D cases, </p> <ul> <li> <p>We first compute the intersections with each pair of slabs</p> <ul> <li>This tells us the time \\(t\\) when the ray is inside that pair: \\(t_{min}\\leq t\\leq t_{max}\\)</li> </ul> </li> <li> <p>When the ray intersects the box? </p> <ul> <li>Take the intersection of \\(t\\) for each pair</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Key ideas: </p> <ul> <li> <p>The ray enters the box only when it enters all pairs of slabs</p> </li> <li> <p>The ray exits the box as long as it exits any pair of slabs</p> </li> </ul> </li> <li> <p>For each pair, calculate the \\(t_{min}\\) and \\(t_{max}\\) (negative is fine)</p> </li> <li> <p>For 3D box, \\(t_{enter}=\\max\\{t_{min}\\},t_{exit}=\\min\\{t_{max}\\}\\)</p> </li> <li> <p>The ray and AABB intersect iff \\(t_{enter}&lt;t_{exit}\\ and\\ t_{exit}\\geq 0\\)</p> <ul> <li> <p>If \\(t_{exit}&lt;0\\): The box is \"behind\" the ray, no intersection</p> </li> <li> <p>If \\(t_{exit}\\geq 0\\ and\\ t_{enter}&lt; 0\\): The ray's origin is inside the box, have intersection.</p> </li> </ul> </li> </ul> <p>Why Axis-Aligned?</p> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#uniform-spatial-partitions-grids","title":"Uniform Spatial Partitions (Grids)","text":"<p>Preprocess: Build Acceleration Grid</p> <ol> <li> <p>Find bounding box</p> </li> <li> <p>Create grid</p> </li> <li> <p>Store each object in overlapping cells</p> </li> </ol> <p></p> <p>How to find Ray-Scene Intersection?</p> <ul> <li> <p>Step through grid in ray traversal order</p> </li> <li> <p>For each grid cell: Test intersection with all objects stored at that cell</p> </li> </ul> <p></p> <p>How to select grid resolution? </p> <ul> <li> <p>One cell: </p> <ul> <li>No speedup</li> </ul> </li> <li> <p>Too many cells: </p> <ul> <li>Inefficiency due to extraneous grid traversal</li> </ul> </li> <li> <p>Heuristic:</p> <ul> <li> <p>\\(\\#cells = C \\times \\#objs\\)</p> </li> <li> <p>\\(C \\approx 27\\) in 3D</p> </li> </ul> </li> </ul> <p>When uniform grids work well? </p> <ul> <li>Grids work well on large collections of objects that are distributed evenly in size and space</li> </ul> <p>When They fail?</p> <ul> <li>\"Teapot in a stadium\" problem: Have to spend lots of time to test grid without objects inside</li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#spatial-partitions","title":"Spatial Partitions","text":""},{"location":"Learning/GAMES101/RayTracing/#data-structures-to-partition-space","title":"Data Structures to Partition Space","text":"<ul> <li> <p>Oct-Tree: Recursively divides a space into 8 (4 in 2D) regions with equivalent volume. Stop when the number of objects in the space is 0 or below a threshold.</p> <ul> <li>The branching factor of an Oct-Tree grows exponentially with the number of dimensions.</li> </ul> </li> <li> <p>KD-Tree: Recursively divides a space into two regions by selecting an axis and splitting the data along a chosen coordinate. The axis used for division typically cycles through the dimensions (e.g., x, y, z) at each level of the tree.</p> </li> <li> <p>BSP-Tree: Similar to KD-Tree, but the divisions are not necessarily axis-aligned.</p> <ul> <li>Hard to perform division in high dimension</li> </ul> </li> </ul> <p></p> <ul> <li>you could have these in both 2D and 3D. Here we will illustrate principles in 2D.</li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#kd-tree","title":"KD-Tree","text":"<p>Pre-Processing</p> <ul> <li> <p>Divide the space into several regions following the rule. Then we get a KD-Tree.</p> </li> <li> <p>Note that nodes 1 and 2 should also be subdivided but we don't demonstrate it here for simplicity</p> </li> </ul> <p></p> <p>Data Structure for KD-Trees</p> <ul> <li> <p>Internal nodes store</p> <ul> <li> <p>split axis: x-, y-, or z-axis</p> </li> <li> <p>split position: coordinate of split plane along axis</p> </li> <li> <p>children: pointers to child nodes</p> </li> <li> <p>No objects are stored in internal nodes</p> </li> </ul> </li> <li> <p>Leaf nodes store</p> <ul> <li>list of objects</li> </ul> </li> </ul> <p>Traversing a KD-Tree</p> <ul> <li> <p>First check the split axis and split position of the root node to decide whether the ray intersect with its child.</p> </li> <li> <p>Recursively traverse the subtree that intersects.</p> </li> <li> <p>If the node is leaf node, test the intersection of all objects inside it.</p> </li> </ul> <p></p> <p>Problem: </p> <ul> <li> <p>It's hard to determine the intersection between box and triangle</p> </li> <li> <p>An object can be in many leaf nodes</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#object-partitions-bounding-volume-hierarchy-bvh","title":"Object Partitions &amp; Bounding Volume Hierarchy (BVH)","text":""},{"location":"Learning/GAMES101/RayTracing/#building-bvh","title":"Building BVH","text":"<p>Procedure: </p> <ul> <li> <p>Find the bounding box of the current set objects</p> </li> <li> <p>Recursively split the set of objects into two subsets</p> <ul> <li>Need to minimize the overlap of bounding boxes</li> </ul> </li> <li> <p>Recompute the bounding box of the subsets</p> </li> <li> <p>Stop when necessary</p> </li> <li> <p>Store objects in each leaf node</p> </li> </ul> <p></p> <p>How to subdivide a node?</p> <ul> <li> <p>Choose a dimension to split</p> </li> <li> <p>Heuristic #1: Always choose the longest axis in node</p> </li> <li> <p>Heuristic #2: Split node at location of median object</p> <ul> <li>i.e. make the childs have same number of nodes. The tree can be more balanced</li> </ul> </li> </ul> <p>Termination criteria?</p> <p>. Heuristic: stop when node contains few elements (e.g. 5)</p>"},{"location":"Learning/GAMES101/RayTracing/#data-structure-for-bvh","title":"Data Structure for BVH","text":"<p>Internal nodes store</p> <ul> <li> <p>Bounding box</p> </li> <li> <p>Children: pointers to child nodes </p> </li> </ul> <p>Leaf nodes store</p> <ul> <li> <p>Bounding box</p> </li> <li> <p>List of objects</p> </li> </ul> <p>Nodes represent subset of primitives in scene</p> <ul> <li>All objects in subtree</li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#bvh-traversal","title":"BVH Traversal","text":"<pre><code>Intersect (Ray ray, BVH node) {\n    if (ray misses node.bbox) return;\n\n    if (node is a leaf node)\n        test intersection with all objs;\n        return closest intersection;\n\n    hit1 = Intersect(ray, node.child1);\n    hit2 = Intersect(ray, node.child2);\n\n    return the closer of hit1, hit2;\n\n}\n</code></pre>"},{"location":"Learning/GAMES101/RayTracing/#spatial-vs-object-partitions","title":"Spatial vs Object Partitions","text":"<p>Spatial partition (e.g.KD-tree)</p> <ul> <li> <p>Partition space into non-overlapping regions</p> </li> <li> <p>An object can be contained in multiple regions</p> </li> </ul> <p>Object partition (e.g. BVH)</p> <ul> <li> <p>Partition set of objects into disjoint subsets</p> </li> <li> <p>Bounding boxes for each set may overlap in space</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#radiometry","title":"Radiometry","text":"<p>Why Radiometry?</p> <ul> <li> <p>Blinn-Phong model is just a approximation: </p> <ul> <li> <p>Many assumptions are incorrect.</p> </li> <li> <p>Many quantities do not have a definite physical meaning.</p> </li> </ul> </li> <li> <p>Whitted style ray tracing does not gives the correct result</p> </li> </ul> <p>Radiometry: </p> <ul> <li> <p>Measurement system and units for illumination</p> </li> <li> <p>Accurately measure the spatial properties of light</p> <ul> <li>New terms: Radiant flux, intensity, irradiance, radiance</li> </ul> </li> <li> <p>Perform lighting calculations in a physically correct manner</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#radient-energy-and-flux-power","title":"Radient Energy and Flux (Power)","text":"<p>Radiant Energy: </p> <ul> <li>Definition: Radiant energy is the energy of electromagnetic radiation. It is measured in units of joules, and denoted by the symbol:</li> </ul> \\[ Q\\left[\\text{J}=\\text{Joule}\\right] \\] <p>Radiant flux (power):</p> <ul> <li>Definition: Radiant flux (power) is the energy emitted, reflected, transmitted or received, per unit time.</li> </ul> \\[ \\Phi\\equiv\\frac{\\mathrm{d}Q}{\\mathrm{d}t}\\text{ [W = Watt] [lm = lumen]} \\] <ul> <li>Flux can also be understood as #photons flowing through a sensor in unit time</li> </ul> <p></p> <p>important Light Measurements of Interest</p> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#radiant-intensity","title":"Radiant Intensity","text":"<p>Definition: </p> <ul> <li>The radiant (luminous) intensity is the power per unit solid angle emitted by a point light source.</li> </ul> \\[ I(\\omega)\\equiv\\frac{\\mathrm{d}\\Phi}{\\mathrm{d}\\omega}\\left[\\frac{\\mathrm{W}}{\\mathrm{sr}}\\right]\\left[\\frac{\\mathrm{lm}}{\\mathrm{sr}}=\\mathrm{cd}=\\mathrm{candela}\\right] \\] <p>Angle: ratio of subtended arc length on circle to radius</p> <ul> <li> <p>\\(\\theta = \\frac{l}{r}\\)</p> </li> <li> <p>Circle has \\(2\\pi\\) radians</p> </li> </ul> <p></p> <p>Solid angle: ratio of subtended area on sphere to radius squared</p> <ul> <li> <p>\\(\\Omega=\\frac{A}{r^2}\\)</p> </li> <li> <p>Sphere has \\(4\\pi\\) steradians</p> </li> </ul> <p></p> <p>Differential Solid Angles</p> \\[ \\begin{align*} \\mathrm{d}A&amp;=(r\\mathrm{d}\\theta)(r\\sin{\\theta}\\mathrm{d}\\phi)\\\\ &amp;=r^2\\sin{\\theta}\\ \\mathrm{d}\\theta \\mathrm{d}\\phi \\end{align*} \\] \\[ \\begin{align*} \\mathrm{d}\\omega=\\frac{\\mathrm{d}A}{r^2}=\\sin{\\theta}\\ \\mathrm{d}\\theta\\mathrm{d}\\phi \\end{align*} \\] <p></p> <p>For sphere \\(S^2\\), the solid angle is: </p> \\[ \\begin{align*} \\Omega&amp;=\\int_{S^2}\\mathrm{d}\\omega\\\\ &amp;=\\int_{0}^{2\\pi}\\int_{0}^{\\pi}\\sin{\\theta}\\ \\mathrm{d}\\theta\\mathrm{d}\\phi\\\\ &amp;=4\\pi \\end{align*} \\] <p>We use \\(\\omega\\) to denote a direction vector (unit length)</p> <p></p> <p>For Isotropic Point Source, </p> \\[ \\begin{align*} \\Phi&amp;=\\int_{S^2}I\\mathrm{d}\\omega\\\\ &amp;=4\\pi I \\end{align*} \\] \\[ I=\\frac{\\Phi}{4\\pi} \\] <p></p>"},{"location":"Learning/GAMES101/RayTracing/#irradiance","title":"Irradiance","text":"<p>Definition: </p> <ul> <li>The irradiance is the power per (perpendicular/projected) unit area incident on a surface point.</li> </ul> \\[ E(\\mathbf{x})\\equiv\\frac{\\mathrm{d}\\Phi(\\mathbf{x})}{\\mathrm{d}A}\\left[\\frac{\\mathrm{W}}{\\mathrm{m}^2}\\right]\\left[\\frac{\\mathrm{lm}}{\\mathrm{m}^2}=\\mathrm{lux}\\right] \\] <p>This explains Lambert's cosine law we discussed before: </p> <p></p> <p>This also explains irradiance falloff</p> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#radiance","title":"Radiance","text":"<p>Radiance is the fundamental field quantity that describes the distribution of light in an environment</p> <ul> <li> <p>Radiance is the quantity associated with a ray</p> </li> <li> <p>Rendering is all about computing radiance</p> </li> </ul> <p></p> <p>Definition: </p> <ul> <li>The radiance (luminance) is the power emitted, reflected, transmitted or received by a surface, per unit solid angle, per projected unit area.</li> </ul> \\[ L(\\mathrm{p},\\omega) \\equiv\\frac{\\mathrm{d}^2\\Phi(\\mathrm{p},\\omega)}{\\mathrm{d}\\omega\\mathrm{d}A\\cos\\theta} \\left[\\frac{\\mathrm{W}}{\\mathrm{sr}\\ \\mathrm{m}^2}\\right]\\left[\\frac{\\mathrm{cd}}{\\mathrm{m}^2}=\\frac{\\mathrm{lm}}{\\mathrm{sr}\\ \\mathrm{m}^2}=\\mathrm{nit}\\right] \\] <ul> <li>\\(\\cos{\\theta}\\) accounts for projected surface area</li> </ul> <p></p> <p>Recall</p> <ul> <li> <p>Irradiance: power per projected unit area</p> </li> <li> <p>Intensity: power per solid angle</p> </li> </ul> <p>So</p> <ul> <li> <p>Radiance: Irradiance per solid angle</p> </li> <li> <p>Radiance: Intensity per projected unit area</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#incident-radiance","title":"Incident Radiance","text":"<p>Incident radiance is the irradiance per unit solid angle arriving at the surface.</p> <ul> <li>i.e. it is the light arriving at the surface along a given ray (point on surface and incident direction).</li> </ul> \\[ L(\\mathrm{p},\\omega)=\\frac{\\mathrm{d}E(\\mathrm{p})}{\\mathrm{d}\\omega\\cos\\theta} \\] <p></p>"},{"location":"Learning/GAMES101/RayTracing/#exiting-radiance","title":"Exiting Radiance","text":"<p>Exiting surface radiance is the intensity per unit projected area leaving the surface.</p> <ul> <li>e.g. for an area light it is the light emitted along a given ray (point on surface and exit direction).</li> </ul> \\[ L(\\mathrm{p},\\omega)=\\frac{\\mathrm{d}I(\\mathrm{p},\\omega)}{\\mathrm{d}A\\cos\\theta} \\] <p></p>"},{"location":"Learning/GAMES101/RayTracing/#irradiance-vs-radiance","title":"Irradiance vs. Radiance","text":"<ul> <li> <p>Irradiance: total power received by area \\(\\mathrm{d}A\\)</p> </li> <li> <p>Radiance: power received by area \\(\\mathrm{d}A\\) from \"direction\" \\(\\mathrm{d}\\omega\\)</p> </li> </ul> \\[ \\begin{aligned} dE(\\mathrm{p},\\omega) &amp; =L_i(\\mathrm{p},\\omega)\\cos\\theta\\mathrm{d}\\omega \\\\ E(\\mathrm{p}) &amp; =\\int_{H^2}L_i(\\mathrm{p},\\omega)\\cos\\theta\\mathrm{d}\\omega \\end{aligned} \\] <p></p>"},{"location":"Learning/GAMES101/RayTracing/#bidirectional-reflectance-distribution-function-brdf","title":"Bidirectional Reflectance Distribution Function (BRDF)","text":"<p>Reflection at a Point:</p> <ul> <li> <p>Radiance from direction \\(\\omega_i\\) turns into the power \\(E\\) that \\(\\mathrm{d}A\\) receives</p> </li> <li> <p>Then power \\(E\\) will become the radiance to any other direction \\(\\omega _o\\)</p> </li> <li> <p>Differential irradiance incoming: \\(dE(\\omega_i)=L(\\omega_i)\\cos{\\theta_i}d\\omega_i\\)</p> </li> <li> <p>Differential radiance exiting (due to \\(dE(\\omega_i)\\)): \\(dL_r(\\omega_r)\\)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#brdf","title":"BRDF","text":"<ul> <li>The Bidirectional Reflectance Distribution Function (BRDF) represents how much light is reflected into each outgoing direction \\(\\omega _r\\) from each incoming direction \\(\\omega _i\\)</li> </ul> \\[ f_r(\\omega_i\\rightarrow\\omega_r)=\\frac{dL_r(\\omega_r)}{dE_i(\\omega_i)}=\\frac{dL_r(\\omega_r)}{L_i(\\omega_i)\\cos{\\theta_i}d\\omega_i}\\left[\\frac{1}{\\mathrm{sr}}\\right] \\]"},{"location":"Learning/GAMES101/RayTracing/#reflection-equation","title":"Reflection Equation","text":"<ul> <li> <p>Defines how much light is reflected into the outgoing direction \\(\\omega_r\\)</p> </li> <li> <p>Integrate light from all incoming direction \\(\\omega_i\\) times BRDF</p> </li> </ul> \\[ L_r(p,\\omega_r)=\\int_{H^2}f_r(p,\\omega_i\\rightarrow\\omega_r)L_i(p,\\omega_i)\\cos{\\theta_i}d\\omega_i \\] <p></p> <ul> <li> <p>Challenge: Recursive Equation</p> <ul> <li> <p>The Reflected radiance \\(L_r(p,\\omega_r)\\) depends on incoming radiance \\(L_i(p,\\omega_i)\\)</p> </li> <li> <p>But incoming radiance depends on reflected radiance (at another point in the scene)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#rendering-equation","title":"Rendering Equation","text":"<ul> <li>The surface may emit light spontaneously. Adding an Emission term to reflection equation to make it general</li> </ul> \\[ L_o(p,\\omega_o)=L_e(p,\\omega_o) + \\int_{\\Omega+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot \\omega_i)d\\omega_i \\] <p>Note</p> <p>Here we assume that all directions are pointing outwards</p> <p>Note that \\(L_i\\) is also the reflected light computed from the rendering function. So we can reformulate the equation as following: </p> \\[ l(u)=e(u)+\\int l(v)K(u,v)dv \\] <p>where</p> <ul> <li> <p>\\(l(u/v)\\) denotes the light from direction \\(u/v\\)</p> </li> <li> <p>\\(K(u,v)dv\\) is the kernel of equation (Light Transport Operator)</p> </li> </ul> <p>It can be discretized to a simple matrix equation (or system of simultaneous linear equations): </p> \\[ L=E+KL \\] <p>where \\(L,E\\) are vectors and \\(K\\) is the light transport matrix.</p> \\[ \\begin{align*} L&amp;=E+KL\\\\ IL-KL&amp;=E\\\\ L&amp;=(I-K)^{-1}E\\\\ L&amp;=(I+K+K^2+K^3+\\dots)\\quad(\\text{Binomial Theorem})\\\\ L&amp;=E+KE+K^2E+K^3E+\\dots \\end{align*} \\] <p>Now we can have a deeper understanding that \\(E\\) is the emission directly from light sources, \\(KE\\) is the direct illumination on surfaces, \\(K^2E\\) is indirect illumination (One bounce), \\(\\dots\\)</p> <p></p> <p></p> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#monte-carlo-integration","title":"Monte Carlo Integration","text":"<p>Why? </p> <ul> <li>We want to solve an integral, but it can be too difficult to solve analytically</li> </ul> <p>What &amp; How? </p> <ul> <li>Estimate the integral of a function by averaging random samples of the functoins's value</li> </ul> <p></p> <p>For the definite integral of given function \\(f(x)\\), define: </p> <ul> <li> <p>Definite integral \\(\\int_{a}^{b}f(x)dx\\)</p> </li> <li> <p>Random varirable \\(X_i \\sim p(x)\\)</p> </li> <li> <p>Monte Carlo estimator: \\(F_N =\\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_i)}{p(X_i)}\\)</p> </li> </ul> <p></p> <p>For uniform random variable, </p> \\[ \\begin{align*} &amp;X_i \\sim p(x)=C\\\\ &amp;\\int_{a}^{b}p(x)dx=1\\\\ &amp;\\Rightarrow C=\\frac{1}{b-a} \\end{align*} \\] <p>This brings us the Basic Monte Carlo estimator: </p> \\[ F_N=\\frac{b-a}{N}\\sum_{i=1}^Nf(X_i) \\] <p>In conclusion, we estimate the integral by the following formula: </p> \\[ \\int f(x)dx=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_i)}{p(X_i)}\\quad X_i\\sim p(x) \\] <ul> <li> <p>The more samples, the less variance</p> </li> <li> <p>Sample on \\(x\\), integrate on \\(x\\)</p> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#path-tracing","title":"Path Tracing","text":"<p>Motivation: Whitted-style ray tracing: </p> <ul> <li> <p>Always perform specular reflections / refractions</p> </li> <li> <p>Stop bouncing at diffuse surfaces</p> </li> <li> <p>Problem: </p> <ul> <li> <p>Where should the ray be reflected for glossy materials?</p> </li> <li> <p>No reflections between diffuse materials?</p> </li> </ul> </li> </ul> <p></p> <p>Whitted-Style Ray Tracing is Wrong</p> <ul> <li>The rendering equation is correct</li> </ul> \\[ L_o(p,\\omega_o)=L_e(p,\\omega_o) + \\int_{\\Omega+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot \\omega_i)d\\omega_i \\] <ul> <li> <p>But it involves: </p> <ul> <li> <p>Solving an integral over the hemisphere</p> </li> <li> <p>Recursive execution</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/RayTracing/#a-simple-monte-carlo-solution","title":"A Simple Monte Carlo Solution","text":"<p>Suppose we want to render one pixel (point) in the following scene for direct illumination only.</p> <ul> <li> <p>An area light</p> </li> <li> <p>Assume all directions are pointing outwards</p> </li> </ul> <p></p> <p>We want to compute the radiance at point \\(p\\) towards the camera: </p> \\[ L_o(p,\\omega_o)=\\int_{\\Omega+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot \\omega_i)d\\omega_i \\] <ul> <li> <p>Using Monte Carlo integration: \\(\\int_a^b f(x)dx\\approx\\frac{1}{N}\\sum_{k=1}^{N}\\frac{f(X_k)}{p(X_k)}\\quad X_k\\sim p(x)\\)</p> </li> <li> <p>\\(f(x)=L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot \\omega_i)\\)</p> </li> <li> <p>\\(p(\\omega_i)=\\frac{1}{2\\pi}\\)</p> <ul> <li>Since we assume uniformly sampling the hemisphere and the solid angle for a sphere is \\(4\\pi\\)</li> </ul> </li> </ul> <p>So, in general: </p> \\[ L_o(p,\\omega_o)\\approx \\frac{1}{N}\\sum_{i=1}^{N}\\frac{L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot \\omega_i)}{p(\\omega_i)} \\] <p>It's a correct shading algorithm for direct illumination</p> <pre><code>shade (p, wo): \n    Randomly choose N directions wi~pdf\n    Lo = 0.0\n    For each wi\n        Trace a ray r(p, wi)\n        If ray r hit the light\n            Lo += (1 / N) * L_i * f_r * cosine / pdf(wi)\n    Return Lo\n</code></pre>"},{"location":"Learning/GAMES101/RayTracing/#introducing-global-illumination","title":"Introducing Global Illumination","text":"<p>One more step forward: what if a ray hits an object?</p> <ul> <li>For the following example, \\(Q\\) reflects direct illumination to \\(P\\)</li> </ul> <p></p> <p>We can modify our algorithm, when the ray hits an object at q, recursively compute the shade at q</p> <pre><code>shade (p, wo): \n    Randomly choose N directions wi~pdf\n    Lo = 0.0\n    For each wi:\n        Trace a ray r(p, wi)\n        If ray r hit the light:\n            Lo += (1 / N) * L_i * f_r * cosine / pdf(wi)\n        Else If ray r hit an object at q:\n            Lo += (1 / N) * shade(q, -wi) * f_r * cosine / pdf(wi)\n    Return Lo\n</code></pre>"},{"location":"Learning/GAMES101/RayTracing/#path-tracing_1","title":"Path Tracing","text":"<p>Problem 1: Explosion of \\(\\#rays\\) as \\(\\#bounces\\) go up: \\(\\#rays=N^{\\#bounces}\\)</p> <p></p> <p>Key Observation: </p> <ul> <li>\\(\\#rays\\) will not explode iff \\(N=1\\)</li> </ul> <p>From now on, we always assume that only 1 ray is traced at each shading point, this is path tracing (Distributed Ray Tracing if \\(N\\neq1\\))</p> <pre><code>shade (p, wo): \n    Randomly choose 1 directions wi~pdf\n    Trace a ray r(p, wi)\n    If ray r hit the light:\n        Return * L_i * f_r * cosine / pdf(wi)\n    Else If ray r hit an object at q:\n        Return shade(q, -wi) * f_r * cosine / pdf(wi)\n    Return Lo\n</code></pre> <p>This can be too noisy, but we can just trace more paths through each pixel and average their radiance. It's similar to ray casting in ray tracing and called Ray Generation</p> <pre><code>ray_generation (camPos, pixel):\n    Uniformly choose N sample positions within the pixel\n    pixel_radiance = 0.0\n    For each sample in the pixel\n        Shoot a ray r(camPos, cam_to_sample)\n        If ray r hit the scene at p\n            pixel_radiance += 1 / N * shade(p, sample_to_cam)\n    Return pixel_radiance\n</code></pre> <p></p> <p>Problem 2: The recursive algorithm will never stop</p> <ul> <li>Dilemma: the light does not stop bouncing in reality. Cutting #bounces equals cutting energy</li> </ul> <p>Solution: Russian Roulette (RR)</p> <ul> <li> <p>Previously, we always shoot a ray at a shading point and get the shading result \\(L_o\\)</p> </li> <li> <p>Now, set a probability \\(P\\ (0&lt;P&lt;1)\\)</p> <ul> <li> <p>With probability \\(P\\), shoot a ray and return the shading result divided by \\(P\\): \\(Lo / P\\)</p> </li> <li> <p>With probability \\(1-P\\), don't shoot a ray and you'll get \\(0\\)</p> </li> </ul> </li> <li> <p>The expectation is still \\(L_o\\): \\(E=P\\cdot (L_o/ P)+(1-P)\\cdot 0=L_o\\)</p> </li> </ul> <pre><code>shade (p, wo): \n    Manually specify a probability P_RR\n    Randomly select ksi in a uniform dist. in [0, 1]\n    If (ksi &gt; P_RR) return 0.0;\n\n    Randomly choose ONE direction wi~pdf(w)\n    Trace a ray r(p, wi)\n    If ray r hit the light:\n        Return L_i * f_r * cosine / pdf(wi) / P_RR\n    Else If ray r hit an object at q:\n        Return shade(q, -wi) * f_r * cosine / pdf(wi) / P_RR\n</code></pre> <p>Now we already have a correct version of path tracing!</p> <p>But it's not really efficient.</p> <p></p>"},{"location":"Learning/GAMES101/RayTracing/#sampling-the-light","title":"Sampling the Light","text":"<p>Why inefficient? </p> <ul> <li>Only part of rays will hit the light while others are wasted if we uniformly sample the hemisphere</li> </ul> <p>Idea: </p> <ul> <li>Always sample the light so that no rays are wasted.</li> </ul> <p>Take the following case for example: </p> <ul> <li> <p>Assume uniformly sampling on the light, \\(pdf=\\frac{1}{A}\\)</p> <ul> <li>Because \\(\\int pdf\\ dA=1\\), where \\(A\\) is the light area</li> </ul> </li> <li> <p>The rendering equation integrates on the solid angle \\(d\\omega_i\\), we need transform it to an integral of \\(dA\\): </p> <ul> <li>Recall the alternative defination of solid angle, Projected area on the unit sphere: \\(d\\omega=\\frac{dA\\cos{\\theta'}}{\\|x'-x\\|^2}\\) (Note: \\(\\theta'\\) instead of \\(\\theta\\))</li> </ul> </li> <li> <p>Then we can rewrite the rendering equation as </p> </li> </ul> \\[ \\begin{align*} L_o(p,\\omega_o)&amp;=\\int_{\\Omega+}L_i(x,\\omega_i)f_r(x,\\omega_i,\\omega_o)\\cos{\\theta}d\\omega_i\\\\ &amp;=\\int_A L_i(x,\\omega_i)f_r(x,\\omega_i,\\omega_o)\\frac{\\cos{\\theta}\\cos{\\theta'}}{\\|x'-x\\|^2}dA \\end{align*} \\] <p></p> <p>Previously, we assume the light is \"accidentally\" shot by uniform hemisphere sampling</p> <p>Now we consider the radiance coming from two parts:</p> <ol> <li> <p>light source (direct, no need to have RR)</p> </li> <li> <p>other reflectors (indirect, RR)</p> </li> </ol> <p></p> <p>One final thing: what if the sample on the light is blocked? </p> <ul> <li>check when shooting</li> </ul> <pre><code>shade (p, wo):\n    # Contribution from the light source.\n    Uniformly sample the light at x' (pdf_light = 1 / A)\n    Shoot a ray from p to x'\n    If the ray is not blocked in the middle:\n        L_dir = L i * f_r * cos 0 * cos 0' / |x' - p| ^2 / pdf_light\n\n    # Contribution from other reflectors.\n    L_indir = 0.0\n    Test Russian Roulette with probability P_RR\n    Uniformly sample the hemisphere toward wi (pdf_hemi = 1 / 2pi)\n    Trace a ray r(p, wi)\n    If ray r hit a non-emitting object at q\n        L_indir = shade(q, -wi) * f_r * cos 0 / pdf_hemi / P_RR\n\n    Return L dir + L indir\n</code></pre> <p></p>"},{"location":"Learning/GAMES101/Shading/","title":"Shading","text":"<p>Definition: The process of applying a material to an object</p>"},{"location":"Learning/GAMES101/Shading/#blinn-phone-reflectance-model","title":"Blinn-Phone Reflectance Model","text":"<ul> <li>Shading is local, we only care the property of the object itself, while not caring about other objects.</li> <li>So no shadows will be generated (shading \\(\\neq\\) shadow)</li> </ul> <p>Compute light reflected toward camera at a specific shading point:</p> <p>Inputs:</p> <ul> <li>Viewer direction \\(\\mathbf{v}\\)</li> <li>Surface normal \\(\\mathbf{n}\\)</li> <li>Light drection \\(\\mathbf{l}\\)</li> <li>Surface parameters (color, shininess, ...)</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#diffuse-reflection","title":"Diffuse Reflection","text":"<ul> <li>Light is scattered uniformly in all directions</li> <li>Surface color is the same for all viewing directions</li> </ul> <ul> <li> <p>The light hitting the same surface at different angles will produce varying levels of brightness.</p> </li> <li> <p>How much light(energy) is received? Lambert's cosine law: </p> </li> </ul> <p></p> <ul> <li> <p>According to the law of energy conservation, the energy emitted by a point light source is the same at both near and far spheres.</p> </li> <li> <p>So the energy of the light will diminish with the distance between the object and the point light source.</p> </li> </ul> <p></p> <p>Lambertian (Diffuse) Shading: </p> \\[ L_d=k_d(I/r^2)\\max{(0, \\mathbf{n}\\cdot\\mathbf{l})} \\] <ul> <li>\\(L_d\\) is diffusely reflected light</li> <li>\\(k_d\\) is the diffuse coefficient(color), which represents the color and the proportion of light that is diffusely reflected by the surface.</li> <li>\\(I/r^2\\) is the energy arrived at the shading point</li> <li>\\(\\max{(0, \\mathbf{n}\\cdot\\mathbf{l})}\\) represents the energy received by the shading point. We constrain the minimum value to 0 because a negative value indicates that the light is hitting the back of the surface, which is not visible.</li> <li>Diffuse reflection is independent of view direction</li> <li>Produces diffuse appearance</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#specular-reflection","title":"Specular Reflection","text":"<ul> <li> <p>Intensity depends on view direction</p> </li> <li> <p>Bright near mirror reflection direction</p> <ul> <li>\\(\\mathbf{v}\\) close to mirror direction \\(\\leftrightarrow\\) half vector \\(\\mathbf{h}\\) near normal</li> </ul> </li> <li> <p>Measure \"near\" by dot product of unit vectors</p> </li> </ul> \\[ \\begin{align*} \\mathbf{h}&amp;=\\text{bisector}(\\mathbf{v}, \\mathbf{l})=\\frac{\\mathbf{v}+\\mathbf{l}}{\\|\\mathbf{v}+\\mathbf{l}\\|} \\\\ L_s&amp;=k_s(I/r^2)\\max{(0, \\cos{\\alpha})^p}\\\\ &amp;=k_s(I/r^2)\\max{(0, \\mathbf{n}\\cdot\\mathbf{h})^p} \\end{align*} \\] <p></p> <ul> <li>\\(L_s\\) is specularly reflected light</li> <li>\\(k_s\\) is specular coefficient, generally \\((1,1,1)\\)(white)</li> <li>Increasing \\(p\\) narrows the reflection lobe</li> </ul> <p> </p>"},{"location":"Learning/GAMES101/Shading/#ambient","title":"Ambient","text":"<ul> <li>Light from environment, does not depend on anything</li> <li>Add constant color to account for disregarded illumination and fill in black shadows</li> <li>This is approximate and fake!</li> </ul> \\[ L_a=k_aI_a \\]"},{"location":"Learning/GAMES101/Shading/#blinn-phong-reflection-model","title":"Blinn-Phong Reflection Model","text":"\\[ \\begin{align*} L&amp;=L_a+L_d+L_s\\\\ &amp;=k_aI_a+k_d(I/r^2)\\max{(0, \\mathbf{n}\\cdot\\mathbf{l})}+k_s(I/r^2)\\max{(0, \\mathbf{n}\\cdot\\mathbf{h})^p} \\end{align*} \\]"},{"location":"Learning/GAMES101/Shading/#shading-frequencies","title":"Shading Frequencies","text":"<p>So far, we have only discussed shading at a specific point.</p>"},{"location":"Learning/GAMES101/Shading/#shade-each-triangle-flat-shading","title":"Shade each triangle (flat shading)","text":"<ul> <li> <p>Triangle face is flat: one normal vector</p> </li> <li> <p>Shade once per triangle</p> </li> <li> <p>Not good for smooth surfaces</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#shade-each-vertex-gouraud-shading","title":"Shade each vertex (Gouraud shading)","text":"<ul> <li> <p>Each vertex has a normal vector</p> </li> <li> <p>Interpolate colors from vertices across triangle</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#shade-each-pixel-phong-shading","title":"Shade each pixel (Phong shading)","text":"<ul> <li> <p>Interpolate normal vectors across each triangle</p> </li> <li> <p>Compute full shading model at each pixel</p> </li> <li> <p>Not the Blinn-Phong Reflectance Model</p> </li> </ul> <p></p> <ul> <li>Flat shading also produces good results with more faces. </li> </ul>"},{"location":"Learning/GAMES101/Shading/#defining-per-vertex-normal-vectors","title":"Defining Per-Vertex Normal Vectors","text":"<ul> <li> <p>Best to get vertex normals from the underlying geometry (e.g. a sphere)</p> </li> <li> <p>Otherwise have infer vertex normals from triangle faces</p> <ul> <li>Simple scheme: average surrounding face normals</li> </ul> </li> </ul> \\[ N_v=\\frac{\\sum_iN_i}{\\|\\sum_iN_i\\|} \\] <ul> <li>Barycentric interpolation of vertex normals</li> </ul> <p></p> <p>Note</p> <p>Don't forget to normalize the interpolated directions</p>"},{"location":"Learning/GAMES101/Shading/#graphics-real-time-pipeline","title":"Graphics (real time) pipeline","text":""},{"location":"Learning/GAMES101/Shading/#shader-programs","title":"Shader Programs","text":"<ul> <li>Program vertex and fragment processing stages</li> <li>Describe operation on a single vertex (or fragment)</li> </ul> fragment shader<pre><code>uniform sampler2D myTexture;\nuniform vec3 lightDir;\nvarying vec2 uv;\nvarying vec3 norm;\n\nvoid diffuseShader()\n{\n    vec3 kd;\n    kd = texture2d(myTexture, uv);\n    kd *= clamp(dot(-lightDir, norm), 0.0, 1.0);\n    gl_FragColor = vec4(kd, 1.0);\n}\n</code></pre> <ul> <li>Shader function executes once per fragment.</li> <li>Outputs color of surface at the current fragment's screen sample position.</li> <li>This shader performs a texture lookup to obtain the surface's material color at this point, then performs a diffuse lighting calculation.</li> </ul>"},{"location":"Learning/GAMES101/Shading/#interpolation-across-triangles-barycentric-coordinates","title":"Interpolation Across Triangles: Barycentric coordinates","text":"<ul> <li> <p>Why do we want to interpolate?</p> <ul> <li>Specify values at vertices</li> <li>Obtain smoothly varying values across triangles</li> </ul> </li> <li> <p>What do we want to interpolate?</p> <ul> <li>Texture coordinates, colors, normal vectors, ...</li> </ul> </li> <li> <p>How do we interpolate?</p> <ul> <li>Barycentric coordinates</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#barycentric-coordinates","title":"Barycentric coordinates","text":"<ul> <li> <p>Given a triangle with vertex coordinates \\(A\\), \\(B\\), and \\(C\\), any point coplanar with the triangle can be represented as a linear combination of \\(A\\), \\(B\\), and \\(C\\).</p> <ul> <li>Here \\(A,B\\) and \\(C\\) are the coordinate of the vertex, i.e. \\(A=(x_A,y_A)\\)</li> </ul> </li> <li> <p>This forms a coordinate system for triangles \\((\\alpha, \\beta, \\gamma)\\)</p> <ul> <li>\\((x,y)=\\alpha A + \\beta B + \\gamma C\\)</li> <li>\\(\\alpha + \\beta + \\gamma = 1\\)<ul> <li>this constrain the points to be in the same plane</li> </ul> </li> <li>All three coordinates are non-negative if \\((x,y)\\) is inside the triangle.</li> </ul> </li> </ul> <p></p> <ul> <li>How can we get the Barycentric coordinate for a point inside the triangle?<ul> <li>The coefficient are proportional to areas</li> </ul> </li> </ul> \\[ \\begin{align*} \\alpha &amp;= \\frac{A_A}{A_A+A_B+A_C}\\\\ \\beta &amp;= \\frac{A_B}{A_A+A_B+A_C}\\\\ \\gamma &amp;= \\frac{A_C}{A_A+A_B+A_C} \\end{align*} \\] <p></p> <ul> <li> <p>What's the barycentric coordinate of the centroid?</p> <ul> <li>\\(\\alpha = \\beta = \\gamma = \\frac{1}{3}\\)</li> </ul> </li> <li> <p>You can also get the coordinate using the following formula</p> </li> </ul> \\[ \\begin{aligned}  &amp; \\alpha=\\frac{-(x-x_B)(y_C-y_B)+(y-y_B)(x_C-x_B)}{-(x_A-x_B)(y_C-y_B)+(y_A-y_B)(x_C-x_B)} \\\\  &amp; \\beta=\\frac{-(x-x_C)(y_A-y_C)+(y-y_C)(x_A-x_C)}{-(x_B-x_C)(y_A-y_C)+(y_B-y_C)(x_A-x_C)} \\\\  &amp; \\gamma=1-\\alpha-\\beta \\end{aligned} \\]"},{"location":"Learning/GAMES101/Shading/#using-barycentric-coordinates","title":"Using barycentric coordinates","text":"<ul> <li>Linearly interpolate values at vertices</li> </ul> \\[ V=\\alpha V_A+\\beta V_B + \\gamma V_C \\] <ul> <li>\\(V_A, V_B, V_C\\) can be positions, texture coordinates, color, normal, depth, material attributes ...</li> </ul> <p>Note</p> <p>However, barycentric coordinates are not invariant under projection! The barycentric coordinates may change after projection. It is better to compute this in 3D space.</p>"},{"location":"Learning/GAMES101/Shading/#bilinear-interpolation","title":"Bilinear Interpolation","text":"<ul> <li>Define: \\(\\text{lerp}(x,v_0,v_1)=v_0+x(v_1-v_0)\\)</li> <li>First interpolate horizontally: <ul> <li>\\(u_0=\\text{lerp}(s,u_{00},u_{10})\\)</li> <li>\\(u_1=\\text{lerp}(s,u_{01},u_{11})\\)</li> </ul> </li> <li>The vertical lerp to get the result:<ul> <li>\\(f(x,y) = \\text{lerp}(t,u_{0},u_{1})\\)</li> </ul> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#texture-mapping","title":"Texture Mapping","text":"<ul> <li> <p>Surfaces are 2D images lives in 3D world space</p> </li> <li> <p>Every 3D surface point maps to a point in the 2D image (texture)</p> </li> </ul> <p></p> <ul> <li>Each triangle \"copies\" a piece of the texture image to the surface</li> </ul> <p></p> <ul> <li>Each triangle vertex is assigned a texture coordinate \\((u,v)\\). Generally, both \\(u\\) and \\(v\\) are in range \\([0,1]\\) </li> </ul>"},{"location":"Learning/GAMES101/Shading/#applying-textures","title":"Applying Textures","text":"<ul> <li>Simple Texture Mapping: Diffuse Color</li> </ul> <pre><code>for each rasterized screen sample (x,y):\n    (u,v) = evaluate texture coordinate at (x,y)\n    texcolor = texture.sample(u,v);\n    set sample's color to texcolor;\n</code></pre>"},{"location":"Learning/GAMES101/Shading/#texture-magnification","title":"Texture Magnification","text":"<ul> <li> <p>What if the texture is too small?</p> </li> <li> <p>Generally don't want this: insufficient texture resolution</p> </li> <li> <p>texel: a pixel on a texture</p> </li> <li> <p>Many texels are mapped to one pixel</p> </li> </ul> <p></p> <ul> <li>What if the texture is too big?</li> </ul> <p></p> <ul> <li>Why?<ul> <li>The near pixel covers a small range of texels, while the far pixel covers a large range.</li> <li>Problems arise when we use a single texel to represent a large range.</li> </ul> </li> </ul> <p></p> <ul> <li>Will supersampling work?<ul> <li>Yes, high quality, but costly</li> <li>When highly minified, many texels in pixel footprint</li> <li>Signal frequency too large in a pixel</li> <li>Need even higher sampling frequency</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Let's understand this problem in another way</p> <ul> <li>What if we don't sample?</li> <li>Just need to get the average value within a range!</li> </ul> </li> <li> <p>Point Query: Given a point, return its value</p> </li> <li> <p>Average Range Query: Given a range, return its average value.</p> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#mipmap","title":"Mipmap","text":"<ul> <li>Allowing fast, approximate and square range queries</li> </ul> <ul> <li> <p>Level 0 is the original image</p> </li> <li> <p>Resolution is quartered with each level increase.</p> </li> <li> <p>Can be precomputed</p> </li> <li> <p>Storage overhead is only one-third of the original storage</p> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#computing-mipmap-level-d","title":"Computing Mipmap Level \\(D\\)","text":"<ul> <li>Estimate texture footprint using texture coordinates of neigboring screen samples. That is, how many units a texel in texture space will move when a pixel move one unit.</li> </ul> \\[ D=\\log_2L\\quad L=\\max\\left(\\sqrt{\\left(\\frac{du}{dx}\\right)^2+\\left(\\frac{dv}{dx}\\right)^2},\\sqrt{\\left(\\frac{du}{dy}\\right)^2+\\left(\\frac{dv}{dy}\\right)^2}\\right) \\] <ul> <li> <p>\\(D\\) rounds to the nearset integer</p> </li> <li> <p>Query the texel in Level \\(D\\)</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#smooth-boundary","title":"Smooth Boundary","text":"<ul> <li>Problem: The result is not smooth in the boundary of different level</li> </ul> <ul> <li>How to solve? Using trilinear interpolation</li> </ul> <ul> <li>Limitations: Overblur</li> </ul> <ul> <li>Why? There are irregular pixel footprint in texture.</li> </ul>"},{"location":"Learning/GAMES101/Shading/#anisotropic-filtering","title":"Anisotropic Filtering","text":"<p>Ripmaps and summed area tables * Can look up axis-aligned rectangular zones * Diagonal footprints still a problem</p> <p></p> <p>EWA filtering * Use multiple lookups * Weighted average * Mipmap hierarchy still helps * Can handle irregular footprints</p> <p></p>"},{"location":"Learning/GAMES101/Shading/#application","title":"Application","text":"<p>In modern GPUs, texture = memory + range query (filtering)</p> <ul> <li>General method to bring data to fragment calculations</li> </ul> <p>Many applications</p> <ul> <li>Environment lighting</li> <li>Store microgeometry</li> <li>Procedural textures</li> <li>Solid modeling</li> <li>Volume rendering</li> </ul>"},{"location":"Learning/GAMES101/Shading/#environment-map","title":"Environment Map","text":"<p>We can use texture to represent light from environment</p> <p></p> <p>Note</p> <p>Actually, the environment light is related to both position and direction, but here we approximate it by assuming the light source is infinitely far away.</p>"},{"location":"Learning/GAMES101/Shading/#spherical-environment-map","title":"Spherical Environment Map","text":"<p>Assuming you place a mirror sphere in the middle of the scene, you will be able to observe the reflections of the environment light on its surface, which helps in understanding the light's behavior. </p> <ul> <li>This inspires us to store the environment light on a sphere, which is called a Spherical Environment Map.</li> </ul> <p></p> <ul> <li>Problem: The top and bottom parts are prone to distortion</li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#cube-map","title":"Cube Map","text":"<ul> <li> <p>Store the environment light on a cube instead of a sphere, which is more uniform in each position.</p> </li> <li> <p>The cube is textured with 6 square texture maps</p> </li> <li> <p>Each direction vector maps to a cube point along it.</p> </li> </ul> <p></p> <ul> <li> <p>Much less distortion</p> </li> <li> <p>When querying a direction, need to first find the corresponding face.</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#bump-normal-mapping","title":"Bump &amp; Normal Mapping","text":"<p>For a bumpy surface, it is costly to represent it using triangles</p> <ul> <li> <p>Use bump mapping to represent relative height to the underlying surface</p> </li> <li> <p>Fake the detailed geometry</p> </li> </ul> <p></p> <p>Bump mappng adds surface detail without adding more triangles</p> <ul> <li> <p>Perturb surface normal per pixel for shading computation</p> </li> <li> <p>\"Height shift\" per texel defined by a texture</p> </li> <li> <p>The black line is the original surface while the orange one represents the perturbed surface</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#perturb-the-normal","title":"Perturb The Normal","text":"<p>In flatland (2D case), assume the surface normal is \\(n(p)=(0,1)\\)</p> <ul> <li> <p>Derivative at \\(p\\) is \\(dp=c\\times [h(p+1)-h(p)]\\)</p> <ul> <li>\\(c\\) is a constant to control the effect of bump mapping</li> </ul> </li> <li> <p>Perturbed normal is then \\(n(p)=(-dp,1).normalize()\\)</p> </li> </ul> <p>In 3D case, assume the original surface normal \\(n(p)=(0,0,1)\\)</p> <ul> <li> <p>Derivatives at p are </p> <ul> <li>\\(dp/du=c_1\\times[h(\\mathbf{u}+1)-h(\\mathbf{u})]\\)</li> <li>\\(dp/dv=c_1\\times[h(\\mathbf{v}+1)-h(\\mathbf{v})]\\)</li> </ul> </li> <li> <p>Perturbed normal is then \\(n(p)=(-dp/du,-dp/dv,1).normalize()\\)</p> </li> </ul> <p>Note</p> <p>Note that this is in local coordinate since we constrain the original normal to \\((0,0,1)\\). The result should be transformed to world coordinate later.</p>"},{"location":"Learning/GAMES101/Shading/#displacement-mapping","title":"Displacement Mapping","text":"<p>Displacement Mapping is a more advanced approach</p> <ul> <li> <p>Uses the same texture as in bumping mapping</p> </li> <li> <p>Actually moves the vertices</p> </li> <li> <p>More triangle needed to get a good result</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#3d-procedural-noise-solid-modeling","title":"3D Procedural Noise + Solid Modeling","text":"<ul> <li> <p>3D texture: Any point in 3D space has a texture value. The interior of an object also has texture.</p> </li> <li> <p>Define 3D noise function to generate texture</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES101/Shading/#provide-precomputed-shading","title":"Provide Precomputed Shading","text":"<p>Texture can also store precomputed information</p> <ul> <li> <p>ambient occlusion: store information about which part of the ambient light will be occluded</p> </li> <li> <p>multiply the shading result with the texture map. 0 means occluded.</p> </li> </ul> <p></p> <p>Note</p> <p>Textures can store various types of information, including but not limited to color. The meaning depends on how you interpret it in the shader.</p>"},{"location":"Learning/GAMES101/Shading/#gamma-correction","title":"Gamma Correction","text":"<p>Gamma Correction is an important technique in graphics used to adjust the brightness representation of images on display devices. </p>"},{"location":"Learning/GAMES101/Shading/#why-is-gamma-correction-needed","title":"Why is Gamma Correction Needed?","text":"<ul> <li> <p>Human Perception</p> <ul> <li> <p>The human eye is more sensitive to changes in brightness in dark areas and less sensitive to changes in bright areas.</p> </li> <li> <p>This perceptual characteristic can be described by a curve approximately following a power function.</p> </li> </ul> </li> <li> <p>Display Device Characteristics:</p> <ul> <li>Most display devices (e.g., monitors, TVs) have non-linear brightness output, typically following a curve approximately defined by \\(V_{out}=V^{\\gamma}_{in}\\) is the display device's gamma value (usually 2.2).</li> </ul> </li> <li> <p>Without gamma correction, images may appear too dark or too bright on display devices, failing to accurately reproduce the brightness of the original scene.</p> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#principle","title":"Principle","text":"<ul> <li> <p>Gamma Encoding:</p> <ul> <li>During image saving or transmission, brightness values are gamma-encoded \\(V_{encoded}=V_{linear}^{1/\\gamma}\\) </li> </ul> </li> <li> <p>Gamma Decoding:</p> <ul> <li>During image display, brightness values are gamma-decoded \\(V_{linear}=V_{encoded}^{\\gamma}\\) </li> </ul> </li> <li> <p>typically using \\(\\gamma=2.2\\)</p> </li> </ul>"},{"location":"Learning/GAMES101/Shading/#implementation","title":"Implementation","text":"<p>We always perform rendering or image processing in linear space, since brightness values in linear space are proportional to physical light intensity.</p> <ul> <li> <p>If textures are stored in gamma space (e.g., JPEG, PNG formats), perform gamma decoding on texture colors to transform them into linear space in the shader:</p> </li> <li> <p>Perform gamma encoding on the final color for outputing on screen.</p> </li> </ul> <pre><code>vec3 gammaCorrect(vec3 color) {\n    return pow(color, vec3(1.0 / 2.2));\n}\n\nvec3 gammaDecode(vec3 color) {\n    return pow(color, vec3(2.2));\n}\n\nvoid main() {\n    vec3 textureColor = texture2D(uSampler, vTexCoord).rgb;\n    vec3 linearColor = gammaDecode(textureColor);  // Gamma decoding\n    // Perform lighting calculations in linearColor\n    vec3 finalColor = gammaCorrect(linearColor);  // Gamma encoding\n    gl_FragColor = vec4(finalColor, 1.0);\n}\n</code></pre>"},{"location":"Learning/GAMES202/RTEnv/","title":"Real Time Environment Lighting","text":""},{"location":"Learning/GAMES202/RTEnv/#environment-lighting","title":"Environment Lighting","text":"<ul> <li> <p>An image representing distant lighting from all directions</p> <ul> <li>we assume the distant lighting is from infinitely far away</li> </ul> </li> <li> <p>Spherical map vs. cube map</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#shading-from-environment-lighting","title":"Shading from Environment Lighting","text":"<ul> <li> <p>Informally named Image-Based Lighting (IBL)</p> </li> <li> <p>How to use it to shade a point (without shadows)?</p> <ul> <li>Solving the rendering equation</li> </ul> </li> </ul> \\[ L_{o}\\left(\\mathrm{p}, \\omega_{o}\\right)=\\int_{\\Omega^{+}} \\boxed{L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right)} \\boxed{f_{r}\\left(\\mathrm{p}, \\omega_{i}, \\omega_{o}\\right)  \\cos \\theta_{i}} \\mathrm{~d} \\omega_{i} \\] <ul> <li> <p>General solution: Monte Carlo integration</p> <ul> <li> <p>Numerical </p> </li> <li> <p>Large amount of samples required </p> </li> </ul> </li> <li> <p>Problem: can be slow</p> <ul> <li>Can we avoid sampling?</li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#the-classic-approximation","title":"The Classic Approximation","text":"<ul> <li> <p>Observation</p> <ul> <li> <p>If the BRDF is glossy \u2014 small support! </p> </li> <li> <p>If the BRDF is diffuse \u2014 smooth! </p> </li> <li> <p>Good for the following approximation we mentioned before</p> </li> </ul> </li> </ul> \\[ \\int_{\\Omega} f(x) g(x) \\, dx \\approx \\frac{\\int_{\\Omega_{G}} f(x) \\, dx}{\\int_{\\Omega_{G}} \\, dx} \\cdot \\int_{\\Omega} g(x) \\, dx \\] <ul> <li> <p>BRDF satisfies the accuracy condition in any case</p> <ul> <li>We can safely take the lighting term out</li> </ul> </li> </ul> \\[ \\begin{align*} L_o(p,\\omega_o) \\approx \\boxed{\\frac{\\int_{\\Omega_{f_r}} L_i(p,\\omega_i) \\, \\mathrm{d}\\omega_i}{\\int_{\\Omega_{f_r}} \\, \\mathrm{d}\\omega_i}} \\cdot \\int_{\\Omega^+} f_r(p,\\omega_i,\\omega_o) \\cos\\theta_i \\, \\mathrm{d}\\omega_i \\end{align*} \\]"},{"location":"Learning/GAMES202/RTEnv/#the-split-sum-1st-stage","title":"The Split Sum: 1st Stage","text":"<ul> <li> <p>Prefiltering of the environment lighting</p> <ul> <li> <p>Pre-generating a set of differently filtered environment lighting </p> </li> <li> <p>Filter size in-between can be approximated via trilinear interp.</p> </li> </ul> </li> </ul> <p></p> <ul> <li>Then query the pre-filtered environment lighting at the \\(r\\) (mirror reflected) direction!</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#the-split-sum-2nd-stage","title":"The Split Sum: 2nd Stage","text":"<ul> <li> <p>The second term is still an integral</p> <ul> <li>How to avoid sampling this term</li> </ul> </li> </ul> \\[ \\begin{align*} L_o(p,\\omega_o) \\approx {\\frac{\\int_{\\Omega_{f_r}} L_i(p,\\omega_i) \\, \\mathrm{d}\\omega_i}{\\int_{\\Omega_{f_r}} \\, \\mathrm{d}\\omega_i}} \\cdot \\boxed{\\int_{\\Omega^+} f_r(p,\\omega_i,\\omega_o) \\cos\\theta_i \\, \\mathrm{d}\\omega_i} \\end{align*} \\] <ul> <li> <p>Idea</p> <ul> <li> <p>Precompute its value for all possible combinations of variables roughness, color (Fresnel term), etc. </p> </li> <li> <p>But we\u2019ll need a huge table with extremely high dimemsions</p> </li> </ul> </li> <li> <p>Recall: Microfacet BRDF</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>Idea &amp; Observation</p> <ul> <li> <p>Try to split the variables again! </p> </li> <li> <p>The Schlick approximated Fresnel term \\(F\\) is much simpler: Just the \"base color\" \\(R_0\\) and the half angle \\(\\theta\\)</p> </li> </ul> </li> <li> <p>Taking the Schlick\u2019s approximation into the 2nd term</p> <ul> <li>The \u201cbase color\u201d is extracted</li> </ul> </li> <li> <p>Both integrals can be precomputed</p> </li> </ul> \\[ \\begin{align*} \\int_{\\Omega^+}f_r(p,\\omega_i,\\omega_o)\\cos\\theta_i\\mathrm{d}\\omega_i\\approx &amp;R_0\\int_{\\Omega^+}\\frac{f_r}{F}\\left(1-(1-\\cos\\theta_i)^5\\right)\\cos\\theta_i\\mathrm{d}\\omega_i+ \\\\ &amp;\\int_{\\Omega^+}\\frac{f_r}{F}(1-\\cos\\theta_i)^5\\cos\\theta_i\\mathrm{d}\\omega_i \\end{align*} \\] <ul> <li> <p>Each integral produces one value for each (roughness, incident angle) pair</p> <ul> <li>Therefore, each integral results in a 2D table (texture)</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#the-split-sum-approximation","title":"The Split Sum Approximation","text":"<ul> <li> <p>Finally, completely avoided sampling</p> </li> <li> <p>Very fast and almost identical results</p> </li> </ul> <p></p> <ul> <li> <p>In the industry</p> <ul> <li>Integral -&gt; Sum</li> </ul> </li> </ul> \\[ \\frac{1}{N}\\sum_{k=1}^{N}\\frac{L_{i}(\\mathrm{l}_{k})f(\\mathrm{l}_{k},\\mathrm{v})\\cos\\theta_{\\mathrm{l}_{k}}}{p(\\mathrm{l}_{k},\\mathrm{~v})}\\approx\\left(\\frac{1}{N}\\sum_{k=1}^{N}L_{i}(\\mathrm{l}_{k})\\right)\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\frac{f(\\mathrm{l}_{k},\\mathrm{v})\\cos\\theta_{\\mathrm{l}_{k}}}{p(\\mathrm{l}_{k},\\mathrm{~v})}\\right) \\] <ul> <li>That\u2019s why it\u2019s called split sum rather than \u201csplit integral\u201d</li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#shadow-from-environment-lighting","title":"Shadow from Environment Lighting","text":"<ul> <li> <p>In general, very difficult for real-time rendering</p> </li> <li> <p>Different perspectives of view</p> <ul> <li> <p>As a many-light problem: Cost of SM is linearly to #light </p> </li> <li> <p>As a sampling problem: Visibility term \\(V\\) can be arbitrarily complex and \\(V\\) cannot be easily separated from the environment</p> </li> </ul> </li> <li> <p>Industrial solution</p> <ul> <li>Generate one (or a little bit more) shadows from the brightest light sources</li> </ul> </li> <li> <p>Related research</p> <ul> <li> <p>Imperfect shadow maps </p> </li> <li> <p>Light cuts </p> </li> <li> <p>RealTimeRayTracing (might be the ultimate solution) </p> </li> <li> <p>Precomputed radiance transfe</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#background-knowledge","title":"Background Knowledge","text":""},{"location":"Learning/GAMES202/RTEnv/#fourier-expansion","title":"Fourier Expansion","text":"<ul> <li>Represent a function as a weighted sum of sines and cosines with different frequency</li> </ul> \\[f(x) = \\frac{A}{2} + \\frac{2A \\cos(4w)}{\\pi} - \\frac{2A \\cos(3tw)}{3\\pi} + \\frac{2A \\cos(5tw)}{5\\pi} - \\frac{2A \\cos(7tw)}{7\\pi} + \\cdots\\]"},{"location":"Learning/GAMES202/RTEnv/#filtering-in-frequency-space","title":"Filtering in Frequency Space","text":"<ul> <li>Visualizing Image Frequency Content</li> </ul> <ul> <li>Filtering = Getting rid of certain frequency contents</li> </ul> <ul> <li>Convolution in spatial domain equals multiplication in frequency domain</li> </ul> <ul> <li>Any product integral can be considered as filtering</li> </ul> \\[ \\int_{\\Omega}f(x)g(y)dx \\] <ul> <li> <p>Low frequency == smooth function / slow changes / etc.</p> </li> <li> <p>The frequency of the integral is the lowest of any individual's</p> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#basis-function","title":"Basis Function","text":"<ul> <li>A set of functions that can be used to represent other functions in general</li> </ul> \\[ f(x) = \\sum_ic_i\\cdot B_i(x) \\] <ul> <li> <p>The Fourier series is a set of basis functions</p> </li> <li> <p>The polynomial series can also be a set of basis functions</p> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#real-time-environment-lighting_1","title":"Real-time Environment Lighting","text":""},{"location":"Learning/GAMES202/RTEnv/#spherical-harmonics","title":"Spherical Harmonics","text":"<ul> <li> <p>A set of 2D basis functions \\(B_i(\\omega)\\) defined on the sphere (functions of direction \\(\\theta,\\phi\\))</p> <ul> <li>Analogous to Fourier series in 1D</li> </ul> </li> <li> <p>There are \\(2i-1\\) functions in \\(l=i\\) and they are called the ith order basis function</p> <ul> <li>The frequency increases as \\(l\\) increases.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Each SH basis function \\(B_i(\\omega)\\) is associated with a Legendre polynomial</p> </li> <li> <p>Projection: obtaining the coefficients of each SH basis function</p> </li> </ul> \\[ c_i=\\int_{\\Omega}f(\\omega)B_i(\\omega)d\\omega \\] <ul> <li> <p>Reconstruction: restoring the original function using (truncated) coefficients and basis functions</p> <ul> <li>If we restore the original function up to the 4th order, the retained frequency will also be up to the frequency of the 4th order SH basis function.</li> </ul> </li> <li> <p>Properties: </p> <ul> <li> <p>orthonormal</p> </li> <li> <p>simple projection/reconstruction</p> </li> <li> <p>simple rotation</p> </li> <li> <p>simple convolution</p> </li> <li> <p>few basis functions means low frequencies</p> </li> </ul> </li> <li> <p>Diffuse BRDF acts like a low-pass filter, so the multiply intergral of BRDF and environment light has low frequency. We can just use the low order SH basis to describe the environment light for diffuse object.</p> <ul> <li>Note: using SH to describe light directly and this is about shading instead of shadowing</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#precomputed-radiance-transfer-prt","title":"Precomputed Radiance Transfer (PRT)","text":"<p>PRT handles shadows and global illumination</p> <p>Rendering under environment lighting: </p> <p></p> <ul> <li> <p>Assumption: we assume the objects in the scene are stationary and the environment light can change.</p> </li> <li> <p>Watching the shading point from a given angle \\(o\\), </p> <ul> <li> <p>\\(L(i)\\) is the environment light</p> </li> <li> <p>\\(V(i)\\) is the visibility, describing the occluding information for all direction (the environment light comes from all directions)</p> </li> <li> <p>BRDF can also be stored as a 2D map when we fix \\(o\\)</p> </li> </ul> </li> <li> <p>But the computation is prohibitive</p> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#prt","title":"PRT","text":"<p>The rendering computation is prohibitive for the method above, so here comes PRT</p> <p>Basic idea: </p> \\[ L(\\mathbf{o}) =\\int L\\left(\\mathbf{i}\\right)V\\left(\\mathbf{i}\\right)\\rho\\left(\\mathbf{i},\\mathbf{o}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\mathbf{d}\\mathbf{i} \\] <ul> <li> <p>We denote \\(L\\left(\\mathbf{i}\\right)\\) as lighting and \\(V\\left(\\mathbf{i}\\right)\\rho\\left(\\mathbf{i},\\mathbf{o}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\) as light transport</p> </li> <li> <p>We can approximate lighting using basis functions</p> <ul> <li>\\(L\\left(\\mathbf{i}\\right)\\approx\\sum l_i B_i(\\mathbf{i})\\)</li> </ul> </li> <li> <p>In precomputation stage:  </p> <ul> <li>compute light transport, and project to basis function space</li> </ul> </li> <li> <p>In runtime stage: </p> <ul> <li>using dot product for diffuse or matrix-vector multiplication for glossy</li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#diffuse-case","title":"Diffuse Case","text":"\\[ L(\\mathbf{o}) =\\int L\\left(\\mathbf{i}\\right)V\\left(\\mathbf{i}\\right)\\rho\\left(\\mathbf{i},\\mathbf{o}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\mathbf{d}\\mathbf{i} \\] <p>For diffuse material, the BRDF is a constant \\(\\rho\\). We extract it to the outside and expand \\(L(\\mathbf{i})\\) using SH basis. Then we get: </p> \\[ L(\\mathbf{o}) \\approx \\rho\\sum l_i\\boxed{\\int B_i\\left(\\mathbf{i}\\right)V\\left(\\mathbf{i}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\mathbf{d}\\mathbf{i}} \\] <p>The boxed part can be precomputed, so it turns out to be: </p> \\[ L(\\mathbf{o}) \\approx \\rho\\sum l_i T_i \\] <p>It reduce rendering computation to dot product</p> <ul> <li> <p>Run-time Rendering</p> <ul> <li>First, project the lighting to the basis to obtain \\(l_i\\) </li> <li>Or, rotate the lighting instead of re-projection </li> <li>Then, compute the dot product </li> </ul> </li> </ul> <p>We can expand \\(T\\) as </p> \\[ T\\left(\\mathbf{i}\\right)\\approx\\sum t_j B_j(\\mathbf{i}) \\] <p>Then we get: </p> \\[ \\begin{align*} L(\\mathbf{o}) &amp;=\\int_{\\Omega^+} L\\left(\\mathbf{i}\\right)V\\left(\\mathbf{i}\\right)\\rho\\left(\\mathbf{i},\\mathbf{o}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\mathbf{d}\\mathbf{i} \\\\ &amp;=\\sum_i\\sum_jl_it_j\\int_{\\Omega^+}B_i(\\mathbf{i})B_j(\\mathbf{i})d\\mathbf{i} \\end{align*} \\] <ul> <li> <p>Why is this a dot product? (This seems to be \\(O(n^2)\\) rather than \\(O(n)\\))</p> <ul> <li>Since SH basis is orthonormal</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#glossy-case","title":"Glossy Case","text":"<p>For glossy case, BRDF is related to viewing direction \\(\\mathbf{o}\\), so the equation becomes: </p> \\[ \\begin{align*} L(\\mathbf{o}) &amp;=\\int_{\\Omega^+} L\\left(\\mathbf{i}\\right)V\\left(\\mathbf{i}\\right)\\rho\\left(\\mathbf{i},\\mathbf{o}\\right)\\max(0,\\boldsymbol{n}\\cdot\\mathbf{i})\\mathbf{d}\\mathbf{i} \\\\ &amp;\\approx \\sum l_iT_i(\\mathbf{o}) \\end{align*} \\] <p>\\(T\\) can be expressd using SH basis: </p> \\[ T_i(\\mathbf{o})\\approx\\sum t_{ij}B_j(\\mathbf{o}) \\] <p>\\(t_{ij}\\) is the transport matrix (since \\(T\\) relates to both \\(\\mathbf{i}\\) and \\(\\mathbf{j}\\) now). Then we get: </p> \\[ L({\\mathbf{o}})\\approx\\sum(\\sum l_it_{ij})B_j(\\mathbf{o}) \\] <p></p> <ul> <li>When rendering: vector-matrix multiplication</li> </ul> <p>Time Complexity: </p> <ul> <li> </li> <li> <p>Diffuse Rendering</p> <ul> <li>At each point: dot-product of size 16</li> </ul> </li> <li> <p>Glossy Rendering  </p> <ul> <li>At each point:  vector(16) * matrix (16*16)</li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTEnv/#sh-basis-91625","title":"SH Basis: 9/16/25","text":""},{"location":"Learning/GAMES202/RTEnv/#interreflections-and-caustics","title":"Interreflections and caustics","text":"<ul> <li> <p>$LE: $ light to eye (directly)</p> </li> <li> <p>$LGE: $ light to glossy to eye (1 bounce)</p> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#limitations","title":"Limitations","text":"<ul> <li> <p>Low-frequency  </p> <ul> <li>Due to the nature of SH </li> </ul> </li> <li> <p>Dynamic lighting, but static scene/material </p> <ul> <li>Changing scene/material invalidates precomputed light transport </li> </ul> </li> <li> <p>Big precomputation data</p> </li> </ul>"},{"location":"Learning/GAMES202/RTEnv/#wavelet","title":"Wavelet","text":"<ul> <li> <p>2D haar wavelet, a kind of basis function in 2D block</p> </li> <li> <p>Projection:   </p> <ul> <li> <p>Wavelet Transformation</p> </li> <li> <p>After projection, many basis coefficient will be zero. Just retain a small number of non-zero coefficients (a non-linear approximation)</p> </li> </ul> </li> <li> <p>All-frequency representation</p> </li> </ul>"},{"location":"Learning/GAMES202/RTGL/","title":"Real-Time Global Illumination","text":"<p>In RTR, people seek simple and fast solutions to one bounce indirect illumination</p> <p></p>"},{"location":"Learning/GAMES202/RTGL/#real-time-global-illumination-in-3d","title":"Real-Time Global Illumination in 3D","text":""},{"location":"Learning/GAMES202/RTGL/#reflective-shadow-maps-rsm","title":"Reflective Shadow Maps (RSM)","text":"<p>Key Observations</p> <ul> <li> <p>Any directly lit surface will act as a light source again</p> </li> <li> <p>Which surface patches are directly lit? </p> <ul> <li> <p>Perfectly solved with a classic shadow map </p> </li> <li> <p>Each pixel on the shadow map is a small surface patch</p> </li> </ul> </li> <li> <p>What is the contribution from each surface patch to \\(p\\)?</p> <ul> <li> <p>An integration over the solid angle covered by the patch </p> </li> <li> <p>Can be converted to the integration on the area of the patch</p> </li> <li> <p>Then sum up all the surface patches\u2019 contributions</p> </li> </ul> </li> </ul> <p></p> <p>Assumption</p> <ul> <li> <p>Any reflector (not shading point) is diffuse </p> </li> <li> <p>Therefore, outgoing radiance is uniform toward all directions</p> </li> </ul> \\[ \\begin{align*} L_o(p, \\omega_o) &amp;= \\int_{\\Omega_{patch}} L_i(p, \\omega_i) \\, V(p, \\omega_i) f_r(p, \\omega_i, \\omega_o) \\, \\cos \\theta_i \\, d\\omega_i \\\\ &amp;= \\int_{A_{patch}} L_i(q \\to p) \\, V(p, \\omega_i) f_r(p, q \\to p, \\omega_o) \\frac{\\cos \\theta_p \\cos \\theta_q}{\\|q - p\\|^2} \\, dA \\end{align*} \\] <p>For a diffuse reflective patch: </p> <ul> <li> <p>BRDF at that patch \\(f_r=\\rho/\\pi\\)</p> </li> <li> <p>\\(L_i=f_r\\cdot \\frac{\\Phi}{dA}\\) (\\(\\Phi\\) is the incident flux or energy)</p> </li> </ul> <p>Therefore, </p> \\[ E_q(p, \\omega_o)=\\Phi_q\\frac{\\max\\{0, (\\cos\\theta_p)\\}\\max\\{0, (\\cos\\theta_q)\\}}{\\|q-p\\|^2} \\] <p>Not all pixels in the RSM can contribute</p> <ul> <li> <p>Visibility (still, difficult to deal with) </p> </li> <li> <p>Orientation </p> </li> <li> <p>Distance</p> </li> </ul> <p>Acceleration</p> <ul> <li> <p>In theory, all pixels in the shadow map can contribute to \\(p\\)</p> </li> <li> <p>Can we decrease the number? </p> <ul> <li>project shading point to shadow map, sample in the near region</li> </ul> </li> </ul> <p>What is needed to record in an RSM?</p> <ul> <li>Depth, world coordinate, normal, flux, etc.</li> </ul> <p></p> <p>Pros</p> <ul> <li>Easy to implement </li> </ul> <p>Cons</p> <ul> <li> <p>Performance scales linearly w/ #lights </p> </li> <li> <p>No visibility check for indirect illumination </p> </li> <li> <p>Many assumptions: diffuse reflectors, depth as distance, etc. </p> </li> <li> <p>Sampling rate / quality tradeoff</p> </li> </ul>"},{"location":"Learning/GAMES202/RTGL/#light-propagation-volumes-lpv","title":"Light Propagation Volumes (LPV)","text":"<p>Key problem</p> <ul> <li>Query the radiance from any direction at any shading point </li> </ul> <p>Key idea</p> <ul> <li>Radiance travels in a straight line and does not change </li> </ul> <p>Key solution</p> <ul> <li>Use 3D grids to propagate radiance from directly illuminated surfaces to anywhere else</li> </ul> <p></p> <p>Steps</p> <ol> <li> <p>Generation of radiance point set scene representation </p> <ul> <li> <p>This is to find directly lit surfaces </p> </li> <li> <p>Simply applying RSM would suffice! </p> </li> <li> <p>May use a reduced set of diffuse surface patches (virtual light sources)</p> </li> </ul> </li> <li> <p>Injection of point cloud of virtual light sources into radiance volume </p> <ul> <li> <p>Pre-subdivide the scene into a 3D grid, each grid stores the geometry and material info of the scene (normal, albedo, occlusion).</p> </li> <li> <p>For each grid cell, find enclosed virtual light sources </p> </li> <li> <p>Sum up their directional radiance distribution </p> </li> <li> <p>Project to first 2 orders of SHs(4 in total)</p> </li> </ul> </li> </ol> <p></p> <ol> <li> <p>Volumetric radiance propagation </p> <ul> <li> <p>For each grid cell, collect the radiance received from each of its 6 faces </p> </li> <li> <p>Sum up, and again use SH to represent </p> </li> <li> <p>Repeat this propagation several times till the volume becomes stable (use geometry and material info to simulate the reflection ...)</p> </li> </ul> </li> </ol> <p></p> <ol> <li> <p>Rendering scene with final light propagation volume</p> <ul> <li> <p>For any shading point, find the grid cell it is located in </p> </li> <li> <p>Grab the incident radiance in the grid cell (from all directions) </p> </li> <li> <p>Shade (based on normal and view direction)</p> </li> </ul> </li> </ol> <p>Problems: </p> <ul> <li>Light leaking: Since we assume the radiance in each grid is uniform, if the grid is large, the back face will also be lit.</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTGL/#voxel-global-illumination-vxgi","title":"Voxel Global Illumination (VXGI)","text":"<ul> <li> <p>Still a two-pass algorithm</p> </li> <li> <p>Two main differences with RSM</p> <ul> <li> <p>Directly illuminated pixels -&gt; (hierarchical) voxels </p> </li> <li> <p>Sampling on RSM -&gt; tracing reflected cones in 3D (Note the inaccuracy in sampling RSM)</p> </li> </ul> </li> <li> <p>Voxelize the entire scene</p> </li> <li> <p>Build a hierarchy</p> </li> </ul> <p></p> <ul> <li> <p>Pass 1 from the light</p> </li> <li> <p>Store the incident and normal distributions in each voxel </p> <ul> <li>so with the material information, the outcoming direction is known </li> </ul> </li> <li> <p>Update on the hierarchy</p> </li> </ul> <p></p> <ul> <li> <p>Pass 2 from the camera</p> <ul> <li> <p>For glossy surfaces, trace 1 cone toward the reflected direction </p> </li> <li> <p>Query the hierarchy based on the (growing) size of the cone</p> </li> <li> <p>For diffuse, trace several cones (e.g. 8)</p> </li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"Learning/GAMES202/RTGL/#real-time-global-illumination-screen-space","title":"Real-Time Global Illumination (screen space)","text":"<p>What is \"screen space\"?</p> <ul> <li> <p>Using information only from \u201cthe screen\u201d </p> </li> <li> <p>In other words, post processing on existing renderings</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTGL/#screen-space-ambient-occlusion-ssao","title":"Screen Space Ambient Occlusion (SSAO)","text":"<p>Why AO?</p> <ul> <li> <p>Cheap to implement </p> </li> <li> <p>But enhances the sense of relative positions</p> </li> </ul> <p></p> <p>What is SSAO?</p> <ul> <li> <p>An approximation of global illumination </p> </li> <li> <p>In screen space </p> </li> </ul> <p>Key idea</p> <ul> <li> <p>We don\u2019t know the incident indirect lighting. Let\u2019s assume it is constant (for all shading points, from all directions)</p> </li> <li> <p>Considering different visibility (towards all directions) at different shading points, since the ambient received corresponds to occlusion</p> </li> </ul> <p></p> <ul> <li>Aassuming diffuse materials</li> </ul> <p></p> <p>Theory</p> <ul> <li>Still, everything starts from the rendering equation</li> </ul> \\[ L_o(p, \\omega_o) = \\int_{\\Omega^+} \\boxed{L_i(p, \\omega_i) f_r(p, \\omega_i, \\omega_o)} \\boxed{V(p, \\omega_i)} \\cos\\theta_i \\, \\mathrm{d}\\omega_i \\] <ul> <li>And again, from \"the RTR approximation / equation\"!</li> </ul> \\[ \\int_{\\Omega} f(x) g(x) \\, dx \\approx \\frac{\\int_{\\Omega_{G}} f(x) \\, dx}{\\int_{\\Omega_{G}} dx} \\cdot \\int_{\\Omega} g(x) \\, dx \\] <ul> <li>Separating the visibility term</li> </ul> \\[ \\begin{align*} \\frac{\\textrm{indir}}{L_o(\\mathbf{p},\\omega_o)} \\approx \\boxed{\\frac{\\int_{\\Omega^+} V(\\mathbf{p}, \\omega_i) \\cos\\theta_i \\, \\mathrm{d}\\omega_i}{\\int_{\\Omega^+} \\cos\\theta_i \\, \\mathrm{d}\\omega_i}}\\cdot \\boxed{\\int_{\\Omega^{+}} L_{i}^{\\textrm{indir}}(\\mathbf{p},\\omega_{i}) f_{r}(\\mathbf{p},\\omega_{i},\\omega_{o})\\cos\\theta_{i}\\, \\mathrm{d}\\omega_{i}} \\end{align*} \\] <ul> <li> <p>\\(\\int_{\\Omega^+} \\cos\\theta_i \\, \\mathrm{d}\\omega_i=\\iint\\cos{\\theta_i}\\sin{\\theta_i}\\,\\mathrm{d}\\theta_i\\mathrm{d}\\phi=\\pi\\). For the left box, \\(\\boxed{}\\triangleq k_A=\\frac{\\int_{\\Omega^+} V(\\mathbf{p}, \\omega_i) \\cos\\theta_i \\, \\mathrm{d}\\omega_i}{\\pi}\\)</p> <ul> <li>This is the weight-averaged visibility \\(\\bar{V}\\) from all directions</li> </ul> </li> <li> <p>Since we assume constant ambient and diffuse material, for the right box, \\(\\boxed{}=L_i^{indir}(\\rho)\\cdot \\frac{\\rho}{\\pi}\\cdot\\pi=L_i^{indir}(\\rho)\\cdot\\rho\\)</p> <ul> <li>This is a constant for AO</li> </ul> </li> <li> <p>A deeper understanding 1</p> <ul> <li> <p>Actually, \\(\\frac{\\int_{\\Omega_{G}} f(x) \\, dx}{\\int_{\\Omega_{G}} dx}\\) is a kind of weighted sum (average here) over the support of \\(G\\)</p> </li> <li> <p>And for const \\(G=L\\cdot f_r\\) in AO, this approximation is accurate.</p> </li> </ul> </li> </ul> \\[ \\begin{align*} \\int_{\\Omega} f(x) g(x) \\, dx &amp;\\approx \\frac{\\int_{\\Omega_{G}} f(x) \\, dx}{\\int_{\\Omega_{G}} dx} \\cdot \\int_{\\Omega} g(x) \\, dx\\\\ &amp;= \\overline{f(x)} \\cdot \\int_{\\Omega} g(x) \\, dx \\end{align*} \\] <ul> <li> <p>A deeper understanding 2</p> <ul> <li> <p>Why can we take the cosine term with \\(\\mathrm{d}\\omega_i\\)?</p> </li> <li> <p>Projected solid angle \\(\\mathrm{d}x_\\perp=\\cos{\\theta_i}\\mathrm{d}\\omega_i\\)</p> <ul> <li> <p>Unit hemisphere -&gt; unit disk</p> </li> <li> <p>Integration of projected solid angle == the area of the unit disk == \\(\\pi\\)</p> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Actually, a much simpler understanding</p> <ul> <li> <p>Uniform incident lighting \\(L_i\\) is constant</p> </li> <li> <p>Diffuse BSDF \\(f_r=\\frac{\\rho}{\\pi}\\) is also constant</p> </li> <li> <p>Therefore, taking both out of the intergral:  </p> </li> </ul> </li> </ul> \\[ \\begin{aligned} L_{o}(\\mathrm{p},\\omega_{o}) &amp; =\\int_{\\Omega^+}L_i(\\mathrm{p},\\omega_i)f_r(\\mathrm{p},\\omega_i,\\omega_o)V(\\mathrm{p},\\omega_i)\\cos\\theta_i\\mathrm{d}\\omega_i \\\\  &amp; =\\frac{\\rho}{\\pi}\\cdot L_i(p)\\cdot\\int_{\\Omega^+}V(\\mathrm{p},\\omega_i)\\cos\\theta_i\\mathrm{d}\\omega_i \\end{aligned} \\] <p>How to compute the occlusion values \\(k_A\\) in real time?</p> <ul> <li> <p>Ambient occlusion approximation: limited radius</p> <ul> <li> <p>For a indoor scene, all ray will ultimately be occluded</p> </li> <li> <p>So we limit to local occlusion in a hemisphere of radius \\(R\\), only check occlusion in this region.</p> </li> <li> <p>More efficient and works better in enclosed areas such as indoors.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>SSAO: Ambient occlusion using the z-buffer</p> <ul> <li> <p>Use the readily available depth buffer as an approximation of the scene geometry.</p> </li> <li> <p>Take samples in a sphere around each pixel and test against buffer. </p> </li> <li> <p>We can approximately consider these samples unoccluded if their depth values are smaller than those in the buffer.</p> </li> <li> <p>If more than half of the samples are inside (since we sampled on the sphere), AO is applied, amount depending on ratio of samples that pass and fail depth test.</p> </li> <li> <p>Uses sphere instead of hemisphere, since normal information isn't available.</p> </li> <li> <p>Approximation of the scene geometry, some fails are incorrect and may cause false occlusions.</p> </li> <li> <p>Samples are not weighted by \\(\\cos(\\theta)\\), so not physically accurate, but looks convincing.</p> </li> </ul> </li> </ul> <p></p> <p></p> <ul> <li> <p>Choosing samples</p> <ul> <li> <p>More samples -&gt; greater accuracy</p> </li> <li> <p>Many samples are needed for a good result, but for performance only about 16 samples are used.</p> </li> <li> <p>Positions from randomized texture to avoid banding.</p> </li> <li> <p>Noisy result, blurred with edge preserving blur.</p> </li> </ul> </li> </ul> <p>AO after blur</p> <p></p> <ul> <li> <p>Horizon based ambient occlusion: HBAO</p> <ul> <li> <p>Also done in screen space.</p> </li> <li> <p>Approximates ray-tracing the depth buffer.</p> </li> <li> <p>Requires that the normal is known, and only samples in a hemisphere.</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RTGL/#screen-space-directional-occlusion","title":"Screen Space Directional Occlusion","text":"<p>What is SSDO?</p> <ul> <li> <p>An improvement over SSAO </p> </li> <li> <p>Considering (more) actual indirect illumination </p> </li> </ul> <p>Key idea</p> <ul> <li> <p>Why do we have to assume uniform incident indirect lighting? </p> </li> <li> <p>Some information of indirect lighting is already known!</p> </li> <li> <p>Very similar to path tracing</p> <ul> <li> <p>At shading point \\(p\\), shoot a random ray </p> </li> <li> <p>If it does not hit an obstacle, direct illumination </p> </li> <li> <p>If it hits one, indirect illumination</p> </li> </ul> </li> </ul> <p>Comparison w/ SSAO</p> <ul> <li> <p>AO: indirect illumination + no indirect illumination </p> <ul> <li>It assumes that ambient is received from rays in red circle, while ignore rays in orange circle</li> </ul> </li> <li> <p>DO: no indirect illumination + indirect illumination (same as path tracing)</p> <ul> <li>It assumes that indirect light is received from rays in orange circle (only within a small region)</li> </ul> </li> </ul> <p></p> <p>Screen Space Directional Occlusion:</p> <ul> <li>Consider unoccluded and occluded directions separately</li> </ul> \\[ \\begin{align*} L_{o}^{\\text{dir}}(p, \\omega_{o}) &amp;= \\int_{\\Omega^+,V = 1} L_{i}^{\\text{dir}}(p, \\omega_{i}) f_{r}(p, \\omega_{i}, \\omega_{o}) \\cos \\theta_{i} \\, d\\omega_{i}\\\\ L_{o}^{\\text{indir}}(p, \\omega_{o}) &amp;= \\int_{\\Omega^ +, V = 0} L_{i}^{\\text{indir}}(p, \\omega_{i}) f_{r}(p, \\omega_{i}, \\omega_{o}) \\cos \\theta_{i} \\, d\\omega_{i} \\end{align*} \\] <ul> <li>Indirect illum from a pixel (patch) is derived in RSM</li> </ul> <p>How to know which direction is occluded? </p> <ul> <li> <p>We don't want to do ray tracing for its cost. Similar to HBAO, we sample points in local hemisphere and test their depth with depth buffer from camera.</p> </li> <li> <p>Take the following figure as example and denote the camera position as \\(O\\), we make the following approximation: </p> <ul> <li> <p>Shoot a ray from camera to each sampled points and find their intersection with scene</p> </li> <li> <p>If the sampled points is occluded (depth further than intersection), we consider the intersection will contribute to the indirect light to \\(p\\) (note that the depth of intersections is recorded in depth buffer). </p> <ul> <li>In the left case, \\(A,B,D\\) are occluded and indirect light will be calculated at their intersections. \\(C\\) is unoccluded so we use the ray \\(PC\\) to query the value of Env. map.</li> </ul> </li> <li> <p>This is just an approximation and may fail in some cases, like the right figure.</p> </li> </ul> </li> </ul> <p></p> <p>SSDO: quality closer to offline rendering</p> <p>Issues?</p> <ul> <li> <p>Still, Global Illuminate in a short range</p> </li> <li> <p>Visibility is unaccurate</p> </li> <li> <p>Screen space issue: missing information from unseen surfaces</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTGL/#screen-space-reflection-ssr","title":"Screen Space Reflection (SSR)","text":"<p>What is SSR?</p> <ul> <li> <p>Still, one way to introduce Global Illumination in RTR </p> </li> <li> <p>Performing ray tracing in screen space </p> </li> <li> <p>Does not require 3D primitives (triangles, etc.) </p> </li> </ul> <p>Two fundamental tasks of SSR</p> <ul> <li> <p>Intersection: between any ray and the scene </p> </li> <li> <p>Shading: contribution from intersected pixels to the shading point</p> </li> </ul> <p>Motivation &amp; Assumption: </p> <ul> <li>Most of the reflected color are from the original screen space</li> </ul> <p></p> <p>Basic SSR Algorithm - Mirror Reflection</p> <ul> <li> <p>For each fragment</p> <ul> <li> <p>Compute reflection ray</p> </li> <li> <p>Trace along ray direction(using depth buffer)</p> </li> <li> <p>Use color of intersection point as reflection color</p> </li> </ul> </li> <li> <p>For glossy material, shoot several rays from each pixel</p> </li> </ul> <p></p> <p>How to know the intersection between the reflected ray and scene? </p> <ul> <li> <p>Linear Raymarch</p> <ul> <li> <p>March the relfected ray step by step</p> </li> <li> <p>At each step, check depth value</p> </li> <li> <p>intersect until the depth value less than that of scene</p> </li> <li> <p>Quality depends on step size</p> </li> <li> <p>Can be refined</p> </li> </ul> </li> </ul> <p></p> <p>How to decide the stride? </p> <ul> <li>we can do it dynamically if we know the safe distance that won't intersect with the scene </li> </ul> <p></p> <p>Hierarchical tracing</p> <ul> <li> <p>Generate Depth Mip-map</p> <ul> <li>Use min (closer) values instead of average</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Why Depth Mipmap?</p> <ul> <li> <p>Very similar to the hierarchy (BVH, KD-tree) in 3D</p> </li> <li> <p>Enabling faster rejecting of non-intersecting in a bunch</p> </li> <li> <p>The min operation guarantees a conservative logic</p> <ul> <li> <p>If a ray does not even intersect a larger node,it will never intersect any child nodes of it</p> </li> <li> <p>If the ray intersects the larger node at left, go checking the left child node.</p> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Stackless ray walk of min-Z pyramid</p> <ul> <li> <p>We start with the finest level and and proceed step by step.</p> </li> <li> <p>If no intersection is detected within the current cell, increase the step size for the next iteration.</p> </li> <li> <p>Otherwise, refine the search by examining the corresponding cell at a finer level.</p> <ul> <li>We can know which child cell to check through the intersection of the parent cell</li> </ul> </li> <li> <p>Repeat until intersect at the finest level or no intersection</p> </li> </ul> </li> </ul> <pre><code>mip = 0;\nwhile (level &gt; -1)\n    step through current cell;\n    if (above Z plane) ++level;\n    if (below Z plane) -- level;\n</code></pre> <p> </p> <p>Problem</p> <ul> <li>We don't know anything behind the screen</li> </ul> <p></p> <p></p> <p>Shading using SSR</p> <ul> <li> <p>Absolutely no difference from path tracing</p> <ul> <li> <p>Just again assuming diffuse reflectors / secondary lights</p> </li> <li> <p>We only know the color that reflected to the camera. If we use it to do computation, the reflector must be diffuse so that the color reflected to everywhere is same.</p> </li> </ul> </li> </ul> \\[ L_o(p, \\omega_o) = \\int_{\\Omega^+} L_i(p, \\omega_i) f_r(p, \\omega_i, \\omega_o) \\cos \\theta_i \\, d\\omega_i,\\ L_i(p, \\omega_i)=L_o(q,q\\rightarrow p) \\] <ul> <li> <p>Does it introduce the square distance falloff? </p> <ul> <li>No, since we are doing BRDF sampling instead of light sampling</li> </ul> </li> </ul> <p>Improvements</p> <ul> <li>BRDF importance sampling: don't need to sample uniformly, but sample in a lobe based on BRDF</li> </ul> <p></p> <ul> <li>Hit point reuse across neighbors</li> </ul> <p></p> <ul> <li> <p>Prefiltered samples, weighed be each BRDF</p> <ul> <li>may cause blending of foreground and background</li> </ul> </li> </ul> <p>Pros</p> <ul> <li> <p>Fast performance for glossy and specular reflections </p> </li> <li> <p>Good quality </p> </li> <li> <p>No spikes and occlusion issues </p> </li> </ul> <p>Cons</p> <ul> <li> <p>Not as efficient in the diffuse case* </p> </li> <li> <p>Missing information outside the screen</p> </li> </ul>"},{"location":"Learning/GAMES202/RTPBRM/","title":"Real-Time Physically-Based Materials","text":""},{"location":"Learning/GAMES202/RTPBRM/#pbr-and-pbr-materials","title":"PBR and PBR Materials","text":"<p>Physically-Based Rendering (PBR): </p> <ul> <li> <p>Everything in rendering should be physically based </p> </li> <li> <p>Materials, lighting, camera, light transport, etc. </p> </li> <li> <p>Not just materials, but usually referred to as materials</p> </li> </ul> <p>PBR materials in RTR: </p> <ul> <li> <p>The RTR community is much behind the offline community </p> </li> <li> <p>\u201cPB\u201d in RTR is usually not actually physically based</p> </li> <li> <p>For surfaces, mostly just microfacet models (used wrong so not PBR, left) and Disney principled BRDFs (artist friendly but still not PBR, right)</p> </li> </ul> <p></p> <ul> <li> <p>For volumes, mostly focused on fast and approximate single scattering and multiple scattering (for cloud, hair, skin, etc.)</p> </li> <li> <p>Usually not much new theory, but a lot of implementation hacks </p> </li> <li> <p>Still, performance (speed) is the key factor to consider</p> </li> </ul>"},{"location":"Learning/GAMES202/RTPBRM/#surface-model","title":"Surface Model","text":""},{"location":"Learning/GAMES202/RTPBRM/#recap-microfacet-brdf","title":"Recap: Microfacet BRDF","text":"<ul> <li>microfacets reflect as mirrors but its normals comform to some distribution</li> </ul> \\[ f(\\mathbf{i},\\mathbf{o})=\\frac{{\\mathbf{F}(\\mathbf{i},\\mathbf{h})}\\mathbf{G}(\\mathbf{i},\\mathbf{o},\\mathbf{h})\\mathbf{D}(\\mathbf{h})}{4(\\mathbf{n},\\mathbf{i})(\\mathbf{n},\\mathbf{o})} \\] <ul> <li> <p>\\(\\mathbf{F}(\\mathbf{i},\\mathbf{h})\\) is the Fresnel term, describe reflectance depending on incident angle</p> </li> <li> <p>\\(\\mathbf{G}(\\mathbf{i},\\mathbf{o},\\mathbf{h})\\) is the shadowing-masking term, modeling property of self occlusion</p> </li> <li> <p>\\(\\mathbf{D}(\\mathbf{h})\\) is the distribution of normals</p> </li> </ul>"},{"location":"Learning/GAMES202/RTPBRM/#normal-distribution-function-ndf","title":"Normal Distribution FUnction (NDF)","text":"<p>Key: the distribution of microfacets' normals</p> <p></p> <p>The Normal Distribution Function (NDF)</p> <ul> <li> <p>Note: has nothing to do with the normal distribution in stats </p> </li> <li> <p>Various models to describe it </p> <ul> <li> <p>Beckmann, GGX, etc. </p> </li> <li> <p>Detailed models</p> </li> </ul> </li> <li> <p>The following figures project the values on upper semisphere onto a circle (projected solid angle) to visualize it.</p> </li> </ul> <p></p> <p>Beckmann NDF: </p> <ul> <li>Similar to a Gaussian</li> </ul> \\[ D(h)=\\frac{e^{-\\frac{\\tan^2\\theta_h}{\\alpha^2}}}{\\pi\\alpha^2\\cos^4\\theta_h} \\] <ul> <li> <p>\\(\\alpha\\): roughness of the surface (the smaller, the more like mirror/specular) </p> </li> <li> <p>\\(\\theta_h\\): angle between half vector \\(h\\) and normal \\(n\\)</p> </li> <li> <p>Defined on the slope space (\\(\\tan{\\theta}\\) instead of \\(\\theta\\))</p> <ul> <li>map the value angles to a 2D plane</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Isotropic here, but can also be anisotropic</p> </li> <li> <p>denominator is for normalization</p> <ul> <li> <p>That is, the projected area of microfacet at any direction is same as that of macro surface</p> </li> <li> <p>The dot product is used for normalization here because the normal vector is involved in a dot product operation as part of the BRDF computation. (personal view)</p> </li> </ul> </li> </ul> \\[ \\int_{\\mathbf{m}\\in\\Theta}D(\\mathbf{m})(\\mathbf{v}\\cdot\\mathbf{m})d\\mathbf{m}=\\mathbf{v}\\cdot\\mathbf{n} \\] <ul> <li>When viewing direction is same as normal direction (\\(\\mathbf{v}=\\mathbf{n}\\)), we have: </li> </ul> \\[ \\int_{\\mathbf{m}\\in\\Theta}D(\\mathbf{m})(\\mathbf{n}\\cdot\\mathbf{m})d\\mathbf{m}=1 \\] <p>GGX: </p> \\[ D(h)=\\frac{\\alpha^2}{\\pi(\\cos{\\theta_h})^2(\\alpha^2-1)+1)^2} \\] <ul> <li> <p>Typical characteristic: long tail!</p> <ul> <li>have softer edge</li> </ul> </li> </ul> <p></p> <ul> <li>Comparison: Beckmann vs. GGX</li> </ul> <p></p> <p>Extending GGX </p> <ul> <li> <p>GTR (Generalized Trowbridge-Reitz) </p> </li> <li> <p>Additional parameter \\(\\gamma\\) to control the tail</p> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RTPBRM/#shadowing-masking-term","title":"Shadowing-Masking Term","text":"<p>The geometry term \\(G(i,o,h)\\): </p> <ul> <li> <p>Account for self-occlusion of microfacets </p> </li> <li> <p>Shadowing \u2014 light, masking \u2014 eye </p> </li> <li> <p>Provide darkening esp. around grazing angles</p> </li> </ul> <p>Why is it important?</p> <ul> <li>Suppose no \\(G\\) term, \\(f(i,o)\\) can be arbitarily bright around grazing angle since \\((n,i)\\) or \\((n,o)\\) is small</li> </ul> <p></p> <p>A commonly used shadowing-masking term</p> <ul> <li> <p>The Smith shadowing-masking term </p> </li> <li> <p>Decoupling shadowing and masking</p> </li> </ul> \\[ G(\\mathbf{i},\\mathbf{o},\\mathbf{m})\\approx G_1(\\mathbf{i},\\mathbf{m})G_1(\\mathbf{o},\\mathbf{m}) \\] <p></p>"},{"location":"Learning/GAMES202/RTPBRM/#multiple-bounces","title":"Multiple Bounces","text":"<p>Problem: Missing energy: </p> <ul> <li>Especially prominent when roughness is high because many rays are occluded</li> </ul> <p></p> <p>Adding back the missing energy?</p> <ul> <li> <p>Accurate methods exist  </p> </li> <li> <p>But can be too slow for RTR </p> </li> </ul> <p>Basic idea</p> <ul> <li>Being occluded == next bounce happening</li> </ul>"},{"location":"Learning/GAMES202/RTPBRM/#the-kulla-conty-approximation","title":"The Kulla-Conty Approximation","text":"<p>What's the overall energy of an outgoing 2D BRDF lobe? </p> <ul> <li> <p>Assuming the incoming ambient light \\(L_i=1\\) from any direction </p> </li> <li> <p>observing from \\(\\mu_o\\)</p> </li> </ul> \\[ \\begin{align*} E(\\mu_o)&amp;=\\int_0^{2\\pi}\\int_0^{\\pi/2}f(\\mu_o,\\theta,\\phi)\\cos{\\theta}\\sin{\\theta}d\\theta d\\phi \\\\ &amp;=\\int_0^{2\\pi}\\int_0^1f(\\mu_o,\\mu_i,\\phi)\\mu_id\\mu_id\\phi, \\ \\mu=\\sin\\theta \\end{align*} \\] <p>Key idea: </p> <ul> <li> <p>The loss of energy is \\(1-E(\\mu_o)\\). We can design an additional lobe that integrates to \\(1-E(\\mu_o)\\) to compensate for it.</p> </li> <li> <p>The outgoing BRDF lobe can be different for different incident dir. </p> </li> <li> <p>Consider reciprocity, it can be of the form \\(c(1-E(\\mu_i))(1-E(\\mu_o))\\). \\(c\\) is the constant for normalization</p> </li> </ul> <p>Therefore: </p> \\[ f_{\\mathrm{ms}}(\\mu_o, \\mu_i) = \\frac{(1 - E(\\mu_o))\\left(1 - E(\\mu_i)\\right)}{\\pi\\left(1 - E_{\\mathrm{avg}}\\right)}, \\quad E_{\\mathrm{avg}} = 2 \\int_{0}^{1} E(\\mu) \\, \\mu \\, d\\mu \\] <p>Validation: </p> \\[ \\begin{align*} E_{ms}(\\mu_o)&amp;=\\int_0^{2\\pi}\\int_0^1f(\\mu_o,\\mu_i,\\phi)\\mu_id\\mu_id\\phi\\\\ &amp;=2\\pi\\int_0^1 \\frac{(1 - E(\\mu_o))\\left(1 - E(\\mu_i)\\right)}{\\pi\\left(1 - E_{\\mathrm{avg}}\\right)}\\mu_id\\mu_i \\\\ &amp;=  2\\frac{1-E(\\mu_0)}{1-E_{\\rm avg}}\\int_0^1\\left(1-E\\left(\\mu_i\\right)\\right)\\,\\mu_i\\,d\\mu_i \\\\ &amp;=  \\frac{1-E(\\mu_0)}{1-E_{\\rm avg}}\\left(1-E_{\\rm avg}\\right) \\\\ &amp;=  1-E(\\mu_0) \\end{align*} \\] <ul> <li> <p>But neither \\(E(\\mu)\\) nor \\(E_{avg}\\) are analytic</p> <ul> <li>precompute / tabulate</li> </ul> </li> <li> <p>Dimension / parameters of \\(E(\\mu)\\) and \\(E_{avg}\\)</p> <ul> <li> <p>\\(E(\\mu)\\): roughness (related to one BRDF) &amp; \\(\\mu\\) [therefore, a 2D table]</p> </li> <li> <p>\\(E(avg)\\): roughness [therefore, a 1D table]</p> </li> </ul> </li> </ul> <p></p> <ul> <li>Results</li> </ul> <p></p> <ul> <li> <p>What if the BRDF has color? </p> <ul> <li> <p>Color == absorption == energy loss (as it should) </p> </li> <li> <p>So we\u2019ll just need to compute the overall energy loss </p> </li> </ul> </li> <li> <p>Define the average Frensel (how much energy is reflected, just a number)</p> </li> </ul> \\[ F_{avg}=\\frac{\\int_0^1F(\\mu)\\mu\\mathrm{d}\\mu}{\\int_0^1\\mu\\mathrm{d}\\mu}=2\\int_0^1F(\\mu)\\mu\\mathrm{d}\\mu \\] <ul> <li> <p>And recall that \\(E_{avg}\\) is how much energy that you can see (i.e., will NOT participate in further bounces)</p> </li> <li> <p>Therefore, the proportion of energy (color) that: </p> <ul> <li> <p>You can directly see: \\(F_{avg}E_{avg}\\)</p> </li> <li> <p>After one bounce then be seen: \\(F_{avg}(1-E_{avg})\\cdot F_{avg}E_{avg}\\)</p> <ul> <li>\\(F_{avg}(1-E_{avg})\\): The reflected energy that doesn't leave the surface (will participate in further bounces)</li> </ul> </li> <li> <p>After \\(k\\) bounces: \\(F_{avg}^k(1-E_{avg})^k\\cdot F_{avg}E_{avg}\\)</p> </li> <li> <p>\\(\\dots\\)</p> </li> <li> <p>Sum them up, we have te color term, Which will be directly multiplied on the uncolored additional BRDF</p> </li> </ul> </li> </ul> \\[ \\frac{F_{avg}E_{avg}}{1-F_{avg}(1-E_{avg})} \\] <p></p>"},{"location":"Learning/GAMES202/RTPBRM/#shading-microfacet-models-using-linearly-transformed-cosines-ltc","title":"Shading Microfacet Models using Linearly Transformed Cosines (LTC)","text":"<p>Linearly Transformed Cosines: </p> <ul> <li> <p>Solves the shading of microfacet models</p> </li> <li> <p>Mainly on GGX, though others are also fine </p> </li> <li> <p>No shadows </p> </li> <li> <p>Under polygon shaped lighting</p> </li> </ul> <p></p> <p>Key idea: </p> <ul> <li> <p>Transform outgoing 2D BRDF lobe to a cosine (\\(F(\\omega_i)\\rightarrow \\cos(\\omega_i')\\))</p> <ul> <li>different BRDF and different light \\(\\rightarrow\\) fixed BRDF and different light</li> </ul> </li> <li> <p>The shape of the light can also be transformed along </p> </li> <li> <p>Integrating the transformed light on a cosine lobe is analytic</p> </li> </ul> <p></p> <p>Observations: </p> <ul> <li> <p>\\(\\mathrm{BRDF}\\overset{M^{-1}}{\\longrightarrow}\\mathrm{Cosine}\\)</p> </li> <li> <p>Direction: \\(\\omega_i\\overset{M^{-1}}{\\longrightarrow}\\omega_i'\\)</p> </li> <li> <p>Domain to integrate: \\(P\\overset{M^{-1}}{\\longrightarrow}P'\\)</p> </li> </ul> <p></p> <p>Approach: </p> <ul> <li> <p>A simple change of variable</p> </li> <li> <p>Assume uniform light radiance inside the polygon light source</p> </li> </ul> \\[ \\begin{aligned} L(\\omega_{o}) &amp; =L_i\\cdot\\int_PF(\\omega_i)\\mathrm{d}\\omega_i \\\\  &amp; =L_{i}\\cdot\\int_{P}\\cos(\\omega_{i}^{\\prime})\\mathrm{d}\\frac{M\\omega_{i}^{\\prime}}{\\|M\\omega_{i}^{\\prime}\\|}, \\ \\omega_i=\\frac{M\\omega_{i}^{\\prime}}{\\|M\\omega_{i}^{\\prime}\\|} \\\\  &amp; =L_i\\cdot\\int_{P^{\\prime}}\\cos(\\omega_i^{\\prime})J\\mathrm{d}\\omega_i^{\\prime}\\quad-\\mathrm{Analytic!} \\end{aligned} \\] <ul> <li>Results</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/","title":"Real-time Shadow","text":""},{"location":"Learning/GAMES202/RtShadow/#shadow-mapping","title":"Shadow Mapping","text":"<ul> <li> <p>A 2-Pass Algorithm</p> <ul> <li> <p>The first pass render from light and output a \"depth texture\" from the light source</p> </li> <li> <p>The second pass render a standard image from the eye.</p> <ul> <li> <p>For each visible points in eye view, record the depth and reprojected it back to the light source to get another depth value in pass 1. </p> </li> <li> <p>Visible if the two depths match, otherwise blocked.</p> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>An image-space algorithm</p> <ul> <li> <p>Pro: no knowledge of scene\u2019s geometry is required </p> </li> <li> <p>Con: causing self occlusion and aliasing issues</p> </li> </ul> </li> <li> <p>Well known shadow rendering technique</p> <ul> <li>Basic shadowing technique even for early offline renderings, e.g., Toy Story</li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#issues-in-shadow-mapping","title":"Issues in Shadow Mapping","text":""},{"location":"Learning/GAMES202/RtShadow/#self-occlusion","title":"Self Occlusion","text":"<ul> <li> <p>The \"depth texture\" has its own resolution and the depth values are same in one pixel.</p> <ul> <li>Just like many small sheet perpendicular to light direction, which may cause severe self occlusion when light shoots on surface in grazing angle.</li> </ul> </li> </ul> <p></p> <p>How to solve?</p> <ul> <li> <p>Adding a (variable) bias to reduce self occlusion</p> <ul> <li> <p>Calculate as visible if the difference of two depth is smaller than the bias</p> </li> <li> <p>But introducing detached shadow issue</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Second-depth shadow mapping</p> <ul> <li> <p>Using the midpoint between first and second depths in SM </p> </li> <li> <p>Unfortunately, requires objects to be watertight </p> </li> <li> <p>And the overhead may not worth it</p> </li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#aliasing","title":"Aliasing","text":"<ul> <li>Also caused by resolution</li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#the-math-behind-shadow-mapping","title":"The math behind shadow mapping","text":"<ul> <li> <p>In RTR, we care more about \"approximately equal\"</p> </li> <li> <p>An important approximation throughout RTR: </p> </li> </ul> \\[ \\int_{\\Omega} f(x) g(x) \\, dx \\approx \\frac{\\int_{\\Omega} f(x) \\, dx}{\\int_{\\Omega} \\, dx} \\cdot \\int_{\\Omega} g(x) \\, dx \\] <ul> <li> <p>When is it (more) accurate?</p> <ul> <li> <p>When the integration domain is small and </p> </li> <li> <p>\\(g(x)\\) is smooth enough (i.e. its value does't change dramatically)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#in-shadow-mapping","title":"In Shadow Mapping","text":"<ul> <li>Recall: the rendering equation with explicit visibility</li> </ul> \\[ L_{o}\\left(\\mathrm{p}, \\omega_{o}\\right)=\\int_{\\Omega^{+}} \\boxed{L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right)} \\boxed{f_{r}\\left(\\mathrm{p}, \\omega_{i}, \\omega_{o}\\right)  \\cos \\theta_{i}} \\boxed{V(p,\\omega_i)} \\mathrm{~d} \\omega_{i} \\] <ul> <li>We can Approximate it as</li> </ul> \\[ L_0(p, \\omega_0) \\approx  \\boxed{\\frac {\\int_{\\Omega^+} V(p, \\omega_i) \\, d\\omega_i}{\\int_{\\Omega^+} d\\omega_i}} \\cdot \\int_{\\Omega^+} L_i(p, \\omega_i) f_r(p, \\omega_i, \\omega_0) \\cos \\theta_i \\, d\\omega_i \\] <ul> <li> <p>When is it accurate?</p> <ul> <li> <p>Small support (point / directional lighting) </p> </li> <li> <p>Smooth integrand (diffuse bsdf / constant radiance area lighting)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#percentage-closer-soft-shadows","title":"Percentage Closer Soft Shadows","text":"<p>From Hard Shadows to Soft Shadows</p> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#percentage-closer-filtering-pcf","title":"Percentage Closer Filtering (PCF)","text":"<p>Provides anti-aliasing at shadows' edges</p> <ul> <li> <p>Not for soft shadows (PCSS is, introducing later) </p> </li> <li> <p>Filtering the results of shadow comparisons</p> </li> </ul> <p>Why not filtering the shadow map?</p> <ul> <li> <p>Texture filtering just averages color components, i.e. you'll get blurred shadow map first </p> </li> <li> <p>Averaging depth values, then comparing, you still get a binary visibility</p> </li> </ul> <p>Solution [Reeves, SIGGARPH 87]</p> <ul> <li> <p>Perform multiple (e.g. 7x7) depth comparisons for each fragment in nearby region</p> </li> <li> <p>Then, averages results of comparisons  </p> </li> <li> <p>e.g. for point P on the floor,  </p> <ol> <li> <p>compare its depth with all pixels in the box, e.g. 3x3 </p> </li> <li> <p>get the compared results (1 means visible and 0 means invisible) e.g.      1, 0, 1,     1, 0, 1,     1, 1, 0, </p> </li> <li> <p>take avg. to get visibility, e.g. 0.667</p> </li> </ol> </li> </ul> <p></p> <p>Note that this is not soft shadows in the umbra/penumbra sense</p> <p></p> <p>Does filtering size matter?</p> <ul> <li> <p>Small -&gt; sharper </p> </li> <li> <p>Large -&gt; softer </p> </li> </ul> <p>When the filter is large, the effect is just like the soft shadow</p> <p>Key thoughts</p> <ul> <li> <p>First generate hard shadows, applying the filters in some place to change it into soft shadows </p> </li> <li> <p>What's the correct size to filter?</p> </li> <li> <p>Is it uniform? No</p> </li> </ul> <p>Key obsevation:</p> <ul> <li>The shadow is sharper when it's closer to object that casts it (pen tip), softer when farther (pen barrel)</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#percentage-closer-soft-shadows_1","title":"Percentage Closer Soft Shadows","text":"<p>Key conclusion</p> <ul> <li> <p>Filter size is related to blocker distance (the distance between blocker and receiver)</p> </li> <li> <p>More accurately, relative average projected blocker depth! </p> </li> </ul> <p></p> <p>A mathematical \"translation\"</p> <ul> <li>Larger \\(w\\) brings softer shadow</li> </ul> \\[ w_{Penumbra}=(d_{Receiver}-d_{Blocker})\\cdot w_{Light} / d_{Blocker} \\] <p>Now the only question:</p> <ul> <li> <p>What's the blocker depth \\(d_{Blocker}\\)</p> <ul> <li> <p>For a shading point, the average depth of the pixels that blocks it on shadow map</p> </li> <li> <p>Note that an area light doesn't have shadow map. Here we can approximate it by taking it as point light(put the camera on the center) </p> </li> </ul> </li> </ul> <p>The complete algorithm of PCSS</p> <ul> <li> <p>Step 1: Blocker search (getting the average blocker depth in a certain region) </p> </li> <li> <p>Step 2: Penumbra estimation (use the average blocker depth to determine filter size) </p> </li> <li> <p>Step 3: Percentage Closer Filtering </p> </li> </ul> <p>Which region (on the shadow map) to perform blocker search?</p> <ul> <li> <p>Can be set constant (e.g. 5x5), but can be better with heuristics</p> </li> <li> <p>depends on the light size and receiver's distance from the light</p> </li> <li> <p>We can just set the shadow map on near plane and shoot rays from light to shading points. Use the area under the rays to Calculate block depth.</p> <ul> <li>Note that we should only take the depth of blocked points into account. </li> </ul> </li> </ul> <p></p> <p>The math behind it: </p> <ul> <li>filter / convolution: </li> </ul> \\[ [w*f](p)=\\sum_{q\\in\\mathcal{N}(p)}w(p,q)f(q) \\] <ul> <li>In PCSS: </li> </ul> \\[ V(x)=\\sum_{q\\in\\mathcal{N}(x)}w(p,q)\\cdot\\chi^+[D_{\\mathrm{SM}}(q)-D_{\\mathrm{scene}}(x)] \\] <ul> <li>where \\(\\chi^{+}[x]=1\\text{ if }x&gt;0\\text{, otherwise }0\\). \\(w\\) is the weight.</li> </ul> <p></p> <ul> <li>Therefore, it is not filtering the shadow map then compare</li> </ul> \\[ V(x)\\neq\\chi^+\\{[w*D_{\\mathrm{SM}}](q)-D_{\\mathrm{scene}}(x)\\} \\] <ul> <li>Not filtering the resulting image with binary visibilities</li> </ul> \\[ V(x)\\neq\\sum_{q\\in\\mathcal{N}(x)}w(p,q)V(q) \\] <p>Which steps can be slow? </p> <ul> <li> <p>Looking at every texel inside a region (steps 1 and 3) </p> </li> <li> <p>Softer -&gt; larger filtering region -&gt; slower</p> </li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#variance-soft-shadow-mapping","title":"Variance Soft Shadow Mapping","text":"<p>For fast blocker search (step 1) and filtering (step 3)</p> <p>Let's think from \"percentage closer\" filtering</p> <ul> <li> <p>The percentage of texels that are in front of the shading point, i.e.,  </p> </li> <li> <p>how many texels are closer than \\(t\\) in the search area, i.e.,  </p> </li> <li> <p>how many students did better than you in an exam</p> <ul> <li> <p>Using a histogram -&gt; accurate answer! </p> </li> <li> <p>Using a Normal distribution -&gt; approximate answer! </p> </li> <li> <p>What do you need to define a normal distribution?</p> </li> </ul> </li> </ul> <p>Key idea </p> <ul> <li>Quickly compute the mean and variance of depths in an area </li> </ul> <p>Mean (average)</p> <ul> <li> <p>Hardware MIPMAPing </p> </li> <li> <p>Summed Area Tables (SAT) </p> </li> </ul> <p>Variance</p> <ul> <li> <p>\\(Var(X) = E(X^2) - E^2(X)\\)</p> </li> <li> <p>So you just need the mean of (depth2) </p> <ul> <li>Just generate a \"square-depth map\" along with the shadow map</li> </ul> </li> </ul> <p>We want to know the percentage of texels that are closer than the shading point</p> <ul> <li> <p>Need to calculate the shade's area</p> </li> <li> <p>Accurate answer can be acquired by looking up the \"error table\"</p> </li> </ul> <p></p> <p>Looking up table is cumbersome and it doesn't have to be too accurate:</p> <ul> <li> <p>We can approximate it using Chebychev's inequality (one-tailed version, for \\(t&gt;\\mu\\))</p> <ul> <li>\\(\\mu\\text{: mean, }\\sigma^2\\text{: variance, }\\)it doesn't even assume Gaussian distribution.</li> </ul> </li> </ul> \\[ P(x&gt;t)\\leq \\frac{\\sigma^2}{\\sigma^2+(t-\\mu)^2} \\] <p></p> <p>Performance</p> <ul> <li> <p>Shadow map generation: </p> <ul> <li> <p>\u201csquare depth map\u201d: parallel, along with shadow map, #pixels </p> </li> <li> <p>Anything else? </p> </li> </ul> </li> <li> <p>Run time </p> <ul> <li> <p>Mean of depth in a range: O(1) </p> </li> <li> <p>Mean of depth square in a range: O(1) </p> </li> <li> <p>Chebychev: O(1) </p> </li> <li> <p>No samples / loops needed! </p> </li> </ul> </li> <li> <p>Step 3 (filtering) solved perfectly</p> <ul> <li>But need to update mipmap and shadow map for moving scene</li> </ul> </li> </ul> <p>Now we come back to step 1, we also need range query in block search within an area</p> <ul> <li> <p>Also require sampling (loop) earlier, also inefficient </p> </li> <li> <p>The average depth of blockers </p> </li> <li> <p>Not the average depth \\(z_{avg}\\) </p> </li> <li> <p>The average depth of those texels whose depth \\(z &lt; t\\)</p> </li> </ul> <p>Key idea: </p> <ul> <li> <p>For blocker in blue \\((z&lt;t)\\), the average \\(z_{occ}\\) is what we want to compute</p> </li> <li> <p>For Non-blocker \\((z&gt;t)\\), the average is \\(z_{unocc}\\)</p> </li> <li> <p>We have the following formula: </p> </li> </ul> \\[ \\frac{N_1}{N}z_{unocc}+\\frac{N_2}{N}z_{occ}=z_{avg} \\] <ul> <li> <p>we can make the following approximations:  </p> <ul> <li> <p>\\({N_1}/{N}=P(x&gt;t)\\), using Chebychev</p> </li> <li> <p>\\({N_1}/{N}=1 - P(x&gt;t)\\)</p> </li> <li> <p>We really don't know \\(z_{unocc}\\), here we assume \\(z_{unocc}=t\\), i.e. they have the same depth as the shading point</p> </li> </ul> </li> </ul> <p></p> <ul> <li>Step 1 solved with negligible additional cost</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#mipmap-and-summed-area-variance-shadow-maps","title":"MIPMAP and Summed-Area Variance Shadow Maps","text":"<p>Key observation: </p> <ul> <li> <p>When computing \\(P(x\\geq t)\\), we need to quickly grab \\(\\mu\\) and \\(\\sigma\\) from an arbitary range (rectangular)</p> </li> <li> <p>For the average \\(\\mu\\), this is rectangular range query</p> <ul> <li>Can be handled by both MIPMAP and Summed Area Table (SAT)</li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#mipmap-for-range-query","title":"MIPMAP for Range Query","text":"<ul> <li>Note: still approximate even with trilinear interpolation</li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#sat-for-range-query","title":"SAT for Range Query","text":"<p>Range average is same as range sum.</p> <p>Classic data structure and algorithm (prefix sum)</p> <ul> <li>In 1D: </li> </ul> <p></p> <ul> <li>In 2D: </li> </ul> <p></p> <ul> <li>Note: accurate, but need \\(O(n)\\) time and storage to build</li> </ul>"},{"location":"Learning/GAMES202/RtShadow/#moment-shadow-mapping","title":"Moment Shadow Mapping","text":""},{"location":"Learning/GAMES202/RtShadow/#revisit-vssm","title":"Revisit VSSM","text":"<p>Normal distribution is not always good enough to approximate the distribution of fragments' distances.</p> <p></p> <ul> <li> <p>Issues if the depth distribution is inaccurate</p> <ul> <li> <p>Overly dark: may be aceeptable </p> </li> <li> <p>Overly bright: LIGHT LEAKING</p> </li> </ul> </li> </ul> <p></p> <p>Limitations?</p> <ul> <li> <p>Light leaking </p> </li> <li> <p>non-planarity artifact </p> </li> </ul> <p>Chebychev is to blame?</p> <ul> <li>Only valid when \\(t &gt; z_{avg}\\)</li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#moment-shadow-mapping_1","title":"Moment Shadow Mapping","text":"<ul> <li> <p>Goal</p> <ul> <li>Represent a distribution more accurately  (but still not too costly to store) </li> </ul> </li> <li> <p>Idea</p> <ul> <li>Use higher order moments to represent a distribution</li> </ul> </li> <li> <p>Moments</p> <ul> <li> <p>Quite a few variations on the definition </p> </li> <li> <p>We use the simplest: \\(x,x^2,x^3,x^4,\\dots\\)</p> </li> <li> <p>So, VSSM is essentially using the first two orders of moment</p> </li> </ul> </li> <li> <p>What can moments do?</p> <ul> <li> <p>Conclusion: first \\(m\\) orders of moments can represent a function with \\(m/2\\) steps</p> </li> <li> <p>Usually, 4 is good enough to approximate the actual CDF of depth dist. </p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Moment Shadow Mapping</p> <ul> <li> <p>Extremely similar to VSSM </p> </li> <li> <p>When generating the shadow map, record \\(z,z^2,z^3,z^4\\) </p> </li> <li> <p>Restore the CDF during blocker search &amp; PCF</p> </li> </ul> </li> <li> <p>Pro: very nice results</p> </li> <li> <p>Cons</p> <ul> <li> <p>Costly storage (might be fine) </p> </li> <li> <p>Costly performance (in the reconstruction)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"Learning/GAMES202/RtShadow/#distance-field-soft-shadows","title":"Distance Field Soft Shadows","text":"<p>Distance Functions: </p> <ul> <li> <p>At any point, giving the minimum distance (could be signed distance) to the closest location on an object </p> </li> <li> <p>An Example: Blending (linear interp.) a moving boundary</p> </li> </ul> <p></p> <p>Usage 1: ray-SDF intersection </p> <ul> <li> <p>Ray marching (sphere tracing) to perform ray-SDF intersection </p> </li> <li> <p>Very smart idea behind this: </p> <ul> <li> <p>The value of SDF == a \"safe\" distance around, i.e. the ray won't reach any object if it travel a distance less than the SDF at that point  </p> </li> <li> <p>Therefore, each time at p, just travel SDF(p) distance, until intersecting or travelling too far</p> </li> </ul> </li> </ul> <p></p> <p>Usage 2: </p> <ul> <li> <p>Use SDF to determine the (approx.) percentage of occlusion </p> </li> <li> <p>the value of SDF -&gt; a \"safe\" angle seen from the eye, i.e. the ray won't be blocked within this angle (just approximation)</p> <ul> <li>Smaller \u201csafe\u201d angle &lt;-&gt; less visibility</li> </ul> </li> </ul> <p></p> <ul> <li> <p>During ray matching</p> <ul> <li> <p>Calculate the \u201csafe\u201d angle from shading point to light at every step </p> </li> <li> <p>Keep the minimum </p> </li> </ul> </li> </ul> <p></p> <ul> <li>How to compute the angle? </li> </ul> \\[ \\arcsin{\\frac{SDF(p)}{p-o}} \\] <ul> <li>\\(arcsin\\) has high computational cost and actually, we don't need the accurate angle, but a relative relationship</li> </ul> \\[ \\min\\{\\frac{k\\cdot SDF(p)}{p-o}, 1.0\\} \\] <ul> <li>Larger \\(k\\) &lt;-&gt; earlier cutoff of penumbra &lt;-&gt; harder</li> </ul> <p></p> <ul> <li> <p>Pros</p> <ul> <li> <p>Fast</p> </li> <li> <p>High quality </p> </li> </ul> </li> <li> <p>Cons</p> <ul> <li> <p>Need precomputation </p> </li> <li> <p>Need heavy storage </p> </li> <li> <p>Artifact</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/intro/","title":"GAMES202 - Reaf-Time High Quality Rendering","text":"<ul> <li> <p>Real-Time High Quality Rendering</p> <ul> <li> <p>Speed: more than 30 FPS (frames per second), even more for Virtual / Augmented Reality (VR / AR): 90 FPS</p> </li> <li> <p>Interactivity: Each frame generated on the fly</p> </li> </ul> </li> <li> <p>Real-Time High Quality Rendering</p> <ul> <li> <p>Realism: advanced approaches to make rendering more realistic</p> </li> <li> <p>Dependability: all-time correctness (exact or approximate),no tolerance to (uncontrollable) failures</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/intro/#motivation","title":"Motivation","text":"<ul> <li> <p>Today, Computer Graphics is able to generate photorealistic images</p> <ul> <li> <p>Complex geometry, lighting, materials, shadows </p> </li> <li> <p>Computer-generated movies/special effects (difficult or impossible to tell real from rendered...)</p> </li> </ul> </li> <li> <p>But accurate algorithms (esp. ray tracing) are very slow</p> <ul> <li>So they are called offline rendering methods </li> </ul> </li> <li> <p>With proper approximations, we can generate plausible results but runs much faster</p> </li> </ul>"},{"location":"Learning/GAMES202/intro/#cg-basis","title":"CG Basis","text":""},{"location":"Learning/GAMES202/intro/#graphics-pipeline","title":"Graphics Pipeline","text":""},{"location":"Learning/GAMES202/intro/#opengl","title":"OpenGL","text":"<p>Is a set of APIs that call the GPU pipeline from CPU</p> <ul> <li> <p>Therefore, language does not matter! </p> </li> <li> <p>Cross platform </p> </li> <li> <p>Alternatives (DirectX, Vulkan, etc.) </p> </li> </ul> <p>Cons</p> <ul> <li> <p>Fragmented: lots of different versions </p> </li> <li> <p>C style, not easy to use</p> </li> </ul>"},{"location":"Learning/GAMES202/intro/#analogy-oil-painting","title":"Analogy: Oil Painting","text":"<p>A. Place objects/models</p> <ul> <li> <p>Model specification </p> </li> <li> <p>Model transformation </p> </li> <li> <p>User specifies an object\u2019s vertices, normals, texture coords and send them to GPU as a Vertex buffer object (VBO)</p> <ul> <li>The way how attributes stored is similar to .obj files </li> </ul> </li> <li> <p>Use OpenGL functions to obtain matrices</p> <ul> <li> <p>e.g., glTranslate, glMultMatrix, etc. </p> </li> <li> <p>No need to write anything on your own</p> </li> </ul> </li> </ul> <p>B. Set up an easel</p> <ul> <li> <p>View transformation </p> </li> <li> <p>Create / use a framebuffer, which performs the same function as an easel</p> </li> <li> <p>Set camera (the viewing transformation matrix) by simply calling, e.g., gluPerspectiv</p> </li> </ul> <p>C. Attach a canvas to the easel</p> <ul> <li> <p>Analogy of oil painting: </p> <ul> <li> <p>you can also paint multiple pictures using the same easel </p> </li> <li> <p>so you can render different textures using one framebuffer</p> </li> </ul> </li> <li> <p>One rendering pass in OpenGL</p> <ul> <li> <p>A framebuffer is specified to use </p> </li> <li> <p>Specify one or more textures as output (shading, depth, etc.) on one rendering pass</p> </li> <li> <p>Render (fragment shader specifies the content on each texture)</p> </li> </ul> </li> </ul> <p>D. Paint to the canvas</p> <ul> <li> <p>i.e., how to perform shading</p> </li> <li> <p>This is when vertex / fragment shaders will be used </p> </li> <li> <p>For each vertex in parallel   </p> <ul> <li>OpenGL calls user-specified vertex shader: Transform vertex (ModelView, Projection), other ops </li> </ul> </li> <li> <p>For each primitive, OpenGL rasterizes</p> <ul> <li>Generates a fragment for each pixel the fragment covers</li> </ul> </li> <li> <p>For each fragment in parallel</p> <ul> <li> <p>OpenGL calls user-specified fragment shader: Shading and lighting calculations </p> </li> <li> <p>OpenGL handles z-buffer depth test unless overwritten </p> </li> </ul> </li> <li> <p>This is the \u201cReal\u201d action that we care about the most: user-defined vertex, fragment shaders</p> <ul> <li> <p>Other operations are mostly encapsulated </p> </li> <li> <p>Even in the form of GUI-</p> </li> </ul> </li> </ul> <p>E. (Attach other canvases to the easel and continue painting) </p> <p>F. (Use previous paintings for reference)</p>"},{"location":"Learning/GAMES202/intro/#summary","title":"Summary","text":"<p>Summary: in each pass</p> <ul> <li> <p>Specify everything on GPU: </p> <ul> <li> <p>objects, camera, MVP, etc. </p> </li> <li> <p>framebuffer and input/output textures </p> </li> <li> <p>vertex / fragment shaders </p> </li> </ul> </li> <li> <p>Render!</p> </li> </ul>"},{"location":"Learning/GAMES202/intro/#shading-languages","title":"Shading Languages","text":"<ul> <li> <p>Vertex / Fragment shading described by small program</p> </li> <li> <p>Written in language similar to C but with restrictions</p> </li> <li> <p>Long history.  Cook\u2019s paper on Shade Trees, Renderman for offline rendering </p> <ul> <li> <p>In ancient times: assembly on GPUs! </p> </li> <li> <p>Stanford Real-Time Shading Language, work at SGI </p> </li> <li> <p>Still long ago: Cg from NVIDIA </p> </li> <li> <p>HLSL in DirectX (vertex + pixel) </p> </li> <li> <p>GLSL in OpenGL (vertex + fragment)</p> </li> </ul> </li> </ul>"},{"location":"Learning/GAMES202/intro/#shader-setup","title":"Shader Setup","text":"<ul> <li> <p>Initializing (shader itself discussed later)</p> <ul> <li> <p>Create shader (Vertex and Fragment) </p> </li> <li> <p>Compile shader  </p> </li> <li> <p>Attach shader to program </p> </li> <li> <p>Link program  </p> </li> <li> <p>Use program  </p> </li> </ul> </li> <li> <p>Shader source is just sequence of strings</p> </li> <li> <p>Similar steps to compile a normal program</p> </li> </ul>"},{"location":"Learning/GAMES202/intro/#rendering-equation","title":"Rendering Equation","text":"<p>Most important equation in rendering</p> <ul> <li>Describing light transpor</li> </ul> \\[ L_{o}\\left(\\mathrm{p}, \\omega_{o}\\right)=L_{e}\\left(\\mathrm{p}, \\omega_{o}\\right)+\\int_{H^{2}} f_{r}\\left(\\mathrm{p}, \\omega_{i} \\rightarrow \\omega_{o}\\right) L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right) \\cos \\theta_{i} \\mathrm{~d} \\omega_{i} \\] <p>In real-time rendering (RTR)</p> <ul> <li> <p>Visibility is often explicitly considered </p> </li> <li> <p>BRDF is often considered together with the cosine term</p> </li> </ul> \\[ L_{o}\\left(\\mathrm{p}, \\omega_{o}\\right)=\\int_{\\Omega^{+}} \\boxed{L_{i}\\left(\\mathrm{p}, \\omega_{i}\\right)} \\boxed{f_{r}\\left(\\mathrm{p}, \\omega_{i}, \\omega_{o}\\right)  \\cos \\theta_{i}} \\boxed{V(p,\\omega_i)} \\mathrm{~d} \\omega_{i} \\]"},{"location":"Learning/GAMES202/intro/#environment-lighting","title":"Environment Lighting","text":"<ul> <li> <p>Representing incident lighting from all directions</p> <ul> <li> <p>Usually represented as a cube map or a sphere map (texture) </p> </li> <li> <p>We\u2019ll introduce a new representation in this cours</p> </li> </ul> </li> </ul>"},{"location":"Tools/Git/","title":"Git command tutorial","text":""},{"location":"Tools/Git/#_1","title":"\u5de5\u4f5c\u533a\u3001\u6682\u5b58\u533a\u548c\u7248\u672c\u5e93","text":"<ul> <li>workspace\uff1a\u5de5\u4f5c\u533a\uff0c\u5373\u672c\u5730\u80fd\u770b\u5230\u7684\u76ee\u5f55</li> <li>staging area\uff1a\u6682\u5b58\u533a\uff0c\u4e00\u822c\u5b58\u653e\u5728 <code>.git</code> \u76ee\u5f55\u4e0b\u7684 index \u6587\u4ef6\uff08.git/index\uff09\u4e2d\uff0c\u6240\u4ee5\u6709\u65f6\u4e5f\u53eb\u4f5c\u7d22\u5f15\uff08index\uff09</li> <li>local repository\uff1a\u7248\u672c\u5e93\u6216\u672c\u5730\u4ed3\u5e93\uff0c\u5de5\u4f5c\u533a\u6709\u4e00\u4e2a\u9690\u85cf\u76ee\u5f55 .git\uff0c\u8fd9\u4e2a\u4e0d\u7b97\u5de5\u4f5c\u533a\uff0c\u800c\u662f Git \u7684\u7248\u672c\u5e93</li> <li>remote repository\uff1a\u8fdc\u7a0b\u4ed3\u5e93 </li> <li>\u56fe\u4e2d\u7684 objects \u6807\u8bc6\u7684\u533a\u57df\u4e3a Git \u7684\u5bf9\u8c61\u5e93\uff0c\u5b9e\u9645\u4f4d\u4e8e \".git/objects\" \u76ee\u5f55\u4e0b\uff0c\u91cc\u9762\u5305\u542b\u4e86\u521b\u5efa\u7684\u5404\u79cd\u5bf9\u8c61\u53ca\u5185\u5bb9\u3002</li> <li>\u5f53\u5bf9\u5de5\u4f5c\u533a\u4fee\u6539\uff08\u6216\u65b0\u589e\uff09\u7684\u6587\u4ef6\u6267\u884c <code>git add</code> \u547d\u4ee4\u65f6\uff0c\u6682\u5b58\u533a\u7684\u76ee\u5f55\u6811\u88ab\u66f4\u65b0\uff0c\u540c\u65f6\u5de5\u4f5c\u533a\u4fee\u6539\uff08\u6216\u65b0\u589e\uff09\u7684\u6587\u4ef6\u5185\u5bb9\u88ab\u5199\u5165\u5230\u5bf9\u8c61\u5e93\u4e2d\u7684\u4e00\u4e2a\u65b0\u7684\u5bf9\u8c61\u4e2d\uff0c\u800c\u8be5\u5bf9\u8c61\u7684ID\u88ab\u8bb0\u5f55\u5728\u6682\u5b58\u533a\u7684\u6587\u4ef6\u7d22\u5f15\u4e2d\u3002</li> <li>\u5f53\u6267\u884c\u63d0\u4ea4\u64cd\u4f5c <code>git commit</code>\u65f6\uff0c\u6682\u5b58\u533a\u7684\u76ee\u5f55\u6811\u5199\u5230\u7248\u672c\u5e93\uff08\u5bf9\u8c61\u5e93\uff09\u4e2d\uff0cmaster \u5206\u652f\u4f1a\u505a\u76f8\u5e94\u7684\u66f4\u65b0\u3002\u5373 master \u6307\u5411\u7684\u76ee\u5f55\u6811\u5c31\u662f\u63d0\u4ea4\u65f6\u6682\u5b58\u533a\u7684\u76ee\u5f55\u6811\u3002</li> <li>\u5f53\u6267\u884c <code>git reset HEAD</code> \u547d\u4ee4\u65f6\uff0c\u6682\u5b58\u533a\u7684\u76ee\u5f55\u6811\u4f1a\u88ab\u91cd\u5199\uff0c\u88ab master \u5206\u652f\u6307\u5411\u7684\u76ee\u5f55\u6811\u6240\u66ff\u6362\uff0c\u4f46\u662f\u5de5\u4f5c\u533a\u4e0d\u53d7\u5f71\u54cd\u3002</li> </ul>"},{"location":"Tools/Git/#_2","title":"\u57fa\u672c\u64cd\u4f5c","text":""},{"location":"Tools/Git/#_3","title":"\u521b\u5efa\u4ed3\u5e93","text":"<p><code>git init</code> \u5728\u5f53\u524d\u76ee\u5f55\u65b0\u5efaGit\u4ed3\u5e93 <code>git clone</code> \u5c06\u4e00\u4e2a\u8fdc\u7a0b\u4ed3\u5e93clone\u5230\u672c\u5730\u3002\u5176\u590d\u5236\u8fdc\u7a0b\u4ed3\u5e93\u7684\u6240\u6709\u4ee3\u7801\u548c\u5386\u53f2\u8bb0\u5f55\uff0c\u5e76\u5728\u672c\u5730\u521b\u5efa\u4e00\u4e2a\u4e0e\u8fdc\u7a0b\u4ed3\u5e93\u76f8\u540c\u7684\u4ed3\u5e93\u526f\u672c\u3002 <pre><code>$ git clone [url]\n</code></pre></p>"},{"location":"Tools/Git/#_4","title":"\u63d0\u4ea4\u4e0e\u4fee\u6539","text":"<p><code>git add</code> \u5c06\u4fee\u6539\u7684\u6587\u4ef6\u6dfb\u52a0\u5230\u6682\u5b58\u533a <pre><code>$ git add [file1] [file2] ...     //\u6dfb\u52a0\u6307\u5b9a\u6587\u4ef6\n$ git add [dir]                   //\u6dfb\u52a0\u6307\u5b9a\u76ee\u5f55\n$ git add .                       //\u6dfb\u52a0\u5f53\u524d\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n</code></pre> <code>git commit</code> \u5c06\u6682\u5b58\u533a\u7684\u5185\u5bb9\u63d0\u4ea4\u5230\u672c\u5730\u4ed3\u5e93 \u63d0\u4ea4\u65f6\u7684message\u662f\u5fc5\u987b\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>-m</code> \u53c2\u6570\u5728\u547d\u4ee4\u540e\u9762\u76f4\u63a5\u6dfb\u52a0\uff0c\u5426\u5219\u4f1a\u6253\u5f00\u9ed8\u8ba4\u7f16\u8f91\u5668\u6dfb\u52a0 <pre><code>$ git commit -m [message] \n$ git commit [file1] [file2] ... -m [message] //\u53ef\u4ee5\u53ea\u63d0\u4ea4\u6307\u5b9a\u6587\u4ef6\n$ git commit -a                               // -a \u53c2\u6570\u53ef\u4ee5\u4e0dadd\u76f4\u63a5\u63d0\u4ea4 \n</code></pre> <code>git reset</code> \u5c06\u6682\u5b58\u533a\u56de\u9000\u5230\u67d0\u4e00\u6b21\u63d0\u4ea4\u7684\u7248\u672c <pre><code>$ git reset [--soft | --mixed | --hard] [commit]\n$ git reset --hard [commit]       //\u5c06\u6682\u5b58\u533a\u548c\u5de5\u4f5c\u533a\u7684\u5185\u5bb9\u90fd\u91cd\u7f6e\u5230commit\u72b6\u6001\n</code></pre> <code>git restore</code> \u6062\u590d\u6216\u64a4\u9500\u6587\u4ef6\u7684\u66f4\u6539 <pre><code>$ git restore &lt;file&gt;            //\u5c06\u6307\u5b9a\u6587\u4ef6\u6062\u590d\u5230\u6700\u65b0\u7684\u63d0\u4ea4\u72b6\u6001\n$ git restore --staged &lt;file&gt;   //\u4ec5\u6062\u590d\u6682\u5b58\u533a\u7684\u6587\u4ef6\uff0c\u4e0d\u5f71\u54cd\u5de5\u4f5c\u533a\n</code></pre> <code>git revert</code> <pre><code>$ git revert &lt;commit&gt;   //\u751f\u6210\u4e00\u4e2a\u65b0\u7684commit\u8282\u70b9\uff0c\u65b0\u8282\u70b9\u4e0e\u6307\u5b9a\u8282\u70b9\u4e00\u6837\n</code></pre> <code>git mv</code> \u79fb\u52a8\u6216\u91cd\u547d\u540d <code>git rm</code> \u4ece\u6682\u5b58\u533a\u548c\u5de5\u4f5c\u533a\u5220\u9664\u6587\u4ef6 <pre><code>$ git rm &lt;file&gt;           \n$ git rm --cached &lt;file&gt;  // \u4ec5\u5220\u9664\u6682\u5b58\u533a\n</code></pre></p>"},{"location":"Tools/Git/#_5","title":"\u72b6\u6001\u67e5\u770b","text":"<p><code>git status</code> \u67e5\u770bGit\u4ed3\u5e93\u5f53\u524d\u72b6\u6001\uff0c\u663e\u793a\u4ee5\u4e0b\u4fe1\u606f\uff1a * \u5f53\u524d\u5206\u652f\u7684\u540d\u79f0 * \u5f53\u524d\u5206\u652f\u4e0e\u8fdc\u7a0b\u5206\u652f\u7684\u5173\u7cfb\uff08\u4f8b\u5982\uff0c\u662f\u5426\u662f\u6700\u65b0\u7684\uff09 * \u672a\u6682\u5b58\u7684\u4fee\u6539\uff1a\u663e\u793a\u5df2\u4fee\u6539\u4f46\u5c1a\u672a\u4f7f\u7528 git add \u6dfb\u52a0\u5230\u6682\u5b58\u533a\u7684\u6587\u4ef6\u5217\u8868 * \u672a\u8ddf\u8e2a\u7684\u6587\u4ef6\uff1a\u663e\u793a\u5c1a\u672a\u7eb3\u5165\u7248\u672c\u63a7\u5236\u7684\u65b0\u6587\u4ef6\u5217\u8868 <pre><code>$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   Git/git_command.md\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        Git/images/\n</code></pre> <code>-s</code> \u53c2\u6570\u663e\u793a\u7cbe\u7b80\u4fe1\u606f\uff0c\u7b2c\u4e00\u5217\u5b57\u7b26\u8868\u793a\u7248\u672c\u5e93\u4e0e\u6682\u5b58\u533a\u4e4b\u95f4\u7684\u6bd4\u8f83\u72b6\u6001\u3002\u7b2c\u4e8c\u5217\u5b57\u7b26\u8868\u793a\u6682\u5b58\u533a\u4e0e\u5de5\u4f5c\u533a\u4e4b\u95f4\u7684\u6bd4\u8f83\u72b6\u6001\u3002 * <code></code> \u7a7a\u683c\u8868\u793a\u6587\u4ef6\u672a\u6539\u52a8 * <code>M</code> \u6587\u4ef6\u53d1\u751f\u6539\u52a8 * <code>A</code> \u65b0\u589e\u6587\u4ef6 * <code>D</code> \u5220\u9664\u6587\u4ef6 * <code>R</code> \u6587\u4ef6\u91cd\u547d\u540d * <code>?</code> \u6587\u4ef6\u672a\u8ddf\u8e2a <pre><code>$ git status -s\nMM Git/git_command.md\nA  Git/images/workspace_and_localrepo.png\n?? Git/images/basic_op.png\n</code></pre> <code>git diff</code> \u6bd4\u8f83\u6587\u4ef6\u5728\u6682\u5b58\u533a\u548c\u5de5\u4f5c\u533a\u4e2d\u7684\u5dee\u522b <pre><code>$ git diff [file]               //\n$ git diff --cached [file]      //\u663e\u793a\u6682\u5b58\u533a\u4e0e\u4e0a\u4e00\u6b21\u63d0\u4ea4\u7684\u5dee\u5f02\n$ git diff [commit1] [commit2]  //\u663e\u793a\u4e24\u6b21\u63d0\u4ea4\u7684\u5dee\u5f02\n</code></pre> <code>git log</code> \u67e5\u770b\u5386\u53f2\u63d0\u4ea4\u8bb0\u5f55\uff0c<code>--help</code> \u67e5\u770b\u5e2e\u52a9 <code>git blame</code> \u9010\u884c\u663e\u793a\u6307\u5b9a\u6587\u4ef6\u7684\u5177\u4f53\u4fee\u6539</p>"},{"location":"Tools/Git/#_6","title":"\u5206\u652f","text":"<p>\u51e0\u4e4e\u6bcf\u4e00\u79cd\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u90fd\u4ee5\u67d0\u79cd\u5f62\u5f0f\u652f\u6301\u5206\u652f\uff0c\u4e00\u4e2a\u5206\u652f\u4ee3\u8868\u4e00\u6761\u72ec\u7acb\u7684\u5f00\u53d1\u7ebf\u3002 \u4f7f\u7528\u5206\u652f\u610f\u5473\u7740\u4f60\u53ef\u4ee5\u4ece\u5f00\u53d1\u4e3b\u7ebf\u4e0a\u5206\u79bb\u5f00\u6765\uff0c\u7136\u540e\u5728\u4e0d\u5f71\u54cd\u4e3b\u7ebf\u7684\u540c\u65f6\u7ee7\u7eed\u5de5\u4f5c\u3002 <code>git branch</code> <pre><code>$ git branch                  //\u5217\u51fa\u5206\u652f\n$ git branch [branchname]     //\u521b\u5efa\u5206\u652f\n$ git branch -d [branchname]  //\u5220\u9664\u5206\u652f\n</code></pre> <code>git switch</code> \u5207\u6362\u5206\u652f <pre><code>$ git switch [branchname]\n</code></pre> <code>git merge</code> \u5408\u5e76\u5206\u652f <pre><code>$ git merge [branchname]    //\u5c06\u8be5\u5206\u652f\u5408\u5e76\u5230\u5f53\u524d\u5206\u652f\n</code></pre> <code>git checkout</code> \u53ef\u4ee5\u5207\u6362\u5230\u5206\u652f(\u5efa\u8baeswitch)\u6216\u7279\u5b9a\u63d0\u4ea4 \u4ee5\u4e0b\u6307\u4ee4\u5c06\u5934\u6307\u9488\u5207\u6362\u5230commit\uff0c\u6b64\u65f6\u8fdb\u5165\"detached HEAD\"\u72b6\u6001\uff0c\u53ea\u80fd\u67e5\u770b\u5386\u53f2\u8bb0\u5f55\u800c\u4e0d\u80fd\u8fdb\u884c\u5206\u652f\u64cd\u4f5c\uff0c\u4e0d\u5efa\u8bae\u5728\u5206\u79bb\u5934\u6307\u9488\u72b6\u6001\u4e0b\u5de5\u4f5c\uff0c\u56e0\u4e3a\u66f4\u6539\u53ef\u80fd\u4f1a\u4e22\u5931\u3002 <pre><code>$ git checkout [commit]     \n</code></pre></p>"},{"location":"Tools/Git/#_7","title":"\u8fdc\u7a0b\u64cd\u4f5c","text":""},{"location":"Tools/Git/#remote","title":"remote","text":"<p><code>git remote</code> \u547d\u4ee4\u7528\u4e8e\u7528\u4e8e\u7ba1\u7406 Git \u4ed3\u5e93\u4e2d\u7684\u8fdc\u7a0b\u4ed3\u5e93\u3002 <pre><code>//\u5217\u51fa\u5f53\u524d\u4ed3\u5e93\u4e2d\u5df2\u914d\u7f6e\u7684\u8fdc\u7a0b\u4ed3\u5e93\n$ git remote\n//\u5217\u51fa\u5f53\u524d\u4ed3\u5e93\u4e2d\u5df2\u914d\u7f6e\u7684\u8fdc\u7a0b\u4ed3\u5e93\uff0c\u5e76\u663e\u793a\u5b83\u4eec\u7684 URL\u3002\n$ git remote -v\n//\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684\u8fdc\u7a0b\u4ed3\u5e93\u3002\u6307\u5b9a\u4e00\u4e2a\u8fdc\u7a0b\u4ed3\u5e93\u7684\u540d\u79f0\u548c URL\uff0c\u5c06\u5176\u6dfb\u52a0\u5230\u5f53\u524d\u4ed3\u5e93\u4e2d\u3002\n$ git remote add &lt;remote_name&gt; &lt;remote_url&gt;\n//\u5c06\u5df2\u914d\u7f6e\u7684\u8fdc\u7a0b\u4ed3\u5e93\u91cd\u547d\u540d\n$ git remote rename &lt;old_name&gt; &lt;new_name&gt;\n//\u4ece\u5f53\u524d\u4ed3\u5e93\u4e2d\u5220\u9664\u6307\u5b9a\u7684\u8fdc\u7a0b\u4ed3\u5e93\n$ git remote remove &lt;remote_name&gt;\n//\u4fee\u6539\u6307\u5b9a\u8fdc\u7a0b\u4ed3\u5e93\u7684 URL\n$ git remote set-url &lt;remote_name&gt; &lt;new_url&gt;\n//\u663e\u793a\u6307\u5b9a\u8fdc\u7a0b\u4ed3\u5e93\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u5305\u62ec URL \u548c\u8ddf\u8e2a\u5206\u652f\n$ git remote show &lt;remote_name&gt;\n</code></pre></p>"},{"location":"Tools/Git/#fetch-pull","title":"fetch &amp; pull","text":"<p><code>git fetch</code>\u5c06\u8fdc\u7a0b\u4ed3\u5e93\u7684\u6700\u65b0\u5185\u5bb9\u62c9\u53d6\u5230\u672c\u5730\uff0c\u53ef\u4ee5\u5728\u68c0\u67e5\u540e\u51b3\u5b9a\u662f\u5426\u5408\u5e76\u3002 <code>git pull</code>\u5c06\u8fdc\u7a0b\u4ed3\u5e93\u7684\u6700\u65b0\u5185\u5bb9\u62c9\u53d6\u5230\u672c\u5730\u5e76\u76f4\u63a5\u5408\u5e76\u3002\u53ef\u80fd\u4ea7\u751f\u51b2\u7a81\uff0c\u9700\u8981\u624b\u52a8\u89e3\u51b3</p>"},{"location":"Tools/Git/#git-fetch","title":"git fetch","text":"<p><pre><code>$ git fetch &lt;\u8fdc\u7a0b\u4e3b\u673a\u540d&gt; //\u5c06\u8fdc\u7a0b\u4e3b\u673a\u7684\u66f4\u65b0\u5168\u90e8\u53d6\u56de\u672c\u5730\n$ git fetch &lt;\u8fdc\u7a0b\u4e3b\u673a\u540d&gt; &lt;\u5206\u652f\u540d&gt; //\u4ec5\u5c06\u8be5\u5206\u652f\u53d6\u56de\n</code></pre> \u5982\u53d6\u56deLearning\u4e3b\u673a\u7684main\u5206\u652f\uff1a <pre><code>$ git fetch Learning main\nFrom github.com:mostimaaa/Learning\n * branch            main       -&gt; FETCH_HEAD\n</code></pre> \u5982\u679c\u4e4b\u540e\u8981\u8fdb\u884cmerge\u64cd\u4f5c\u6700\u597d\u6307\u660e\u5206\u652f\u540d \u53d6\u56de\u66f4\u65b0\u540e\u4f1a\u8fd4\u56de<code>FETCH_HEAD</code>\uff0c\u6307\u5411\u67d0\u4e2a\u5206\u652f\u5728\u670d\u52a1\u5668\u4e0a\u7684\u6700\u65b0\u72b6\u6001\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u6307\u4ee4\u67e5\u770b\uff1a <pre><code>$ git log -p FETCH_HEAD\ncommit cccb391c24e8bacdb0eb5d88730f8a6278a0d68d (Learning/main)\nAuthor: mostimaaa &lt;112405530+mostimaaa@users.noreply.github.com&gt;\nDate:   Sat Dec 16 19:27:59 2023 +0800\n\n    Initial commit\n\ndiff --git a/README.md b/README.md\nnew file mode 100644\nindex 0000000..f986672\n--- /dev/null\n+++ b/README.md\n@@ -0,0 +1,2 @@\n+# Learning\n+a simple repo for some learning resources\n</code></pre></p>"},{"location":"Tools/Git/#git-pull","title":"git pull","text":"<p><code>git pull = git fetch + git merge</code>\uff0c\u5373\uff1a <pre><code>$ git fetch Learning main   //\u83b7\u53d6\u6700\u65b0\u5206\u652f\uff0c\u8fd4\u56deFETCH_HEAD\n$ git merge FETCH_HEAD  //\u5c06\u8be5\u8fdc\u7a0b\u5206\u652f\u5408\u5e76\u5230\u672c\u5730\u5f53\u524d\u5206\u652f\n</code></pre> \u7528<code>git pull</code>\u53ef\u8868\u793a\u4e3a\uff1a <pre><code>$ git pull &lt;\u8fdc\u7a0b\u4e3b\u673a\u540d&gt; &lt;\u8fdc\u7a0b\u5206\u652f\u540d&gt;:&lt;\u672c\u5730\u5206\u652f\u540d&gt;\n</code></pre> \u5982\u679c\u662f\u4e0e\u672c\u5730\u5f53\u524d\u5206\u652f\u5408\u5e76\u5219\u53ef\u4e0d\u5199\u672c\u5730\u5206\u652f\u540d\uff1a <pre><code>$ git pull Learning main\n</code></pre></p>"},{"location":"Tools/Git/#conflict","title":"conflict","text":"<p>\u8981\u8fdb\u884c\u5408\u5e76\u7684\u5206\u652f\u6709\u4e92\u76f8\u51b2\u7a81\u7684\u6587\u4ef6\u6216\u884c\u4e3a\uff0c\u5982\u4e24\u4e2a\u5206\u652f\u4e2d\u76f8\u540c\u540d\u79f0\u7684\u6587\u4ef6\u6709\u4e0d\u540c\u5185\u5bb9 <pre><code>$ git pull Learning main\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 665 bytes | 55.00 KiB/s, done.\nFrom github.com:mostimaaa/Learning\n * branch            main       -&gt; FETCH_HEAD\n   cccb391..5a0f9f5  main       -&gt; Learning/main\nCONFLICT (modify/delete): README.md deleted in HEAD and modified in 5a0f9f57ec2556de6729f76549832fd2e16c337a.  Version 5a0f9f57ec2556de6729f76549832fd2e16c337a of README.md left in tree.\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre> \u64cd\u4f5c\u6b65\u9aa4\uff1a * \u5c06\u4e24\u4e2a\u5206\u652f\u7684\u4ee3\u7801\u62c9\u53d6\u5230\u672c\u5730 * \u624b\u52a8\u6574\u5408 * \u63d0\u4ea4</p> <p>\u62c9\u53d6\u5230\u672c\u5730\u540e\uff0c\u5728\u672c\u5730\u6253\u5f00\u51b2\u7a81\u7684\u6587\u4ef6\uff0c\u4fee\u6539\u4e3a\u60f3\u8981\u4fdd\u7559\u7684\u5185\u5bb9\u540e\u63d0\u4ea4\u5373\u53ef\u3002 \u5982\uff0c\u4fee\u6539README\u540e\u5982\u4e0b\uff1a <pre><code>86156@lzp MINGW64 ~/desktop/Learning (master|MERGING)\n$ git add README.md\n\n86156@lzp MINGW64 ~/desktop/Learning (master|MERGING)\n$ git commit\n[master c410de8] Merge branch 'main' of github.com:mostimaaa/Learning\n</code></pre></p>"},{"location":"Tools/Git/#push","title":"push","text":"<p><code>git push</code>\u5c06\u672c\u5730\u5206\u652f\u4e0a\u4f20\u5230\u8fdc\u7a0b\u5e76\u5408\u5e76 <pre><code>$ git push &lt;\u8fdc\u7a0b\u4e3b\u673a\u540d&gt; &lt;\u672c\u5730\u5206\u652f\u540d&gt;:&lt;\u8fdc\u7a0b\u5206\u652f\u540d&gt;\n$ git push &lt;\u8fdc\u7a0b\u4e3b\u673a\u540d&gt; &lt;\u672c\u5730\u5206\u652f\u540d&gt;    //\u5982\u8fdc\u7a0b\u5206\u652f\u540d\u4e0e\u672c\u5730\u5206\u652f\u540d\u76f8\u540c\u5219\u53ef\u7701\u7565\n</code></pre> \u5982\u5c06\u672c\u5730master\u5206\u652f\u63a8\u9001\u5230Learning\u4e3b\u673a\u7684main\u5206\u652f\uff1a <pre><code>$ git push Learning master:main\n</code></pre> \u5982\u679c\u672c\u5730\u7248\u672c\u4e0e\u8fdc\u7a0b\u7248\u672c\u6709\u5dee\u5f02\uff0c\u4f46\u53c8\u8981\u5f3a\u5236\u63a8\u9001\u53ef\u4ee5\u4f7f\u7528<code>--force</code>\u53c2\u6570 <pre><code>$ git push --force Learning master:main\n</code></pre> \u5220\u9664\u4e3b\u673a\u7684\u5206\u652f\u53ef\u4ee5\u4f7f\u7528 --delete \u53c2\u6570\uff0c\u4ee5\u4e0b\u547d\u4ee4\u8868\u793a\u5220\u9664 Learning \u4e3b\u673a\u7684 main \u5206\u652f\uff1a <pre><code>$ git push Learning --delete main\n</code></pre></p>"},{"location":"Tools/Python/","title":"Common Tools for Python","text":"<ul> <li> <p>setup: </p> </li> <li> <p>cmake: </p> </li> <li> <p>pytorch + cuda/cpp: </p> </li> </ul>"},{"location":"Tools/Website-building/","title":"How to build personal website with MkDocs","text":""},{"location":"Tools/Website-building/#mkdocs","title":"MkDocs","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. You can easily customize your webpages, preview your site and host it in almost anywhere.</p> <p>Run the following command to install mkdocs <pre><code>pip install mkdocs\n</code></pre> Run the following code to initialize the project <pre><code>mkdocs new my-project\n</code></pre> This will create a new folder in your current directory with basic files to build the site. <code>mkdocs.yml</code> is a configuration file, storing site configuration. <code>docs</code> is a folder which contains your documentation source files. You can also try the following command to generate these files directly in your current directory. <pre><code>mkdocs new .\n</code></pre> <code>mkdocs.yml</code> specifies basic site configurations. A simple demo is showing below. <pre><code>site_name: tianliangtian's personal website\nsite_url: https://tianliangtian.github.io\nsite_author: tianliangtian\n\ntheme:\n  name: material\n\nnav:\n  - Home: index.md\n  - Learning: \n    - Git: Learning/Git.md\n</code></pre> <code>site_name</code> is the name of the site. <code>site_url</code> is the place where you deploy your site.</p> <p>You can customize your site in the <code>theme</code> block</p> <p><code>nav</code> block helps you to arrange the order, title, and nesting of each page in the navigation header. Remember to relate your source files in <code>docs</code> to those pages.</p> <p>You can use the following command to preview your site in the given url. <pre><code>mkdocs serve\n</code></pre> The following commands generate static site files. <pre><code>mkdocs build\n</code></pre> The following command helps you deploy your pages. It will create a new branch called <code>gh-pages</code> in your Github project, execute <code>mkdocs build</code> and push the content in <code>site</code> to that branch. <pre><code>mkdocs gh-deploy\n</code></pre></p>"},{"location":"Tools/Website-building/#material-for-mkdocs","title":"Material for MkDocs","text":"<p>Material for MkDocs is a powerful documentation framework on top of MkDocs, a static site generator for project documentation. Install with pip <pre><code>pip install mkdocs-material\n</code></pre> You can customize your pages in <code>mkdocs.yml</code>. Configuration of my pages is shown below <pre><code>site_name: tianliangtian's pages\nsite_url: https://tianliangtian.github.io\nsite_author: tianliangtian\n\ntheme:\n  name: material\n  icon: \n    logo: material/library\n  features:\n    - navigation.tracking\n    - navigation.tabs\n    - navigation.expand\n    - navigation.indexes\n    - navigation.top\n    - navigation.footer\n    - toc.follow\n    - search.suggest\n    - search.highlight\n    - search.share\n    - header.autohide\n    - content.code.copy\n    - content.code.select\n    - content.code.annotate\n  palette:\n    # Palette toggle for light mode\n    - scheme: default\n      primary: pink\n      accent: teal\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    # Palette toggle for dark mode\n    - scheme: slate\n      primary: pink\n      accent: teal\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\nnav:\n  - Home: index.md\n  - Learning: \n    - Learning/index.md\n    - Git: Learning/Git.md\n    - Website building: Learning/Website-building.md\n\nplugins:\n  - search\n  - tags\n\nextra:\n  social:\n    - icon: fontawesome/brands/bilibili\n      link: https://github.com/tianliangtian\n    - icon: fontawesome/brands/github\n      link: https://github.com/tianliangtian\n  homepage: https://tianliangtian.github.io\n\nmarkdown_extensions:\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n\nextra_javascript:\n  - javascripts/mathjax.js\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n</code></pre> Please  check Material for MkDocs for more details.</p>"},{"location":"Tools/Website-building/#github-pages","title":"Github Pages","text":"<p>If you're already hosting your code on GitHub, GitHub Pages is certainly the most convenient way to publish your project documentation. It's free of charge and pretty easy to set up. There are two types of site. You can choose one of them to cater for your demand according to their different property, but remember that the two types can't exist simultaneously.</p> <ul> <li> <p>User or organization site: Head over to GitHub and create a new public repository named <code>&lt;username&gt;.github.io</code>, where username is your username (or organization name) on GitHub. If the first part of the repository doesn\u2019t exactly match your username, it won\u2019t work, so make sure to get it right. This is the only site you can use if you choose this type. The corresponding url is <code>http(s)://&lt;username&gt;.github.io</code></p> </li> <li> <p>Project site: You can have many repositories for different sites as long as the name of it isn't <code>&lt;username&gt;.github.io</code>. The corresponding url is <code>http(s)://&lt;username&gt;.github.io/&lt;repository&gt;</code></p> </li> </ul>"},{"location":"Tools/Website-building/#critical-steps","title":"Critical Steps","text":""},{"location":"Tools/Website-building/#installation","title":"Installation","text":"<p>Install MkDocs and Material for Mkdocs as mentioned before</p>"},{"location":"Tools/Website-building/#create-new-repo","title":"Create new repo","text":"<p>Create a new repo in Github with name <code>&lt;username&gt;.github.io</code> if you want to establish User or organization site. Clone the repo into a subfolder of your project root with git <pre><code>git clone https://github.com/tianliangtian/tianliangtian.github.io.git\n</code></pre></p>"},{"location":"Tools/Website-building/#create-your-site","title":"Create your site","text":"<p>Go to the directory where you want your project to be located and enter: <pre><code>mkdocs new .\n</code></pre> Setting your configuration in <code>mkdocs.yml</code> and add corresponding files in <code>docs</code> Use the following command to preview <pre><code>mkdocs serve\n</code></pre> Build your site with: <pre><code>mkdocs build\n</code></pre></p>"},{"location":"Tools/Website-building/#publishing-your-site","title":"Publishing your site","text":"<p>Using GitHub Actions you can automate the deployment of your project documentation. At the root of your repository, create a new GitHub Actions workflow, e.g. .github/workflows/ci.yml, and copy and paste the following contents: <pre><code>name: ci \non:\n  push:\n    branches:\n      - master \n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV \n      - uses: actions/cache@v3\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material \n      - run: mkdocs gh-deploy --force\n</code></pre> Now, when a new commit is pushed to either the master or main branches, the static site is automatically built and deployed. Push your changes to see the workflow in action.</p>"},{"location":"Tools/Website-building/#other","title":"Other","text":""},{"location":"Tools/Website-building/#workflow-permission","title":"Workflow permission","text":"<p>Revise your Actions permission and Workflow permissions in your Github repo setting so that the workflow will work. Go to <code>Settings</code> in your repo, find <code>Actions</code> in your left bar and click <code>General</code> * Actions permissions: Allow all actions and reusable workflows * Workflow permissions: Read and write permissions</p>"},{"location":"Tools/Website-building/#choose-the-right-branch","title":"Choose the right branch","text":"<p>The documentation is deployed in branch <code>gh-pages</code> so don't forget to choose it as the branch where your GitHub Pages built from. Go to <code>Settings</code> in your repo, click <code>Pages</code> in your left bar. In <code>Build and deployment</code>, choose <code>gh-pages</code> as the branch. </p>"},{"location":"Tools/Website-building/#referance","title":"Referance","text":"<ul> <li>https://www.mkdocs.org/</li> <li>https://squidfunk.github.io/mkdocs-material/</li> <li>https://yang-xijie.github.io/BLOG/Markdown/mkdocs-site/</li> </ul>"}]}